[
  {
    "objectID": "course/instructors.html",
    "href": "course/instructors.html",
    "title": "Instructor",
    "section": "",
    "text": "George Hagstrom, Ph.D.\nEmail: george.hagstrom@cuny.edu\nI am currently a Doctoral Lecturer in Data Science and Information Systems at the City University of New York. Additionally, I am a consultant with Princeton University where I development mathematical models of marine phytoplankton and heterotrophic bacteria to improve our understanding of the Earth’s Biogeochemical Cycles. Prior to joining CUNY, I was a Research Scientist at Princeton University in the Levin lab at the Department of Ecology and Evolutionary Biology and a postdoctoral researcher in the Magneto Fluids Division at the Courant Institute for Mathematical Sciences. My research interests include the application of Bayesian statistics and Machine Learning to incorporate nontraditional datasets (such as from ’omics) into mechanistic models of marine ecosystems and biogeochemical cycling, trait-based modeling, and critical transitions in complex ecological, social, or economic systems. In my free time, I enjoy cycling and exploring New York City.\n\nContact\nOffice Hours (Zoom is preferred): By appointment. You’re encouraged to schedule an appointment and I have time nearly everyday. You are also encouraged to ask us questions on Slack. If you wish to ask a question in private, you can email me directly.\nFor the most part, you can expect me to respond to questions by email within 24 hours. If you do not hear back from me within 48 hours of sending an email, please resend your message. I will be checking in on the course regularly, just about every day and likely several times each day. Please do not hesitate to ask if you have questions or concerns.",
    "crumbs": [
      "Course information",
      "Instructors"
    ]
  },
  {
    "objectID": "course/calendar.html",
    "href": "course/calendar.html",
    "title": "DATA607 - Data Acquisition and Management",
    "section": "",
    "text": "Note: Schedule is subject to change. Last updated August 20, 2024 06:23PM.\nCUNY SPS Academic Calendar\n\n\n\n\nClick here to import the course calendar into your calendar application",
    "crumbs": [
      "Course information",
      "Calendar"
    ]
  },
  {
    "objectID": "course/schedule.html",
    "href": "course/schedule.html",
    "title": "DATA607 - Data Acquisition and Management",
    "section": "",
    "text": "Click Here to Join the Meetups on Zoom\n\nCourse Schedule\n\n\n\n\n\n\n\n\n\nDate\nStart Time\nModule\nSlides\nVideo\n\n\n\n\nAug 28\n06:45PM\nData Science Workflows and Toolkit\nMeetup 1 Slides\nMeetup 1 Video\n\n\nSep 4\n06:45PM\nVisualizing Data\nMeetup 2 Slides\nMeetup 2 Video\n\n\nSep 11\n06:45PM\nData Tidying and Wrangling\nMeetup 3 Slides\nMeetup 3 Vide\n\n\nSep 18\n06:45PM\nExploratory Data Analysis\nMeetup 4 Slides\nMeetup 4 Video\n\n\nSep 25\n06:45PM\nData Transformations\nMeetup 5 Slides\nMeetup 5 Video\n\n\nOct 2\n06:45PM\nText and Strings\nMeetup 6 Slides\nMeetup 6 Video\n\n\nOct 9\n06:45PM\nDatabases and SQL\nMeetup 7 Slides\nMeetup 7 Video\n\n\nOct 16\n06:45PM\nAdvanced R Programming\nMeetup 8 Slide\n\n\n\nOct 23\n06:45PM\nWebscraping and APIs\n\n\n\n\nOct 30\n06:45PM\nGit and Collaboration\n\n\n\n\nNov 6\n06:45PM\nTidy Text and NLP\n\n\n\n\nNov 13\n06:45PM\nGraphs and Graph Data\n\n\n\n\nNov 20\n06:45PM\nBig Data\n\n\n\n\nNov 27\n\nNo Meetup (Thansgiving)\n\n\n\n\nDec 4\n06:45PM\nCloud Computing\n\n\n\n\nDec 11\n06:45PM",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/schedule.html#meetup-link",
    "href": "course/schedule.html#meetup-link",
    "title": "DATA607 - Data Acquisition and Management",
    "section": "",
    "text": "Click Here to Join the Meetups on Zoom\n\nCourse Schedule\n\n\n\n\n\n\n\n\n\nDate\nStart Time\nModule\nSlides\nVideo\n\n\n\n\nAug 28\n06:45PM\nData Science Workflows and Toolkit\nMeetup 1 Slides\nMeetup 1 Video\n\n\nSep 4\n06:45PM\nVisualizing Data\nMeetup 2 Slides\nMeetup 2 Video\n\n\nSep 11\n06:45PM\nData Tidying and Wrangling\nMeetup 3 Slides\nMeetup 3 Vide\n\n\nSep 18\n06:45PM\nExploratory Data Analysis\nMeetup 4 Slides\nMeetup 4 Video\n\n\nSep 25\n06:45PM\nData Transformations\nMeetup 5 Slides\nMeetup 5 Video\n\n\nOct 2\n06:45PM\nText and Strings\nMeetup 6 Slides\nMeetup 6 Video\n\n\nOct 9\n06:45PM\nDatabases and SQL\nMeetup 7 Slides\nMeetup 7 Video\n\n\nOct 16\n06:45PM\nAdvanced R Programming\nMeetup 8 Slide\n\n\n\nOct 23\n06:45PM\nWebscraping and APIs\n\n\n\n\nOct 30\n06:45PM\nGit and Collaboration\n\n\n\n\nNov 6\n06:45PM\nTidy Text and NLP\n\n\n\n\nNov 13\n06:45PM\nGraphs and Graph Data\n\n\n\n\nNov 20\n06:45PM\nBig Data\n\n\n\n\nNov 27\n\nNo Meetup (Thansgiving)\n\n\n\n\nDec 4\n06:45PM\nCloud Computing\n\n\n\n\nDec 11\n06:45PM",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/syllabus.html",
    "href": "course/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Instructor: George Hagstrom, Ph.D. Class Meetup: 6:45-7:45 Eastern Office Hours: By appointment Email: george.hagstrom@cuny.edu\n\nCourse Description\nIn this course students will learn about core concepts of contemporary data collection and its management. Topics will include an introduction to programming and collaboration in statistical software packages, data visualization techniques, data wrangling and transformation, exploratory data analysis and data quality checks, data acquisition from a variety of sources including databases and the web, tools for working with textual and graph data, feature engineering, and working with large datasets in a cloud computing environment.\nStudents will complete a project to create a working system for a large volume of data using publicly available data sets.\n\n\nCourse Learning Outcomes:\nBy then end of the course, students should be able to:\n\nLoad data into R from various data sources, including CSV files, Excel spreadsheets, relational databases, APIs, and web pages.\nPerform various data cleansing and transformation work, including splitting, combining; resampling; variable creation; data aggregation; sorting and filtering data; strategies for working with outliers and missing data; data visualization and analysis in support of data cleansing activities.\nUnderstand different information architectures, data types, and data structures.\nUnderstand relational and non-relational database design and querying.\nCover relevant ethical issues including data privacy and misinformation.\n\n\n\nProgram Learning Outcomes addressed by the course:\n\nBusiness Understanding. Apply frameworks and processes to build out data analytics solutions from understanding of business goals.\nData Culture. Embody and champion the highest standards for the ethical and moral use of data; understand issues related to data privacy and data security.\nSolid foundational data programming skills, using industry standard tools, essential algorithms, and design patterns for working with structured data, unstructured data and big data.\nData understanding. Collect, describe, model, explore and verify data.\nData preparation. Selecting, cleaning, constructing, integrating, and formatting data.\n\n\n\nHow is this course relevant for data analytics professionals?\nMost data analytics professionals spend most of their time getting data and preparing it for analysis. This is the course that teaches these key skills, as we work with both structured and unstructured data.\n\n\nGrading\n\nMeetup Reflections (10%)\nLabs (50%)\nTidyVerse Recipes\nData Science in Context Presentation (5%)\nProject (25%)\n\n\nGrade Distribution\n\n\n\n\n\n\n\n\n\nQuality of Performance\nLetter Grade\nRange %\nGPA\n\n\n\n\nExcellent - work is of exceptional quality\nA\n93 - 100\n4\n\n\nExcellent\nA-\n90 - 92.9\n3.7\n\n\nGood - work is above average\nB+\n87 - 89.9\n3.3\n\n\nSatisfactory\nB\n83 - 86.9\n3\n\n\nBelow Average\nB-\n80 - 82.9\n2.7\n\n\nPoor\nC+\n77 - 79.9\n2.3\n\n\nPoor\nC\n70 - 76.9\n2\n\n\nFailure\nF\n&lt; 70\n0\n\n\n\n\n\n\nHow This Course Works\nThis course is conducted entirely online. Each week, you will have various resources made available, including weekly readings from the textbooks and occasionally additional readings provided by the instructor. Most weeks will have homework assignments and labs to be submitted (although some chapters will take more than one week, see the schedule for details). There will also be a presentation required and a forum post introduction required. You are expected to complete all assignments by their due dates.\nYou are expected to attend or watch every Meetup. I highly recommend attending the Meetups live if possible but understand that may not be possible for everyone. Recordings will be made available by the next morning on the Meetups page. In addition to highlighting key concepts from each learning module, some topics will be discussed that are not in the textbook. Moreover, we regularly make announcements in the Meetups that will be important to being successful in this course. At the end of each Meetup there will be a short reflective exercise. These will contribute to your participation grade.\nEach students will have to complete one short “Data Science in Context Presentation”- you will sign up for presentation times at the beginning of the semester.\nThe culmination of the course will be the presentation of the analysis of a dataset of your choosing. See the project for more information.\n\n\nTextbooks and Course Materials\n\nR for Data Science (2e) by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. This is the primary text for the course. Available online for free at: https://r4ds.hadley.nz/\nHappy Git and GitHub for the userR, Jennifer Bryan. Available online for free at happygitwithr.com/. This is a short book introducing git and github from the perspective of the statistical computing and data science use cases, and showing how it can be integrated with R and RStudio.\nText Mining with R: A Tidy Approach, Julia Silge and David Robinson. O’Reilly, 2017. Available online for free at https://www.tidytextmining.com/\n\n\n\nAccessibility and Accommodations\nThe CUNY School of Professional Studies is firmly committed to making higher education accessible to students with disabilities by removing architectural barriers and providing programs and support services necessary for them to benefit from the instruction and resources of the University. Early planning is essential for many of the resources and accommodations provided. Please see: http://sps.cuny.edu/student_services/disabilityservices.html\n\n\nOnline Etiquette and Anti-Harassment Policy\nThe University strictly prohibits the use of University online resources or facilities, including Brightspace, for the purpose of harassment of any individual or for the posting of any material that is scandalous, libelous, offensive or otherwise against the University’s policies. Please see: http://media.sps.cuny.edu/filestore/8/4/9_d018dae29d76f89/849_3c7d075b32c268e.pdf\n\n\nAcademic Integrity\nAcademic dishonesty is unacceptable and will not be tolerated. Cheating, forgery, plagiarism and collusion in dishonest acts undermine the educational mission of the City University of New York and the students’ personal and intellectual growth. Please see: http://media.sps.cuny.edu/filestore/8/3/9_dea303d5822ab91/839_1753cee9c9d90e9.pdf\n\n\nStudent Support Services\nIf you need any additional help, please visit Student Support Services: http://sps.cuny.edu/student_resources/",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "posts/2024-09-26-Data-Transformation-Vignette.html",
    "href": "posts/2024-09-26-Data-Transformation-Vignette.html",
    "title": "Data Transformation Coding Demo",
    "section": "",
    "text": "I made a video coding demo (length just under 30 minutes) where I take a time series dataset and apply a variety of different data transformations to it. The coding demo can be watched on this youtube video. This video took an embarrassingly large number of takes due to random interruptions.\nYou can download the R script with all the commands I entered here: DataTransformationVignette.R\nThe data used for this (in case you want to try it) came from the UCI Machine Learning Repository\nYou can also read more about this data on kaggle\nYou can read about the functions used in this coding vignette mode dplyr page on rolling window functions and on the page for the TTR package"
  },
  {
    "objectID": "posts/2024-09-08-Week-3-Info.html",
    "href": "posts/2024-09-08-Week-3-Info.html",
    "title": "Week 3 Info: Data Tidying",
    "section": "",
    "text": "Hello Class, Week 3 has begun. At this point in the course, you should have some familiarity with the tidyverse, its basic functions for data visualization (ggplot) and data transformation (dplyr). So far in this class we have intentionally worked with nice data sets which were in a form ready to analyze using the tidyverse tools. In this week we will discuss what the tidy in tidyverse means and how a concept called tidy data underlies the design philosophy of the entire group of software packages in the tidyverse. Along the way we will learn tools for importing data and tidying it so that it can be analyzed easily with tidyverse tools.\nYou will have a lab assignment titled Tidy Data due next Sunday at midnight.\nFor your reading assignments and more details about this week, head over to Module 3. If you can, complete the reading assignments before the weekly meetup on Wednesday. If you need help with with anything, don’t hesitate to post in slack or contact me directly."
  },
  {
    "objectID": "posts/2024-09-22-Week-5-Info.html",
    "href": "posts/2024-09-22-Week-5-Info.html",
    "title": "Week 5 Info: Data Transformations",
    "section": "",
    "text": "Hello Class,\nToday is the start of week 5, where we will learn about data transformations. Data transformations allow you to view your dataset in a new light, and are key tools for constructing models, performing EDA and visualizations, and eventually building features for machine learning. We will focus on using boolean operations (i.e. logic) to generate new features from existing data, and practice using window functions to transform time series data. Lastly, we will discuss some issues related to dates and times.\nYou will have a lab assignment titled Data Transformations due next Sunday at midnight.\nFor your reading assignments and more details about this week, head over to Module 5. If you can, complete the reading assignments before the weekly meetup on Wednesday. If you need help with with anything, don’t hesitate to post in slack or contact me directly."
  },
  {
    "objectID": "posts/2024-09-04-Meetup-2_Visualizing-Data.html",
    "href": "posts/2024-09-04-Meetup-2_Visualizing-Data.html",
    "title": "Meetup 3: Tidy Data",
    "section": "",
    "text": "Meetup 3 will cover basic principles of data visualization. Click here to join the meetup on Zoom.\nYou can find the meetup slides here\nClick the Module 3 link to see the course materials and instructions for this week."
  },
  {
    "objectID": "posts/2024-10-09-Meetup-7-R-And-SQL.html",
    "href": "posts/2024-10-09-Meetup-7-R-And-SQL.html",
    "title": "Meetup 7: R and SQL",
    "section": "",
    "text": "Meetup 7 will cover joins and maybe SQL if we have time. Click here to join the meetup on Zoom.\nYou can find the meetup slides here\nClick the Module 7 link to see the course materials and instructions for this week."
  },
  {
    "objectID": "posts/2024-08-28-Meetup-1-Data-Science-Workflow-And-Toolkit.html",
    "href": "posts/2024-08-28-Meetup-1-Data-Science-Workflow-And-Toolkit.html",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "",
    "text": "Welcome to the start of the new semester! Tonight is the first class meetup. Click here to join the meetup on Zoom.\nYou can find the meetup slides here as well as a link to the electricity and carbon dixoide emission vignette if you want to experiment with it.\nClick the Module 1 link to see the course materials and instructions for this week."
  },
  {
    "objectID": "posts/2024-10-09-Coding-Vignette-Joins-Batting.html",
    "href": "posts/2024-10-09-Coding-Vignette-Joins-Batting.html",
    "title": "Coding Vignette: Joins and Lahman Batting Database",
    "section": "",
    "text": "I made a short coding demo (length about 20 minutes) where I use the Lahman batting dataset to demonstrate how to do the most important types of joins.\nClick here to download the code for the vignette\nYou can watch the video on youtube youtube by clicking here."
  },
  {
    "objectID": "posts/2024-09-16-Week-4-Info.html",
    "href": "posts/2024-09-16-Week-4-Info.html",
    "title": "Week 4 Info: Exploratory Data Analysis",
    "section": "",
    "text": "Hello Class,\nToday is the start of week 4, which has as its main topic something called exploratory data analysis. Exploratory data analysis is set of techniques applied at the beginning of a statistical analysis to find errors in the data, to discover patterns, and to explore hypotheses. EDA uses visualizations as one of its primary tools, and we will deepen our knowledge of how to make visualizations in ggplot.\nYou will have a lab assignment titled Exploratory Data Analysis due next Sunday at midnight.\nFor your reading assignments and more details about this week, head over to Module 4. If you can, complete the reading assignments before the weekly meetup on Wednesday. If you need help with with anything, don’t hesitate to post in slack or contact me directly."
  },
  {
    "objectID": "posts/2024-08-28-Welcome_to_DATA_607.html",
    "href": "posts/2024-08-28-Welcome_to_DATA_607.html",
    "title": "Welcome to DATA 607",
    "section": "",
    "text": "Welcome to DATA 607, Data Acquisition and Management.\nClick this post now to get started!\nHere are the first steps:\n\nRead the syllabus.\nGo through the course overview materials, and complete the week 1 readings.\nDownload, install, and configure R, then RStudio.\nSign up for a (free) GitHub account.\nIntroduce yourself on the course discussion forum on Brightspace.\nJoin our Slack channel:\n\nAttend our first meetup on Wednesday at 6:45 p.m. ET on Zoom\nUse the google doodle poll to sign up for your Data Science in Context presentation\n\nI recorded a short video which introduces me and gives you more background on the course:"
  },
  {
    "objectID": "posts/2024-10-16-Meetup-8-Functions-And-Iteration.html",
    "href": "posts/2024-10-16-Meetup-8-Functions-And-Iteration.html",
    "title": "Meetup 8: Functions and Iteration",
    "section": "",
    "text": "Meetup 8 will cover joins and maybe SQL if we have time. Click here to join the meetup on Zoom.\nYou can find the meetup slides here\nClick the Module 8 link to see the course materials and instructions for this week."
  },
  {
    "objectID": "posts/2024-10-11-Introduction-to-Databases.html",
    "href": "posts/2024-10-11-Introduction-to-Databases.html",
    "title": "Introduction to Databases",
    "section": "",
    "text": "I made a video (length about 35 minutes) where I explain what a database is and when you might want to use one, introduce the basics of SQL, and go how you can write (and learn about) SQL queries using the dbplyr library in R. This is more of a lecture than a coding vignette, though I will show plenty of code examples.\nYou can watch the video on youtube youtube by clicking here.\nIf you want to look at the individual slides, click here"
  },
  {
    "objectID": "modules/module4.html",
    "href": "modules/module4.html",
    "title": "Module 4 - Exploratory Data Analysis",
    "section": "",
    "text": "Learning Objectives\n\nUnderstand how Exploratory Data Analysis (EDA) informs Data Preparation and Modeling\nVisualizing variation and covariation for categorical and numerical data\nData checks, data cleaning, outliers, missing data summary statistics\nLayers and Stats in ggplot\nReproducible Examples\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 8-11 (Chapter 10 is the most substantial part of the reading assignment)\n\n\n\nAdditional Reading:\nThere are several classic texts on Exploratory Data Analysis. These are somewhat dated but contain important insights:\n\nExploratory Data Analysis. J. W. Tukey. (1977).\nVisualizing Data. W. S. Cleveland. (1993). Hobart press.\nExploratory data mining and data cleaning. T. Dasu and T. Johnson. (2003). John Wiley & Sons.\n\nHere is an example of a newer book which is also excellent:\n\nExploratory Data Analysis Using R. Roland Pearson. CRC Press. (2015).\n\n\n\nVideos\n\nWhole Game Live demo of an Exploratory Data Analysis by Hadley Wickham. This isn’t a super polished video but its really nice to see examples of experts going through their process. The code he typed and data that he used can be found on github. This video has more of a focus on exploring and finding patterns rather than finding problems in a poor dataset.",
    "crumbs": [
      "Topics",
      "4 - Advanced Visualizations and EDA"
    ]
  },
  {
    "objectID": "modules/module13.html",
    "href": "modules/module13.html",
    "title": "Module 13 - Large Data and Distributed Computing",
    "section": "",
    "text": "Learning Objectives\n\nBasic Introduction to Computer Architecture\nWrangling large data sets using data.table\nCloud and Parrallel computing models\nUsing R and Apache Spark together with sparklyr\n\n\n\nReadings\n\ndata.table vignette\nAdvanced R: Why is R slow?\n\n\n\nAdditional Resources:\n\n\nVideos",
    "crumbs": [
      "Topics",
      "13 - Big Data and Cloud Computing"
    ]
  },
  {
    "objectID": "modules/module2.html",
    "href": "modules/module2.html",
    "title": "Module 2 - Data Vizualization and Basic Transformations",
    "section": "",
    "text": "Overview and Deliverables\nThis module focuses on visualization, which is not only one of the first things you when faced with a new dataset but also the primary way that communicate quantitative information with other people. We will discuss principles of good visualizations and how to use ggplot2 for combinations of numerical and categorical data. ggplot2 is based on something called the grammar of graphics, which you will become familiar with as we build plots. Your first homework assignment, Lab 1, will be due at the end of this week. Your reading will also cover some of the basics of using R which might be a review if you already are an experienced user. Here are your deliverables:\n\n9/04: Attend the class meetup (finish the reading first if you can)\n9/06: Submit meetup reflections\n9/08: Submit Lab 1 on the Brightspace page for the course\n\n\n\nLearning Objectives\n\nData Visualization for Numerical and Categorical Variables with ggplot2\nLearn principles of good visualizations\ntidyverse code conventions\nBasic Data Transofrmations with dplyr\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 1-4\n\n\n\nAdditional Resources:\n\nCalling BS: Principle of Proportional Ink\nCalling BS: Misleading Axes\n\nTwo good articles on making honest visualizations and identifying dishonest ones.\n\nFundamentals of Data Visualization. Claus O. Wilkey. (2019). O’Reilly.\n\nThis is my favorite modern book on data visualization, and it also uses ggplot2 as the primary tool. If you are uncertain about what visualizations might be appropriate for a given problem, this is a good place to check for inspiration. The intended audience is scientists but it is equally relevant for people working in other industries. Chapters 1-5 and Chapter 29 would be good additional readings if you wanted to go a little more in depth.\n\nData Visualization: A Practical Introduction. Kieran Healey.\n\nWell written book on R and data visualization. This is a good place to turn if you are struggling with the style of the main course textbook.\n\nA Layered Grammar of Graphics. Hadley Wickham. Journal of computational and graphical statistics 19.1 (2010): 3-28.\n\nThis paper explains the concepts and thinking behind ggplot2.\n\nggplot2 website. This is another excellent source of resources and information on ggplot2",
    "crumbs": [
      "Topics",
      "2 - Visualizing Data"
    ]
  },
  {
    "objectID": "modules/module10.html",
    "href": "modules/module10.html",
    "title": "Module 10 - Git and Collaboration",
    "section": "",
    "text": "Learning Objectives\n\nBasic github commands\nGithub workflows and Branching\n\n\n\nReadings\n\nHappy Git and GitHub for the useR : Sections 15-23\n\n\n\nAdditional Resources:\nSoftware Carpentries git tutorial",
    "crumbs": [
      "Topics",
      "10 - GIT and Collaboration"
    ]
  },
  {
    "objectID": "modules/module8.html",
    "href": "modules/module8.html",
    "title": "Module 8 - Advanced R Programming",
    "section": "",
    "text": "Learning Objectives\n\nDifferent function types and reusing code\nIteration and flow control with purrr\nApplications of iteration to data processing\nLearn important concepts from base R\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 25-27\n\nThe most important R package we will introduce this week is called purrr, which you can read more about by clicking here\n\n\nAdditional Resources:\n\nAdvanced R Programming Chapter on Functional Programming",
    "crumbs": [
      "Topics",
      "8 - Advanced R Programming"
    ]
  },
  {
    "objectID": "modules/module11.html",
    "href": "modules/module11.html",
    "title": "Module 11 - Tidy Text and NLP",
    "section": "",
    "text": "Learning Objectives\n\nThe tidy text format\nLexicons and sentiment analysis\nWord frequency analysis and Zipf’s Law\nn-grams and correlations\n\n\n\nReadings\n\nText Mining with R: Chapters 1-4\n\n\n\nVideos\n\nVideo Intro to Sentiment Analysis",
    "crumbs": [
      "Topics",
      "11 - Tidy Text and NLP"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "",
    "text": "Creative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible."
  },
  {
    "objectID": "license.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "license.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\n\n__Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\n\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\n\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\n\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.t stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#this-week",
    "href": "meetups/Meetup8/Meetup8.html#this-week",
    "title": "Meetup 8: Functions and Iteration",
    "section": "This week",
    "text": "This week\n\nBehind on grading….\nWork on your project proposals, reach out if you want my opinion\nToday:\n\nFunctions\nIterations\n\nVignettes will be synced with next week’s material"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#case-study-functions-make-your-code-simpler",
    "href": "meetups/Meetup8/Meetup8.html#case-study-functions-make-your-code-simpler",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Case Study: Functions Make Your Code Simpler",
    "text": "Case Study: Functions Make Your Code Simpler\n\nI want to make a statistical model of how the ratio of phosphorus to carbon in marine phytoplankton varies with environmental conditions\nI have a mathematical model for what phytoplankton do at different levels of temperature, nutrients, and environments.\nThere are some unknown parameters, use data to constrain them"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#case-study-statistical-model",
    "href": "meetups/Meetup8/Meetup8.html#case-study-statistical-model",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Case Study: Statistical Model",
    "text": "Case Study: Statistical Model\n\nEnvironmental vars: \\(Temp\\), \\(Irr\\), \\(NO3\\), \\(PO4\\),\nParameters \\(pars\\)\nerror \\(\\sigma\\)\nFunction PC that I created (could be linear regression, a neural network, anything, my case something fancy)\nUse Maximum Likelihood or a Bayesian model with priors on parameters: \\[ \\mathrm{P:C} \\sim \\mathrm{lognormal}(PC(Temp,Irr,NO3,PO4,pars),\\sigma)\\]\nWhat does this look like in code?"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#defining-functions",
    "href": "meetups/Meetup8/Meetup8.html#defining-functions",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nFunction has several parts:\n\n\nclean_number = function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  if_else(is_pct, num / 100, num)\n}"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#defining-functions-1",
    "href": "meetups/Meetup8/Meetup8.html#defining-functions-1",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nFunction has several parts:\nName\n\n\nclean_number = function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  if_else(is_pct, num / 100, num)\n}\n\n\nFunction name defined on first line"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#defining-functions-2",
    "href": "meetups/Meetup8/Meetup8.html#defining-functions-2",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nFunction has several parts:\nArguments\n\n\nclean_number = function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  if_else(is_pct, num / 100, num)\n}\n\n\nArguments are external data passed to function"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#defining-functions-3",
    "href": "meetups/Meetup8/Meetup8.html#defining-functions-3",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nFunction has several parts:\nBody\n\n\nclean_number = function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  if_else(is_pct, num / 100, num)\n}\n\n\nBody of function performs computations"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#defining-functions-4",
    "href": "meetups/Meetup8/Meetup8.html#defining-functions-4",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nFunction has several parts:\nOutput\n\n\nclean_number = function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  if_else(is_pct, num / 100, num)\n}\n\n\nLast line is output"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#defining-functions-5",
    "href": "meetups/Meetup8/Meetup8.html#defining-functions-5",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nFunction has several parts:\nOutput\n\n\nclean_number = function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  return(if_else(is_pct, num / 100, num))\n}\n\n\nCan use return statement also"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#calling-functions",
    "href": "meetups/Meetup8/Meetup8.html#calling-functions",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Calling Functions:",
    "text": "Calling Functions:\n\nclean_number(\"$120,000,000.1\")\n\n[1] 1.2e+08\n\nvec = c(\"3.1415\",\"200%\",\"-100\")\n\nvec |&gt; clean_number()\n\n[1]    3.1415    2.0000 -100.0000\n\nour_list = list(1,\"1\",\"one\")\n\nour_list |&gt; clean_number()\n\n[1]  1  1 NA\n\nbillboard |&gt; clean_number()\n\n [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n[76] NA NA NA NA"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#what-are-functions-good-for",
    "href": "meetups/Meetup8/Meetup8.html#what-are-functions-good-for",
    "title": "Meetup 8: Functions and Iteration",
    "section": "What are Functions Good For?",
    "text": "What are Functions Good For?\nIf you find yourself repeating code more than a few times, consider turning it into a function\n\nFunctions make code easier to read\nIf you change your code, easier to change it in one place\nFunctions make it easier to avoid bugs, enable testing\nIt is easy to share a function"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#scoping-rules-in-r",
    "href": "meetups/Meetup8/Meetup8.html#scoping-rules-in-r",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Scoping Rules in R",
    "text": "Scoping Rules in R\n\nsin = 1\n\nsin\n\n[1] 1\n\nsin(0)\n\n[1] 0\n\nz=1\nmyFunc = function(){\n  print(z)\n  z = 5\n  print(z)\n  \n}\nmyFunc()\n\n[1] 1\n[1] 5\n\nprint(z)\n\n[1] 1"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#scoping-environments",
    "href": "meetups/Meetup8/Meetup8.html#scoping-environments",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Scoping: Environments",
    "text": "Scoping: Environments\n\nR variables and functions live in nested environments\nfunctions (and some flow control) create inward scope\n\n\nz = 1\nf = function(x,y){\n  x*y + z\n}\n\nf(0,1)\n\n[1] 1\n\n\n\nR will look for variable names starting at inner scope and then moving outward"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#more-scope",
    "href": "meetups/Meetup8/Meetup8.html#more-scope",
    "title": "Meetup 8: Functions and Iteration",
    "section": "More Scope",
    "text": "More Scope\n\nf = function(x){\n  x + a\n}\n\na=0\nf(1)\n\n[1] 1\n\na=1\nf(1)\n\n[1] 2"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#side-effects",
    "href": "meetups/Meetup8/Meetup8.html#side-effects",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Side Effects",
    "text": "Side Effects\n\nLet’s say you really want your function to change something outside function scope\nUse &lt;&lt;- operator:\n\n\nz = 10\nside_effect = function(x){\n  z &lt;&lt;- x^2\n  return()\n}\nside_effect(-9)\n\nNULL\n\nz\n\n[1] 81\n\n\n\nUse sparingly, has side effect of making your code difficult to understand and overly interconnected"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#data-masking",
    "href": "meetups/Meetup8/Meetup8.html#data-masking",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Data Masking",
    "text": "Data Masking\n\nWhat if we try to make a function that takes a data frame?\n\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by(group_var) |&gt; \n    summarize(mean(mean_var))\n}\n\n\ndiamonds |&gt; grouped_mean(cut, carat)\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `group_var` is not found.\n\n\n\nWhen group_by and summarize are called, they treat group_var and mean_var literally as names of of variables to search for in data\nCalled data masking because real values of group_var and mean_var ignored"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#data-masking-1",
    "href": "meetups/Meetup8/Meetup8.html#data-masking-1",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Data Masking",
    "text": "Data Masking\n\nWe can see that the real values are ignored:\n\n\ndf &lt;- tibble(\n  mean_var = 1,\n  group_var = \"g\",\n  group = 1,\n  x = 10,\n  y = 100\n)\n\ndf |&gt; grouped_mean(group, x)\n\n# A tibble: 1 × 2\n  group_var `mean(mean_var)`\n  &lt;chr&gt;                &lt;dbl&gt;\n1 g                        1\n\ndf |&gt; grouped_mean(group, y)\n\n# A tibble: 1 × 2\n  group_var `mean(mean_var)`\n  &lt;chr&gt;                &lt;dbl&gt;\n1 g                        1\n\n\n\nIt didn’t matter what the argument names we picked were"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#indirection",
    "href": "meetups/Meetup8/Meetup8.html#indirection",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Indirection",
    "text": "Indirection\n\nFundamental Theorem of Software Engineering:\n“We can solve any problem by introducing an extra layer of indirection”\n\n\nAn indirection or reference is a way to refer to something using a name, reference, or container, instead of the value itself\nR expects name to be typed in\nembrace the variable: filter(df, {{var}} == cond)"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#embracing-in-action",
    "href": "meetups/Meetup8/Meetup8.html#embracing-in-action",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Embracing in Action",
    "text": "Embracing in Action\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by({{group_var}}) |&gt; \n    summarize(mean({{mean_var}}))\n}\n\n\ndiamonds |&gt; grouped_mean(cut, carat)\n\n# A tibble: 5 × 2\n  cut       `mean(carat)`\n  &lt;ord&gt;             &lt;dbl&gt;\n1 Fair              1.05 \n2 Good              0.849\n3 Very Good         0.806\n4 Premium           0.892\n5 Ideal             0.703"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#data-maskingtidy-selection",
    "href": "meetups/Meetup8/Meetup8.html#data-maskingtidy-selection",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Data Masking/Tidy Selection",
    "text": "Data Masking/Tidy Selection\n\nlibrary(nycflights13)\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by({{ group_vars }}) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n    )\n}\n\n\nflights |&gt; \n  count_missing(c(year,month,day), dep_time)\n\nError in `group_by()`:\nℹ In argument: `c(year, month, day)`.\nCaused by error:\n! `c(year, month, day)` must be size 336776 or 1, not 1010328."
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#pick-lets-you-tidy-select",
    "href": "meetups/Meetup8/Meetup8.html#pick-lets-you-tidy-select",
    "title": "Meetup 8: Functions and Iteration",
    "section": "pick lets you tidy select",
    "text": "pick lets you tidy select\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n    )\n}\n\nflights |&gt; \n  count_missing(c(year,month,day), dep_time) |&gt; head(6)\n\n# A tibble: 6 × 4\n   year month   day n_miss\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1  2013     1     1      4\n2  2013     1     2      8\n3  2013     1     3     10\n4  2013     1     4      6\n5  2013     1     5      3\n6  2013     1     6      1"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#functional-programming",
    "href": "meetups/Meetup8/Meetup8.html#functional-programming",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Functional Programming",
    "text": "Functional Programming\n\nFunctions are first class objects in R:\n\n\npower &lt;- function(g,exponent) {\n  function(x) {\n    g(x) ^ exponent\n  }\n}\n\n\nf = power(cos,2)\nf(-3.14159)\n\n[1] 1\n\n\n\nStyle of R looks more functional than other languages\npurrr library provides functional programming tools"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#across-iterates-on-columns",
    "href": "meetups/Meetup8/Meetup8.html#across-iterates-on-columns",
    "title": "Meetup 8: Functions and Iteration",
    "section": "across iterates on columns",
    "text": "across iterates on columns\nSuppose we want to apply an operation to many columns at once:\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf\n\n# A tibble: 10 × 4\n        a      b      c      d\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -1.92  -0.360  1.45  -0.710\n 2  0.347  1.40  -0.833 -0.796\n 3 -0.878 -1.47   0.112 -0.160\n 4 -0.851 -0.116  0.271  0.187\n 5 -0.181 -0.416  1.77  -1.26 \n 6 -0.723  0.614  1.00  -0.281\n 7  1.37  -1.14   1.39   0.344\n 8 -0.829  0.603  0.409 -0.610\n 9  0.896  1.11   0.686 -1.39 \n10  1.62  -0.691  0.486  1.70"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#across-iterates-on-columns-1",
    "href": "meetups/Meetup8/Meetup8.html#across-iterates-on-columns-1",
    "title": "Meetup 8: Functions and Iteration",
    "section": "across iterates on columns",
    "text": "across iterates on columns\n\nCount the elements and compute median:\n\n\ndf |&gt; summarise(\n  n = n(),\n  a = mean(a),\n  b = mean(b),\n  c = mean(c),\n  d = mean(d)\n)\n\n# A tibble: 1 × 5\n      n      a       b     c      d\n  &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1    10 -0.115 -0.0460 0.674 -0.297\n\n\n\nInvolves lots of copy/pasting\nDoesn’t scale well with number of columns"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#across-iterates-on-columns-2",
    "href": "meetups/Meetup8/Meetup8.html#across-iterates-on-columns-2",
    "title": "Meetup 8: Functions and Iteration",
    "section": "across iterates on columns",
    "text": "across iterates on columns\n\nAlternative is function across\npart of purrr functional programming library\nArguments: function and selection of columns:\n\n\ndf |&gt; summarise(\n  n = n(),\n  across(a:d,mean)\n)\n\n# A tibble: 1 × 5\n      n      a       b     c      d\n  &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1    10 -0.115 -0.0460 0.674 -0.297\n\n\n\nCan use other selects (i.e. starts_with, everything)"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#anonymous-functions-in-across",
    "href": "meetups/Meetup8/Meetup8.html#anonymous-functions-in-across",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Anonymous functions in across",
    "text": "Anonymous functions in across\n\nHow to pass additional arguments to functions?\nAnonymous (or lambda functions), functions not important enough to be named:\n\n\n\\(x) median(x^2 * exp(-x^2)) \n\nfunction (x) \nmedian(x^2 * exp(-x^2))"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#anonymous-functions-in-across-1",
    "href": "meetups/Meetup8/Meetup8.html#anonymous-functions-in-across-1",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Anonymous functions in across",
    "text": "Anonymous functions in across\n\nCan use this to pass arguments:\n\n\ndf |&gt; summarise(\n  n = n(),\n  across(a:d, \\(x) mean(x,na.rm=TRUE))\n)\n\n# A tibble: 1 × 5\n      n      a       b     c      d\n  &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1    10 -0.115 -0.0460 0.674 -0.297"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#map-family-in-purrr",
    "href": "meetups/Meetup8/Meetup8.html#map-family-in-purrr",
    "title": "Meetup 8: Functions and Iteration",
    "section": "map family in purrr",
    "text": "map family in purrr\n\nmap(vec,f) = [f(vec[1]),f(vec[2]),....] \n\n\nmap(1:3,exp)\n\n[[1]]\n[1] 2.718282\n\n[[2]]\n[1] 7.389056\n\n[[3]]\n[1] 20.08554"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#map_",
    "href": "meetups/Meetup8/Meetup8.html#map_",
    "title": "Meetup 8: Functions and Iteration",
    "section": "map_*",
    "text": "map_*\n\nmap_dbl, map_chr, map_lgl, map_int: assume data type and return vectors\nOperate on data frames \nImagine the data frame has been rotated so the “rows” correspond to columns"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#map_-1",
    "href": "meetups/Meetup8/Meetup8.html#map_-1",
    "title": "Meetup 8: Functions and Iteration",
    "section": "map_*",
    "text": "map_*\n\nmtcars |&gt; head(8)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n\nmtcars |&gt; map_dbl(\\(x) length(unique(x)))\n\n mpg  cyl disp   hp drat   wt qsec   vs   am gear carb \n  25    3   27   22   22   29   30    2    2    3    6 \n\n\n\nYour function must return a single function of the same type as the map function"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#where-are-the-for-loops",
    "href": "meetups/Meetup8/Meetup8.html#where-are-the-for-loops",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Where are the for loops?",
    "text": "Where are the for loops?\n\nfor and while loops exist in R\nThey are discouraged in favor of tools like map\nOnly use them when you must (such as when you need side effects)\n\n\nfor (element in vector){\n  func(element) # Run some code that uses element\n}"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#while-loop-in-action",
    "href": "meetups/Meetup8/Meetup8.html#while-loop-in-action",
    "title": "Meetup 8: Functions and Iteration",
    "section": "while loop in action",
    "text": "while loop in action\n\ncounter = 0\nwhile (rnorm(1) &lt; 2) {\n  counter = counter + 1\n  \n}\n\ncounter\n\n[1] 10"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#data-science-in-context-presentation",
    "href": "meetups/Meetup8/Meetup8.html#data-science-in-context-presentation",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Data Science in Context Presentation",
    "text": "Data Science in Context Presentation"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup8/Meetup8.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#lab-5-review",
    "href": "meetups/Meetup7/Meetup7.html#lab-5-review",
    "title": "Meetup 7: Joins and SQL",
    "section": "Lab 5 Review",
    "text": "Lab 5 Review\n\nHave graded all labs that were handed in last Sunday evening.\nMostly really good, most common mistake was string typos\n\n\ngss_cat |&gt; select(partyid) |&gt; distinct()\n\n# A tibble: 10 × 1\n   partyid           \n   &lt;fct&gt;             \n 1 Ind,near rep      \n 2 Not str republican\n 3 Independent       \n 4 Not str democrat  \n 5 Strong democrat   \n 6 Ind,near dem      \n 7 Strong republican \n 8 Other party       \n 9 No answer         \n10 Don't know"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#lab-5-review-1",
    "href": "meetups/Meetup7/Meetup7.html#lab-5-review-1",
    "title": "Meetup 7: Joins and SQL",
    "section": "Lab 5 Review",
    "text": "Lab 5 Review\n\nHave graded all labs that were handed in last Sunday evening.\nMostly really good, most common mistake was string typos\nTypos with these categories lead to errors in plots\n\nSome should have been red flags (plot with only 2 categories)\nOthers more subtle (numbers off)\nAlways a good idea to look at intermediate output, doublecheck things"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#very-subtle-regex-issues-on-lab-5",
    "href": "meetups/Meetup7/Meetup7.html#very-subtle-regex-issues-on-lab-5",
    "title": "Meetup 7: Joins and SQL",
    "section": "Very Subtle Regex Issues on Lab 5",
    "text": "Very Subtle Regex Issues on Lab 5\n\nYour book (and me also) were a bit uncareful:\n\"[^aeoiu]\" only identifies consonants if the input is purely a word\nThough the book is right in a very technical sense, props to all who noticed this and identified all the consonants"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#project-proposal",
    "href": "meetups/Meetup7/Meetup7.html#project-proposal",
    "title": "Meetup 7: Joins and SQL",
    "section": "Project Proposal",
    "text": "Project Proposal\n\nNo lab next week\nBut project proposal due October 20th\nTarget 2 Pages\n\nIntro\nData\nAnalysis Plan\n\nIdea: Demonstrate full data science workflow\nEmphasis on everything but modeling\nFeel free to ask me about your idea as you develop it"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#this-week",
    "href": "meetups/Meetup7/Meetup7.html#this-week",
    "title": "Meetup 7: Joins and SQL",
    "section": "This Week",
    "text": "This Week\n\nTwo main topics: Joins and SQL\nSeparate video for SQL with a coding vignette"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#combining-tables-from-multiple-tables",
    "href": "meetups/Meetup7/Meetup7.html#combining-tables-from-multiple-tables",
    "title": "Meetup 7: Joins and SQL",
    "section": "Combining Tables From Multiple Tables",
    "text": "Combining Tables From Multiple Tables\n\nData usually stored in separate files/data frames\nAnalysis requires data in a single frame\nNeed tools to combine data-frames"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13-1",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13-1",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13\n\nflights |&gt; select(time_hour,carrier,dep_time) |&gt; head(8) |&gt; kable()\n\n\n\n\ntime_hour\ncarrier\ndep_time\n\n\n\n\n2013-01-01 05:00:00\nUA\n517\n\n\n2013-01-01 05:00:00\nUA\n533\n\n\n2013-01-01 05:00:00\nAA\n542\n\n\n2013-01-01 05:00:00\nB6\n544\n\n\n2013-01-01 06:00:00\nDL\n554\n\n\n2013-01-01 05:00:00\nUA\n554\n\n\n2013-01-01 06:00:00\nB6\n555\n\n\n2013-01-01 06:00:00\nEV\n557"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13-2",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13-2",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13\n\nplanes |&gt; select(tailnum,year,model) |&gt; head(8) |&gt; kable()\n\n\n\n\ntailnum\nyear\nmodel\n\n\n\n\nN10156\n2004\nEMB-145XR\n\n\nN102UW\n1998\nA320-214\n\n\nN103US\n1999\nA320-214\n\n\nN104UW\n1999\nA320-214\n\n\nN10575\n2002\nEMB-145LR\n\n\nN105UW\n1999\nA320-214\n\n\nN107US\n1999\nA320-214\n\n\nN108UW\n1999\nA320-214"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13-3",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13-3",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13\n\nairports |&gt; select(faa,name,lat,lon) |&gt; head(8) |&gt; kable()\n\n\n\n\nfaa\nname\nlat\nlon\n\n\n\n\n04G\nLansdowne Airport\n41.13047\n-80.61958\n\n\n06A\nMoton Field Municipal Airport\n32.46057\n-85.68003\n\n\n06C\nSchaumburg Regional\n41.98934\n-88.10124\n\n\n06N\nRandall Airport\n41.43191\n-74.39156\n\n\n09J\nJekyll Island Airport\n31.07447\n-81.42778\n\n\n0A9\nElizabethton Municipal Airport\n36.37122\n-82.17342\n\n\n0G6\nWilliams County Airport\n41.46731\n-84.50678\n\n\n0G7\nFinger Lakes Regional Airport\n42.88356\n-76.78123"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13-4",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13-4",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13\n\nCan combine data frames along rows that share matching values of a common variable:\n\n\n\n\nflights |&gt; \n  select(dest,origin) |&gt; \n  head(6) |&gt;\n  kable()\n\n\n\n\ndest\norigin\n\n\n\n\nIAH\nEWR\n\n\nIAH\nLGA\n\n\nMIA\nJFK\n\n\nBQN\nJFK\n\n\nATL\nLGA\n\n\nORD\nEWR\n\n\n\n\n\n\n\nairports |&gt; \n  select(faa) |&gt; \n  head(8) |&gt; \n  kable()\n\n\n\n\nfaa\n\n\n\n\n04G\n\n\n06A\n\n\n06C\n\n\n06N\n\n\n09J\n\n\n0A9\n\n\n0G6\n\n\n0G7\n\n\n\n\n\n\n\nColumns which connect different data frames are Keys"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13-5",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13-5",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13\n\nCan combine data frames along rows that share matching values of a common variable:\n\n\n\n\nflights |&gt; \n  select(dest,origin) |&gt; \n  head(6) |&gt;\n  kable()\n\n\n\n\ndest\norigin\n\n\n\n\nIAH\nEWR\n\n\nIAH\nLGA\n\n\nMIA\nJFK\n\n\nBQN\nJFK\n\n\nATL\nLGA\n\n\nORD\nEWR\n\n\n\n\n\n\n\nairports |&gt; \n  select(faa) |&gt; \n  head(8) |&gt; \n  kable()\n\n\n\n\nfaa\n\n\n\n\n04G\n\n\n06A\n\n\n06C\n\n\n06N\n\n\n09J\n\n\n0A9\n\n\n0G6\n\n\n0G7\n\n\n\n\n\n\n\nColumns which connect different data frames are Keys"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13-6",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13-6",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13\n\nCan combine data frames along rows that share matching values of a common variable:\n\n\n\n\nflights |&gt; \n  select(tailnum) |&gt; \n  head(6) |&gt;\n  kable()\n\n\n\n\ntailnum\n\n\n\n\nN14228\n\n\nN24211\n\n\nN619AA\n\n\nN804JB\n\n\nN668DN\n\n\nN39463\n\n\n\n\n\n\n\nplanes |&gt; \n  select(tailnum) |&gt; \n  head(8) |&gt; \n  kable()\n\n\n\n\ntailnum\n\n\n\n\nN10156\n\n\nN102UW\n\n\nN103US\n\n\nN104UW\n\n\nN10575\n\n\nN105UW\n\n\nN107US\n\n\nN108UW\n\n\n\n\n\n\n\nColumns that are common “Keys” used to link data frames"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#primary-keys",
    "href": "meetups/Meetup7/Meetup7.html#primary-keys",
    "title": "Meetup 7: Joins and SQL",
    "section": "Primary Keys",
    "text": "Primary Keys\nA Primary Key for a data frame is a column group of columns within that data frame whose values uniquely determine each row of the data frame.\n\nFor planes it is tailnum\n\n\nplanes |&gt; select(tailnum, year, type, model) |&gt; head(5) |&gt; kable()\n\n\n\n\ntailnum\nyear\ntype\nmodel\n\n\n\n\nN10156\n2004\nFixed wing multi engine\nEMB-145XR\n\n\nN102UW\n1998\nFixed wing multi engine\nA320-214\n\n\nN103US\n1999\nFixed wing multi engine\nA320-214\n\n\nN104UW\n1999\nFixed wing multi engine\nA320-214\n\n\nN10575\n2002\nFixed wing multi engine\nEMB-145LR"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#definition",
    "href": "meetups/Meetup7/Meetup7.html#definition",
    "title": "Meetup 7: Joins and SQL",
    "section": "Definition",
    "text": "Definition\nA Primary Key for a data frame is a column group of columns within that data frame whose values uniquely determine each row of the data frame.\n\nFor planes it is tailnum\n\n\nplanes |&gt; \n  count(tailnum) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: tailnum &lt;chr&gt;, n &lt;int&gt;\n\n\n\nThis count command shows that each row has a unique value of tailnum proving that it is a primary key"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#compound-keys",
    "href": "meetups/Meetup7/Meetup7.html#compound-keys",
    "title": "Meetup 7: Joins and SQL",
    "section": "Compound Keys",
    "text": "Compound Keys\n\nSometimes a single variable is not enough to uniquely identify rows.\nMultiple variables can combine to be a primary key\nFor weather it requires origin and time-hour:\n\n\nweather |&gt; count(time_hour) |&gt; filter(n&gt;1) |&gt; nrow()\n\n[1] 8706\n\nweather |&gt; \n  count(origin,time_hour) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: origin &lt;chr&gt;, time_hour &lt;dttm&gt;, n &lt;int&gt;"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#foreign-keys",
    "href": "meetups/Meetup7/Meetup7.html#foreign-keys",
    "title": "Meetup 7: Joins and SQL",
    "section": "Foreign Keys",
    "text": "Foreign Keys\nA Foreign Key is a variable or group of variables within a data frame that serves as a primary key for another data frame.\n\nflights |&gt; select(flight,time_hour,tailnum) |&gt; head(4) |&gt; kable()\n\n\n\n\nflight\ntime_hour\ntailnum\n\n\n\n\n1545\n2013-01-01 05:00:00\nN14228\n\n\n1714\n2013-01-01 05:00:00\nN24211\n\n\n1141\n2013-01-01 05:00:00\nN619AA\n\n\n725\n2013-01-01 05:00:00\nN804JB\n\n\n\n\n\n\ntailnum is a primary key for planes\nForeign key is what allows for rows to be matched between data frames"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#joins",
    "href": "meetups/Meetup7/Meetup7.html#joins",
    "title": "Meetup 7: Joins and SQL",
    "section": "Joins",
    "text": "Joins\n\nJoins are functions that combine two data frames\n\nsomething_join(x,y,by = join_by(...))\n\nHere something is either: -left, right, full, inner, semi, or anti"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#join-types",
    "href": "meetups/Meetup7/Meetup7.html#join-types",
    "title": "Meetup 7: Joins and SQL",
    "section": "Join Types",
    "text": "Join Types\nTwo main “types” of joins:\n\nMutating Joins:\n\nCombine data between data frames\nleft_join, right_join, full_join, inner_join\n\nFiltering Joins:\n\nFilter rows based on matches\nsemi_join, anti_join"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#mutating-joins",
    "href": "meetups/Meetup7/Meetup7.html#mutating-joins",
    "title": "Meetup 7: Joins and SQL",
    "section": "Mutating Joins",
    "text": "Mutating Joins\n\nleft_join: Keeps all rows of x, matches/adds from y\nright_join: reversed left_join\nfull_join: keeps all rows of both\ninner_join: only rows occurring in both\n\n\nR4DS 19.8"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#left-join",
    "href": "meetups/Meetup7/Meetup7.html#left-join",
    "title": "Meetup 7: Joins and SQL",
    "section": "Left Join",
    "text": "Left Join\n\nStarting Point:\n\n\nR4DS 19.2"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#left-join-1",
    "href": "meetups/Meetup7/Meetup7.html#left-join-1",
    "title": "Meetup 7: Joins and SQL",
    "section": "Left Join",
    "text": "Left Join\n\nR4DS 19.2\nleft_join(x,y) |&gt; kable()\n\n\n\n\nkey\nval_x\nval_y\n\n\n\n\n1\nx1\ny1\n\n\n2\nx2\ny2\n\n\n3\nx3\nNA"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#right-join",
    "href": "meetups/Meetup7/Meetup7.html#right-join",
    "title": "Meetup 7: Joins and SQL",
    "section": "Right Join",
    "text": "Right Join\n\nR4DS 19.2\nright_join(x,y) |&gt; kable()\n\n\n\n\nkey\nval_x\nval_y\n\n\n\n\n1\nx1\ny1\n\n\n2\nx2\ny2\n\n\n4\nNA\ny3"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#inner-join",
    "href": "meetups/Meetup7/Meetup7.html#inner-join",
    "title": "Meetup 7: Joins and SQL",
    "section": "Inner Join",
    "text": "Inner Join\n\nR4DS 19.2\ninner_join(x,y) |&gt; kable()\n\n\n\n\nkey\nval_x\nval_y\n\n\n\n\n1\nx1\ny1\n\n\n2\nx2\ny2"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#full-join",
    "href": "meetups/Meetup7/Meetup7.html#full-join",
    "title": "Meetup 7: Joins and SQL",
    "section": "Full Join",
    "text": "Full Join\n\n\n\n\n\nR4DS 19.2\n\n\n\n\nfull_join(x,y) |&gt; kable()\n\n\n\n\nkey\nval_x\nval_y\n\n\n\n\n1\nx1\ny1\n\n\n2\nx2\ny2\n\n\n3\nx3\nNA\n\n\n4\nNA\ny3"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#what-about-keys",
    "href": "meetups/Meetup7/Meetup7.html#what-about-keys",
    "title": "Meetup 7: Joins and SQL",
    "section": "What about Keys?",
    "text": "What about Keys?\n\nJoin functions automatically try to find a foreign key\nYou may specify a key using join_by and then specifying match using formula:\n\n\nflights |&gt; left_join(airports,join_by(dest == faa)) |&gt; \n  select(year:dep_delay) |&gt; head(5)\n\n# A tibble: 5 × 6\n   year month   day dep_time sched_dep_time dep_delay\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n1  2013     1     1      517            515         2\n2  2013     1     1      533            529         4\n3  2013     1     1      542            540         2\n4  2013     1     1      544            545        -1\n5  2013     1     1      554            600        -6\n\n\n\nPotential for multiple matches and duplication when there is no foreign key"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#filtering-joins",
    "href": "meetups/Meetup7/Meetup7.html#filtering-joins",
    "title": "Meetup 7: Joins and SQL",
    "section": "Filtering Joins",
    "text": "Filtering Joins\n\nsemi_join(x,y) gets rid of rows of x that don’t have matches in y\nUse semi_join to restrict airports to the destination airports\n\n\nairports |&gt; \n  semi_join(flights, join_by(faa == origin))\n\n# A tibble: 3 × 8\n  faa   name                  lat   lon   alt    tz dst   tzone           \n  &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;           \n1 EWR   Newark Liberty Intl  40.7 -74.2    18    -5 A     America/New_York\n2 JFK   John F Kennedy Intl  40.6 -73.8    13    -5 A     America/New_York\n3 LGA   La Guardia           40.8 -73.9    22    -5 A     America/New_York"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#filtering-joins-1",
    "href": "meetups/Meetup7/Meetup7.html#filtering-joins-1",
    "title": "Meetup 7: Joins and SQL",
    "section": "Filtering Joins",
    "text": "Filtering Joins\n\nanti_join(x,y) gets rid of rows of x that have matches in y\nUse anti_join to find implicit missing values\nFlights where weather info lacking:\n\n\nflights |&gt; \n  anti_join(weather) |&gt; \n  head(5) |&gt; \n  select(year:dep_delay) |&gt;\n  print()\n\n# A tibble: 5 × 6\n   year month   day dep_time sched_dep_time dep_delay\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n1  2013     1     1     1153           1200        -7\n2  2013     1     1     1154           1200        -6\n3  2013     1     1     1155           1200        -5\n4  2013     1     1     1155           1200        -5\n5  2013     1     1     1157           1200        -3"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#data-science-in-context-presentations",
    "href": "meetups/Meetup7/Meetup7.html#data-science-in-context-presentations",
    "title": "Meetup 7: Joins and SQL",
    "section": "Data Science in Context Presentations",
    "text": "Data Science in Context Presentations"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup7/Meetup7.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 7: Joins and SQL",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#relational-databases",
    "href": "meetups/Meetup7/Meetup7.html#relational-databases",
    "title": "Meetup 7: Joins and SQL",
    "section": "Relational Databases",
    "text": "Relational Databases"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#features",
    "href": "meetups/Meetup7/Meetup7.html#features",
    "title": "Meetup 7: Joins and SQL",
    "section": "Features",
    "text": "Features\n\nTables linked by keys\nTables correspond to entities\nRows called records\nStored on disk\nTake data from SQL to analyze elsewhere"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#trade-offs",
    "href": "meetups/Meetup7/Meetup7.html#trade-offs",
    "title": "Meetup 7: Joins and SQL",
    "section": "Trade Offs",
    "text": "Trade Offs\nWhen should you use a relational database for your project?\n\nIf many people are using the data\nIf the data takes up lots of space\nIf the data has complex organization\nIf you are planning to scale up"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#dbms-classes",
    "href": "meetups/Meetup7/Meetup7.html#dbms-classes",
    "title": "Meetup 7: Joins and SQL",
    "section": "DBMS Classes",
    "text": "DBMS Classes\nUse Database Management Systems to access data\n\nClient-Server: (Most Traditional). Database hosted on a central server to which you connect (IBM, Oracle)\nCloud: Database hosted on cloud. Newer, easier to scale resources (Google, Amazon)\nIn Process: For smaller datasets with few users. Best for data analysis and learning (sqlite, duckdb)"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#normalization-schemes",
    "href": "meetups/Meetup7/Meetup7.html#normalization-schemes",
    "title": "Meetup 7: Joins and SQL",
    "section": "Normalization Schemes",
    "text": "Normalization Schemes\n\nDatabases often organized according to strict rules called normalization\nThese improve things like space efficiency, data consistency, etc\nMassive research topic in the 1970s"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#normalization-schemes-1",
    "href": "meetups/Meetup7/Meetup7.html#normalization-schemes-1",
    "title": "Meetup 7: Joins and SQL",
    "section": "Normalization Schemes",
    "text": "Normalization Schemes\n\nDatabases often organized according to strict rules called normalization\nThese improve things like space efficiency, data consistency, etc\nMassive research topic in the 1970s\nIdeas often involve separating data into tables corresponding to entities\nEach fact stored in one place"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#sql-basics",
    "href": "meetups/Meetup7/Meetup7.html#sql-basics",
    "title": "Meetup 7: Joins and SQL",
    "section": "SQL Basics",
    "text": "SQL Basics\n\nStructured Query Language\nQueries composed of clauses (must be in order):\n\nSELECT, FROM, WHERE, GROUP BY, ORDER BY\nSELECT is combo of mutate, select, rename, summarize, relocate, summarize\nAlso has JOINS\n\ndbplyr translates tidyverse manipulations to SQL\nDBI is the library that lets you open database connections in R"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#lab-4-review",
    "href": "meetups/Meetup6/Meetup6.html#lab-4-review",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Lab 4 Review",
    "text": "Lab 4 Review\n\nParameter Choices in faceted scatter plots\nTry scale = \"free/free_x/free_y\"\n\n\nstocks |&gt; ggplot(aes(x=volume/1e8,y=log_return)) +\n  geom_point(alpha=0.2) +\n  facet_wrap(~symbol,ncol = 3) +\n  labs(x = \"Adj. vol (10^8 shares)\",\n       y = \"log(return)\",\n       title = \"log(return) vs. Vol for Tech Stocks\") +\n  theme_bw(base_size = 24)"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#lab-4-review-1",
    "href": "meetups/Meetup6/Meetup6.html#lab-4-review-1",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Lab 4 Review",
    "text": "Lab 4 Review"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#lab-4-review-2",
    "href": "meetups/Meetup6/Meetup6.html#lab-4-review-2",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Lab 4 Review",
    "text": "Lab 4 Review"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#lab-4-review-3",
    "href": "meetups/Meetup6/Meetup6.html#lab-4-review-3",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Lab 4 Review",
    "text": "Lab 4 Review\n\nstocks |&gt; filter(symbol %in% c(\"META\",\"ADBE\",\"AMZN\",\"MSFT\")) |&gt; \n  ggplot(aes(y=(volume/1e8),x=log_return)) +\n  geom_point(alpha=0.1,size=.1) +\n  facet_wrap(~symbol,ncol = 2, scales = \"free_y\") +\n  labs(y = \"Adj. vol (10^8 shares)\",\n       x = \"log(return)\",\n       title = \"log(return) vs. Vol for Tech Stocks\") +\n  theme_bw(base_size = 24)"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#lab-4-review-4",
    "href": "meetups/Meetup6/Meetup6.html#lab-4-review-4",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Lab 4 Review",
    "text": "Lab 4 Review"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#case-study-huntingtons-disease",
    "href": "meetups/Meetup6/Meetup6.html#case-study-huntingtons-disease",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Case Study: Huntington’s Disease",
    "text": "Case Study: Huntington’s Disease\n\nTandem Repeats are sections of genetic code where same short element repeats over and over:\nCAGCAGCAGCAGCAGCAGCAG\n\n\nIn certain genes, number of repeats varies, can cause genetic diseases etc\nHuntington’s Disease:\n\nCAA and CAG repeats, code for amino acid Glutamine\n6-35 normal\n36+ causes disease"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#pattern-matching",
    "href": "meetups/Meetup6/Meetup6.html#pattern-matching",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Pattern Matching",
    "text": "Pattern Matching\n\nHow to determine number of repeats CAA + CAG repeats in a section of code?\n\n\nlibrary(tidyverse)\nlibrary(kableExtra)\nDNA =read_file(\"Huntington.txt\")\n\nDNA |&gt; str_sub(1, 800) |&gt; print()\n\n[1] \"\\t1 ttgctgtgtg aggcagaacc tgcgggggca ggggcgggct ggttccctgg ccagccattg\\n       61 gcagagtccg caggctaggg ctgtcaatca tgctggccgg cgtggccccg cctccgccgg\\n      121 cgcggccccg cctccgccgg cgcagcgtct gggacgcaag gcgccgtggg ggctgccggg\\n      181 acgggtccaa gatggacggc cgctcaggtt ctgcttttac ctgcggccca gagccccatt\\n      241 cattgccccg gtgctgagcg gcgccgcgag tcggcccgag gcctccgggg actgccgtgc\\n      301 cgggcgggag accgccatgg cgaccctgga aaagctgatg aaggccttcg agtccctcaa\\n      361 gtccttccag cagcagcagc agcagcagca gcagcagcag cagcagcagc agcagcagca\\n      421 gcagcagcag caacagccgc caccgccgcc gccgccgccg ccgcctcctc agcttcctca\\n      481 gccgccgccg caggcacagc cgctgctgcc tcagccgcag ccgcccccgc cgccgccccc\\n      541 gccgccaccc ggcccggctg tggctgagga gccgctgcac cgaccaaaga aagaactttc\\n      601 agctaccaag aaagaccgtg tgaatcattg tctg\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#processing-steps",
    "href": "meetups/Meetup6/Meetup6.html#processing-steps",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Processing Steps",
    "text": "Processing Steps\n\nGet rid of tabs, numbers, spaces, newlines:\n\n\nDNA = DNA |&gt; str_remove_all(\"[0-9]\") |&gt;   str_remove_all(\"\\t\") |&gt; \n  str_remove_all(\"\\n\") |&gt; str_remove_all(\" \")\n\nDNA |&gt; str_sub(1,60) |&gt; print()\n\n[1] \"ttgctgtgtgaggcagaacctgcgggggcaggggcgggctggttccctggccagccattg\"\n\n\n\nFollowing Regular Expression can match repeats of various lengths: (CAA|CAG){10,}\n\n\nDNA |&gt; str_count(\"caa\") |&gt; print()\n\n[1] 179\n\nDNA |&gt; str_count(\"cag\") |&gt; print()\n\n[1] 471\n\nDNA |&gt; str_count(\"(caa|cag){10,}\") |&gt; print()\n\n[1] 1"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#how-long",
    "href": "meetups/Meetup6/Meetup6.html#how-long",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "How Long?",
    "text": "How Long?\n\nDNA |&gt; str_extract_all(\"(caa|cag){10,}\") |&gt; \n  str_length()/3\n\n[1] 23\n\nDNA |&gt; str_extract_all(\"(caa|cag){10,}\")\n\n[[1]]\n[1] \"cagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcaacag\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#other-examples",
    "href": "meetups/Meetup6/Meetup6.html#other-examples",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Other Examples",
    "text": "Other Examples\n\nPowerful tools like stringr and regex elevate your ability to work with text\nUseful in many domains, from identifying emails and phone numbers to motifs in DNA sequences\n\n\"\\\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\\\.[A-Z]{2,}\\\\b\" pattern matches valid email\n\"\\\\d{5}([ \\\\-]\\\\d{4})?\" matches American Zip codes\n\"/([12]\\\\d{3}-(0[1-9]|1[0-2])-(0[1-9]|[12]\\\\d|3[01]))/\" YYYY-MM-DD dates"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#stringcharacter-basics",
    "href": "meetups/Meetup6/Meetup6.html#stringcharacter-basics",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "String/Character Basics",
    "text": "String/Character Basics\n\nDefined by enclosing in quotes:\n\n\nstring1 = c(\"word\",'another word', \"multiple\\n lines\",\n            \"using \\\"escapes\\\" and \\u00FCnicode \")\nstr_view(string1)\n\n[1] │ word\n[2] │ another word\n[3] │ multiple\n    │  lines\n[4] │ using \"escapes\" and ünicode \n\n\n\nSpecial characters:\n\n\"\\\\n\" newline\n\"\\\\t\" tab\n\"\\\\\" to escape and write special character"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#key-functions",
    "href": "meetups/Meetup6/Meetup6.html#key-functions",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Key functions:",
    "text": "Key functions:\n\nstr_c(): adds strings together\nstr_sub(): subsets strings\nstr_flatten() combines chars in a char vector into single string\n\n\nstr_c(\"Abraham\",\" \",\"Lincoln\")\n\n[1] \"Abraham Lincoln\"\n\nstr_sub(c(\"Abraham\",\"Lincoln\"),start=1,end=4)\n\n[1] \"Abra\" \"Linc\"\n\nstr_flatten(letters)\n\n[1] \"abcdefghijklmnopqrstuvwxyz\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#key-functions-1",
    "href": "meetups/Meetup6/Meetup6.html#key-functions-1",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Key functions:",
    "text": "Key functions:\n\nstr_view() underlying string and pattern matches\nstr_length() self explanatory\nstr_count() counts matches with a pattern\nstr_detect() logical match\n\n\nstr_length(\"Grizzly Bear\")\n\n[1] 12\n\nDNA |&gt; str_count(\"caa\") |&gt; print()\n\n[1] 179\n\n\"Lions, Tigers, and Bears\" |&gt; str_view(\"Tiger\")\n\n[1] │ Lions, &lt;Tiger&gt;s, and Bears\n\n\"Lions, Tigers, and Bears\" |&gt; str_detect(\"Cat\")\n\n[1] FALSE"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#key-functions-2",
    "href": "meetups/Meetup6/Meetup6.html#key-functions-2",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Key functions:",
    "text": "Key functions:\n\nstr_replace() and str_replace_all: substitute\nstr_remove() and str_remove_all(): remove matches\nstr_extract() and str_extract_all(): pull out matches\n\n\n\"Abraham Linc0ln\" |&gt; str_replace_all(\"0\",\"o\")\n\n[1] \"Abraham Lincoln\"\n\nc(\"Blueberry\",\"Blackberry\",\"Lingonberry\") |&gt; str_remove(\"berry\")\n\n[1] \"Blue\"   \"Black\"  \"Lingon\"\n\nDNA |&gt; str_extract_all(\"tttttt\")\n\n[[1]]\n[1] \"tttttt\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#regular-expressions-regex",
    "href": "meetups/Meetup6/Meetup6.html#regular-expressions-regex",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Regular Expressions (regex)",
    "text": "Regular Expressions (regex)\n\nRegular Expressions are a powerful language for describing complex patterns\nString search tools often use them by default\nRegular Characters:\na-z, A-z, 0-9\nMetacharacters:\n\n. , * , () , [] , {} , | , \\ , $ , ? , ^ , +\nThese add special meaning to the patterns"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#wildcard-.",
    "href": "meetups/Meetup6/Meetup6.html#wildcard-.",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Wildcard .",
    "text": "Wildcard .\n\n. a symbol that can stand for any character\nFind all instances of “a” followed by two letters of any type, followed by another “a”:\n\n\nwords |&gt; str_view(\"a..a\")\n\n [35] │ &lt;alwa&gt;ys\n [43] │ &lt;appa&gt;rent\n [49] │ &lt;area&gt;\n [53] │ &lt;arra&gt;nge\n [62] │ av&lt;aila&gt;ble\n[598] │ par&lt;agra&gt;ph\n[801] │ st&lt;anda&gt;rd"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#quantifiers",
    "href": "meetups/Meetup6/Meetup6.html#quantifiers",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Quantifiers ?, +, *",
    "text": "Quantifiers ?, +, *\n\nFind patterns that repeat\n? for optional matches:\n\n\nstr = \"eTeeTeeeTeeeeT\"\nstr |&gt; str_view(\"ee?\")\n\n[1] │ &lt;e&gt;T&lt;ee&gt;T&lt;ee&gt;&lt;e&gt;T&lt;ee&gt;&lt;ee&gt;T\n\n\n\n+ for repeat matches:\n\n\nstr |&gt; str_view(\"ee+\")\n\n[1] │ eT&lt;ee&gt;T&lt;eee&gt;T&lt;eeee&gt;T\n\n\n\n* for any number of repeat matches, including 0:\n\n\nstr |&gt; str_view(\"ee*\")\n\n[1] │ &lt;e&gt;T&lt;ee&gt;T&lt;eee&gt;T&lt;eeee&gt;T"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#advanced-quantifiers-mn",
    "href": "meetups/Meetup6/Meetup6.html#advanced-quantifiers-mn",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Advanced Quantifiers {m,n}",
    "text": "Advanced Quantifiers {m,n}\n\nUse {m,n} to specify at least “m” matches but no more than “n” matches:\n\n\nAASeq = read_file(\"HuntingtonTrans.txt\") |&gt; \n  str_remove_all(\"\\t\") |&gt; \n  str_remove_all(\"\\n\") |&gt; \n  str_remove_all(\" \")\nAASeq |&gt; str_extract_all(\"Q{2,}\") |&gt; print()\n\n[[1]]\n [1] \"QQQQQQQQQQQQQQQQQQQQQQQ\" \"QQQ\"                    \n [3] \"QQ\"                      \"QQ\"                     \n [5] \"QQ\"                      \"QQ\"                     \n [7] \"QQ\"                      \"QQ\"                     \n [9] \"QQ\"                      \"QQ\"                     \n[11] \"QQ\"                      \"QQ\"                     \n[13] \"QQ\"                     \n\nAASeq |&gt; str_extract_all(\"Q{3,10}\") |&gt; print()\n\n[[1]]\n[1] \"QQQQQQQQQQ\" \"QQQQQQQQQQ\" \"QQQ\"        \"QQQ\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#character-classes",
    "href": "meetups/Meetup6/Meetup6.html#character-classes",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Character Classes",
    "text": "Character Classes\n\nUse square brackets to denote a group of characters to match\n\n\nwords |&gt; str_view(\"[aeiou]tt[aeiou]\")\n\n [60] │ &lt;atte&gt;nd\n[107] │ b&lt;otto&gt;m\n[173] │ comm&lt;itte&gt;e\n[470] │ l&lt;ette&gt;r\n[508] │ m&lt;atte&gt;r\n\n\n\n“^” matches everything not in the brackets\n\n\nwords |&gt; str_view(\"q[^u]\")\nwords |&gt; str_view(\"q[u]\") |&gt; head(5)\n\n[276] │ e&lt;qu&gt;al\n[665] │ &lt;qu&gt;ality\n[666] │ &lt;qu&gt;arter\n[667] │ &lt;qu&gt;estion\n[668] │ &lt;qu&gt;ick"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#special-characters-and-escaping",
    "href": "meetups/Meetup6/Meetup6.html#special-characters-and-escaping",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Special Characters and Escaping",
    "text": "Special Characters and Escaping\n\nUse “\\” if you need to match a metacharacter for real, i.e.\n“\\.”, “\\”, “\\+” etc\nSpecial character groups also denoted with “\" -”\\s” space, opposite: “\\S” -“\\d” number, “\\D” not number -“\\w” letter or number, “\\W” not letter or number\nRanges: “[A-E]”, “[1-4]”\nNEED TO DOUBLE ESCAPE IN R!\n\n\nstr_view(\"abc123\",\"\\\\d\")\n\n[1] │ abc&lt;1&gt;&lt;2&gt;&lt;3&gt;"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#logical-or",
    "href": "meetups/Meetup6/Meetup6.html#logical-or",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Logical Or",
    "text": "Logical Or\n\n“|” allows for specifying alternative expressions to match\n\n\nfruit |&gt; str_view(\"apple|berry|melon\") |&gt; head(10)\n\n [1] │ &lt;apple&gt;\n [6] │ bil&lt;berry&gt;\n [7] │ black&lt;berry&gt;\n[10] │ blue&lt;berry&gt;\n[11] │ boysen&lt;berry&gt;\n[13] │ canary &lt;melon&gt;\n[19] │ cloud&lt;berry&gt;\n[21] │ cran&lt;berry&gt;\n[29] │ elder&lt;berry&gt;\n[32] │ goji &lt;berry&gt;"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#grouping",
    "href": "meetups/Meetup6/Meetup6.html#grouping",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Grouping",
    "text": "Grouping\n\n“()” makes whatever inside act like a group\nExample from earlier, finding repeats of a sequence\n\n\nDNA |&gt; str_extract_all(\"(caa|cag){3,60}\")\n\n[[1]]\n[1] \"cagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcaacag\"\n[2] \"cagcagcag\"                                                            \n[3] \"cagcagcag\"                                                            \n[4] \"caacagcaa\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#backreferencing",
    "href": "meetups/Meetup6/Meetup6.html#backreferencing",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Backreferencing",
    "text": "Backreferencing\n\nIf something is contained in parentheses, you can reference it using “\\1”, “\\2”, etc\n\n\nDNA |&gt; str_extract_all(\"(......)\\\\1+\")\n\n[[1]]\n [1] \"cagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcag\"\n [2] \"ccgccgccgccgccgccg\"                                          \n [3] \"cagccgcagccg\"                                                \n [4] \"ggccctggccct\"                                                \n [5] \"gaggaggaggag\"                                                \n [6] \"gaggaggaggag\"                                                \n [7] \"cctcgtcctcgt\"                                                \n [8] \"cccccaccccca\"                                                \n [9] \"gggcctgggcct\"                                                \n[10] \"tgagcttgagct\"                                                \n[11] \"aaaaaaaaaaaaaaaaaa\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#anchors",
    "href": "meetups/Meetup6/Meetup6.html#anchors",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Anchors",
    "text": "Anchors\n\n“^” start of string, “$” end of string\n\n\nwords |&gt; str_view(\"^a.+d$\")\n\n[12] │ &lt;add&gt;\n[17] │ &lt;afford&gt;\n[38] │ &lt;and&gt;\n[52] │ &lt;around&gt;\n[60] │ &lt;attend&gt;"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#boundary",
    "href": "meetups/Meetup6/Meetup6.html#boundary",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Boundary",
    "text": "Boundary\n\n“\\b” the boundary of a word\n\n\nsentences |&gt; str_view(\"ink\") |&gt; head(5)\n\n [12] │ A rod is used to catch p&lt;ink&gt; salmon.\n [73] │ The &lt;ink&gt; stain dried on the finished page.\n [96] │ Bail the boat to stop it from s&lt;ink&gt;ing.\n[148] │ The spot on the blotter was made by green &lt;ink&gt;.\n[206] │ The club rented the r&lt;ink&gt; for the fifth night.\n\nsentences |&gt; str_view(\"\\\\bink\\\\b\")\n\n [73] │ The &lt;ink&gt; stain dried on the finished page.\n[148] │ The spot on the blotter was made by green &lt;ink&gt;.\n[217] │ It is hard to erase blue or red &lt;ink&gt;.\n[321] │ Fill the &lt;ink&gt; jar with sticky glue."
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#data-science-in-context-presentation",
    "href": "meetups/Meetup6/Meetup6.html#data-science-in-context-presentation",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Data Science in Context Presentation",
    "text": "Data Science in Context Presentation"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup6/Meetup6.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#lab-1-and-tidyverse",
    "href": "meetups/Meetup3/Meetup3.html#lab-1-and-tidyverse",
    "title": "Meetup 3: Tidy Data",
    "section": "Lab 1 and Tidyverse",
    "text": "Lab 1 and Tidyverse\n\nGraded most of Lab 1\nCheck the feedback even if your score was great\nBiggest challenge was faceted histogram\nTidyverse vs. Base R"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#unstructured-data",
    "href": "meetups/Meetup3/Meetup3.html#unstructured-data",
    "title": "Meetup 3: Tidy Data",
    "section": "Unstructured Data",
    "text": "Unstructured Data\nNeed to reinvent wheel every time to analyze:"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#tidy-data-rebranded-data-matrix",
    "href": "meetups/Meetup3/Meetup3.html#tidy-data-rebranded-data-matrix",
    "title": "Meetup 3: Tidy Data",
    "section": "Tidy Data (Rebranded Data Matrix?)",
    "text": "Tidy Data (Rebranded Data Matrix?)\n\nColumns are variables\nRows are observations\nOne value per cell\n\n\nFig 5.1 from R4DS"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#example",
    "href": "meetups/Meetup3/Meetup3.html#example",
    "title": "Meetup 3: Tidy Data",
    "section": "Example",
    "text": "Example\nUntidy:\n\n\n# A tibble: 5 × 58\n  country   year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544 new_sp_m4554\n  &lt;chr&gt;    &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Afghani…  2001         129          379          349          274          204\n2 Afghani…  2002          90          476          481          368          246\n3 Afghani…  2003         127          511          436          284          256\n4 Afghani…  2004         139          537          568          360          358\n5 Afghani…  2005         151          606          560          472          453\n# ℹ 51 more variables: new_sp_m5564 &lt;dbl&gt;, new_sp_m65 &lt;dbl&gt;, new_sp_f014 &lt;dbl&gt;,\n#   new_sp_f1524 &lt;dbl&gt;, new_sp_f2534 &lt;dbl&gt;, new_sp_f3544 &lt;dbl&gt;,\n#   new_sp_f4554 &lt;dbl&gt;, new_sp_f5564 &lt;dbl&gt;, new_sp_f65 &lt;dbl&gt;,\n#   new_sn_m014 &lt;dbl&gt;, new_sn_m1524 &lt;dbl&gt;, new_sn_m2534 &lt;dbl&gt;,\n#   new_sn_m3544 &lt;dbl&gt;, new_sn_m4554 &lt;dbl&gt;, new_sn_m5564 &lt;dbl&gt;,\n#   new_sn_m65 &lt;dbl&gt;, new_sn_f014 &lt;dbl&gt;, new_sn_f1524 &lt;dbl&gt;,\n#   new_sn_f2534 &lt;dbl&gt;, new_sn_f3544 &lt;dbl&gt;, new_sn_f4554 &lt;dbl&gt;, …\n\n\nTidy:\n\n\n# A tibble: 7 × 6\n  country      year diagnosis gender age   count\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1 Afghanistan  2001 sp        m      014     129\n2 Afghanistan  2001 sp        m      1524    379\n3 Afghanistan  2001 sp        m      2534    349\n4 Afghanistan  2001 sp        m      3544    274\n5 Afghanistan  2001 sp        m      4554    204\n6 Afghanistan  2001 sp        m      5564    139\n7 Afghanistan  2001 sp        m      65      103"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#why-does-the-tidyverse-work",
    "href": "meetups/Meetup3/Meetup3.html#why-does-the-tidyverse-work",
    "title": "Meetup 3: Tidy Data",
    "section": "Why Does the TidyVerse Work?",
    "text": "Why Does the TidyVerse Work?\n\n\nTidyverse functions:\n\nTake tidy data as input\nReturn tidy data as output\nCan freely chain together functions\n\n\n\ntidyDataset |&gt; \n  filter(...) |&gt; \n  mutate(...) |&gt; \n  select(...) |&gt; \n  group_by(...) |&gt; \n  summarise(...) |&gt; \n  ggplot(aes(...)) +\n  ...\n\n\nTidyverse is successful because it insists on the tidy format and because it is designed around it. Other ecosystems (base R, pandas, etc) don’t always return tidy output from tidy input."
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#originated-from-relational-databases",
    "href": "meetups/Meetup3/Meetup3.html#originated-from-relational-databases",
    "title": "Meetup 3: Tidy Data",
    "section": "Originated from Relational Databases",
    "text": "Originated from Relational Databases\n\n\nOriginal Tidy Data Def:\n\nColumns are variables\nRows are observations\nOne observational unit per table\n\n\n\n\n\nOriginal Tidy Data Paper\n\n\n\nIf you have studied relational databases you might recognize Codd’s 3rd normal form."
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#new-versus-old-tidy",
    "href": "meetups/Meetup3/Meetup3.html#new-versus-old-tidy",
    "title": "Meetup 3: Tidy Data",
    "section": "New Versus Old Tidy",
    "text": "New Versus Old Tidy\n New Tidy- repeats a lot of data on songs but easier to analyze because of flat structure"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#new-versus-old-tidy-1",
    "href": "meetups/Meetup3/Meetup3.html#new-versus-old-tidy-1",
    "title": "Meetup 3: Tidy Data",
    "section": "New Versus Old Tidy",
    "text": "New Versus Old Tidy\n Old tidy: Space efficient like a database"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#wide-data",
    "href": "meetups/Meetup3/Meetup3.html#wide-data",
    "title": "Meetup 3: Tidy Data",
    "section": "Wide Data",
    "text": "Wide Data\n\nAll data corresponding to a single observational unit in a single row\nValues of First Column unique\nEasy to process visually\nSome software works better with wide data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrack\nwk1\nwk2\nwk3\nwk4\nwk5\nwk6\nwk7\nwk8\nwk9\nwk10\n\n\n\n\nBaby Don’t Cry (Keep…\n87\n82\n72\n77\n87\n94\n99\nNA\nNA\nNA\n\n\nThe Hardest Part Of …\n91\n87\n92\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nKryptonite\n81\n70\n68\n67\n66\n57\n54\n53\n51\n51\n\n\nLoser\n76\n76\n72\n69\n67\n65\n55\n59\n62\n61\n\n\nWobble Wobble\n57\n34\n25\n17\n17\n31\n36\n49\n53\n57"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#tall-data",
    "href": "meetups/Meetup3/Meetup3.html#tall-data",
    "title": "Meetup 3: Tidy Data",
    "section": "Tall Data",
    "text": "Tall Data\n\n\n\nFirst column values repeat\nWorks well with software like R\nHarder to process visually\nTidy data typically Tall\n\n\n\n\n\n\n\ntrack\nWeek\nRank\n\n\n\n\nBaby Don’t Cry (Keep…\n1\n87\n\n\nBaby Don’t Cry (Keep…\n2\n82\n\n\nBaby Don’t Cry (Keep…\n3\n72\n\n\nBaby Don’t Cry (Keep…\n4\n77\n\n\nBaby Don’t Cry (Keep…\n5\n87\n\n\nBaby Don’t Cry (Keep…\n6\n94\n\n\nBaby Don’t Cry (Keep…\n7\n99\n\n\nThe Hardest Part Of …\n1\n91\n\n\n\n\n\n\nTall and wide can contain the same info, distinction between them is not always completely precise."
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-longer",
    "href": "meetups/Meetup3/Meetup3.html#pivot-longer",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Longer",
    "text": "Pivot Longer\n\n\n\n\n\n\n\nid\nbp1\nbp2\n\n\n\n\nA\n100\n120\n\n\nB\n140\n115\n\n\nC\n120\n125\n\n\n\n\n\n\n\n\n\n\n\nid\nmeasurement\nvalue\n\n\n\n\nA\nbp1\n100\n\n\nA\nbp2\n120\n\n\nB\nbp1\n140\n\n\nB\nbp2\n115\n\n\nC\nbp1\n120\n\n\nC\nbp2\n125"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-longer-1",
    "href": "meetups/Meetup3/Meetup3.html#pivot-longer-1",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Longer",
    "text": "Pivot Longer\n\ndf_long = df |&gt; pivot_longer(\n                   cols = bp1:bp2,\n                   names_to = \"measurement\",\n                   values_to = \"value\"\n                   )\n\n\n\n# A tibble: 6 × 3\n  id    measurement value\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 A     bp1           100\n2 A     bp2           120\n3 B     bp1           140\n4 B     bp2           115\n5 C     bp1           120\n6 C     bp2           125"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-longer-2",
    "href": "meetups/Meetup3/Meetup3.html#pivot-longer-2",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Longer",
    "text": "Pivot Longer\n\ndf_long = df |&gt; pivot_longer(\n                   cols = bp1:bp2,\n                   names_to = \"measurement\",\n                   values_to = \"value\"\n                   )\n\n\nR4DS 5.3"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-longer-3",
    "href": "meetups/Meetup3/Meetup3.html#pivot-longer-3",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Longer",
    "text": "Pivot Longer\n\ndf_long = df |&gt; pivot_longer(\n                   cols = bp1:bp2,\n                   names_to = \"measurement\",\n                   values_to = \"value\"\n                   )\n\n\nR4DS 5.4"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-longer-4",
    "href": "meetups/Meetup3/Meetup3.html#pivot-longer-4",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Longer",
    "text": "Pivot Longer\n\ndf_long = df |&gt; pivot_longer(\n                   cols = bp1:bp2,\n                   names_to = \"measurement\",\n                   values_to = \"value\"\n                   )\n\n\nR4DS 5.5"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-wider",
    "href": "meetups/Meetup3/Meetup3.html#pivot-wider",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Wider",
    "text": "Pivot Wider\n\nSelect column for names and column for values\nDistinct values in column become new column headers\nValues get mapped from other column\nPotential for NA values…..\n\n\n\n\n\n\nid\nmeasurement\nvalue\n\n\n\n\nA\nbp1\n100\n\n\nA\nbp1\n102\n\n\nB\nbp2\n120\n\n\nB\nbp2\n140\n\n\nA\nbp3\n115"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-wider-1",
    "href": "meetups/Meetup3/Meetup3.html#pivot-wider-1",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Wider",
    "text": "Pivot Wider\n\n\n\n\n\n\n\nid\nmeas\nval\n\n\n\n\nA\nbp1\n100\n\n\nB\nbp1\n102\n\n\nB\nbp2\n120\n\n\nA\nbp2\n140\n\n\nA\nbp3\n115\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nbp1\nbp2\nbp3\n\n\n\n\nA\n100\n140\n115\n\n\nB\n102\n120\nNA\n\n\n\n\n\n\n\ndf |&gt; pivot_wider(\n  names_from = meas,\n  values_from = val\n)"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-wider-non-unique-values",
    "href": "meetups/Meetup3/Meetup3.html#pivot-wider-non-unique-values",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Wider: Non-unique values",
    "text": "Pivot Wider: Non-unique values\n\n\n\n\n\n\n\nid\nmeas\nval\n\n\n\n\nA\nbp1\n100\n\n\nB\nbp1\n102\n\n\nB\nbp2\n120\n\n\nA\nbp2\n140\n\n\nA\nbp2\n115\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nbp1\nbp2\n\n\n\n\nA\n100\n140, 115\n\n\nB\n102\n120\n\n\n\n\n\n\n\ndf |&gt; pivot_wider(\n  names_from = meas,\n  values_from = val\n)"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-wider-multiple-rows",
    "href": "meetups/Meetup3/Meetup3.html#pivot-wider-multiple-rows",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Wider: Multiple Rows",
    "text": "Pivot Wider: Multiple Rows\n\n\n\n\n\n\n\nid\nmeas\nval\ntime\n\n\n\n\nA\nbp1\n100\nday\n\n\nB\nbp1\n102\nnight\n\n\nB\nbp2\n120\nnight\n\n\nA\nbp2\n140\nday\n\n\nA\nbp3\n115\nmorning\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntime\nbp1\nbp2\nbp3\n\n\n\n\nA\nday\n100\n140\nNA\n\n\nB\nnight\n102\n120\nNA\n\n\nA\nmorning\nNA\nNA\n115"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#when-to-use-each-pivot",
    "href": "meetups/Meetup3/Meetup3.html#when-to-use-each-pivot",
    "title": "Meetup 3: Tidy Data",
    "section": "When to use each pivot",
    "text": "When to use each pivot\n\npivot_longer tidies data where variables and/or data are in the column names\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nyear\nsp_m_014\nsp_m_1524\nsp_m_2534\nsp_m_3544\nsp_m_4554\n\n\n\n\nAfghanistan\n1980\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\n1981\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\n1982\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\n1983\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\n1984\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#when-to-use-each-pivot-1",
    "href": "meetups/Meetup3/Meetup3.html#when-to-use-each-pivot-1",
    "title": "Meetup 3: Tidy Data",
    "section": "When to use each pivot",
    "text": "When to use each pivot\n\npivot_wider tidies data where measurements are spread across multiple rows\n\n\n\n\n\n\nstation\ndate\nmeasure\nvalue\n\n\n\n\n1\n2024/09/01\ntemp\n25.0 C\n\n\n1\n2024/09/01\npressure\n700mmHg\n\n\n1\n2024/09/01\nhumidity\n60%\n\n\n1\n2024/09/01\nrain\n.2in\n\n\n1\n2024/09/02\ntemp\n22.0 C\n\n\n1\n2024/09/02\npressure\n800mmHg\n\n\n1\n2024/09/02\nhumidity\n40%"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#tidy-data-defined-based-on-your-goals",
    "href": "meetups/Meetup3/Meetup3.html#tidy-data-defined-based-on-your-goals",
    "title": "Meetup 3: Tidy Data",
    "section": "Tidy Data Defined Based on Your Goals",
    "text": "Tidy Data Defined Based on Your Goals\n\nDefinition of “variable” and “measurement” is partially up to you!\nPick the definitions that are best suited for you analysis"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#data-science-in-context-presentation",
    "href": "meetups/Meetup3/Meetup3.html#data-science-in-context-presentation",
    "title": "Meetup 3: Tidy Data",
    "section": "Data Science in Context Presentation",
    "text": "Data Science in Context Presentation"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup3/Meetup3.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 3: Tidy Data",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here\n\n\n\n\nDATA 607"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#lab-3-summary",
    "href": "meetups/Meetup5/Meetup5.html#lab-3-summary",
    "title": "Meetup 5: Data Transformation",
    "section": "Lab 3 Summary",
    "text": "Lab 3 Summary\n\nGraded almost every lab 3 that has been turned in.\nComments:\n\nNo getting rid of data without a reason that comes from domain info!\nStats don’t work the same on “circular” variables like time of day"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#data-transformation",
    "href": "meetups/Meetup5/Meetup5.html#data-transformation",
    "title": "Meetup 5: Data Transformation",
    "section": "Data Transformation",
    "text": "Data Transformation\n\nProcess of creating new variables from current data\nNeeded to prepare data for your analyses and questions\nData transformations can lead to dramatically improved insight at little cost\nTypes: Numbers, logical variables, dates, later strings and factors."
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#case-study-brain-and-body-size",
    "href": "meetups/Meetup5/Meetup5.html#case-study-brain-and-body-size",
    "title": "Meetup 5: Data Transformation",
    "section": "Case Study: Brain and Body Size",
    "text": "Case Study: Brain and Body Size"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#no-correlation",
    "href": "meetups/Meetup5/Meetup5.html#no-correlation",
    "title": "Meetup 5: Data Transformation",
    "section": "No Correlation?",
    "text": "No Correlation?\n\nCompute the Pearson correlation between brain mass and body mass:\n\n\ncor(Animals$body,Animals$brain)\n\n[1] -0.005341163\n\n\n\nWe expect a strong relationship between brain mass and body mass\nVery difficult to believe that there is no relationship between brain mass and body mass."
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#log-transformation",
    "href": "meetups/Meetup5/Meetup5.html#log-transformation",
    "title": "Meetup 5: Data Transformation",
    "section": "Log Transformation",
    "text": "Log Transformation\n\n\n\nBrain and Body size:\n\nPositive variables\nRange over orders of magnitude\nResult from multiplicative growth processes\n\nSuggests we try logarithms\n\n\n\n\n\nxkcd"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#log-transformation-different-story",
    "href": "meetups/Meetup5/Meetup5.html#log-transformation-different-story",
    "title": "Meetup 5: Data Transformation",
    "section": "Log Transformation: Different Story",
    "text": "Log Transformation: Different Story\n\n\nAnimals |&gt; \n  summarize(Correlation = cor(log(body),log(brain)) ) |&gt; print()\n\n  Correlation\n1   0.7794935"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#booleanlogical-operations",
    "href": "meetups/Meetup5/Meetup5.html#booleanlogical-operations",
    "title": "Meetup 5: Data Transformation",
    "section": "Boolean/Logical Operations",
    "text": "Boolean/Logical Operations\n\nLogical Types: TRUE and FALSE\nNOT: !TRUE = FALSE, !FALSE=TRUE\nAND: only TRUE & TRUE = TRUE rest FALSE\nOR: only FALSE | FALSE = FALSE rest TRUE\n\nComparison statements don’t work like in English:\n\n4 &gt; 5 | 6 # Wrong\n\n[1] TRUE\n\n(4 &gt; 5) | (4 &gt; 6) # Right\n\n[1] FALSE"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#use-boolean-to-create-new-variable-for-comparison",
    "href": "meetups/Meetup5/Meetup5.html#use-boolean-to-create-new-variable-for-comparison",
    "title": "Meetup 5: Data Transformation",
    "section": "Use Boolean to Create New Variable for Comparison",
    "text": "Use Boolean to Create New Variable for Comparison\n\nlibrary(kableExtra)\nnycflights13::flights |&gt; select(year,month,day,arr_delay,carrier,origin,dest) |&gt;  head(10) |&gt; kable()\n\n\n\n\nyear\nmonth\nday\narr_delay\ncarrier\norigin\ndest\n\n\n\n\n2013\n1\n1\n11\nUA\nEWR\nIAH\n\n\n2013\n1\n1\n20\nUA\nLGA\nIAH\n\n\n2013\n1\n1\n33\nAA\nJFK\nMIA\n\n\n2013\n1\n1\n-18\nB6\nJFK\nBQN\n\n\n2013\n1\n1\n-25\nDL\nLGA\nATL\n\n\n2013\n1\n1\n12\nUA\nEWR\nORD\n\n\n2013\n1\n1\n19\nB6\nEWR\nFLL\n\n\n2013\n1\n1\n-14\nEV\nLGA\nIAD\n\n\n2013\n1\n1\n-8\nB6\nJFK\nMCO\n\n\n2013\n1\n1\n8\nAA\nLGA\nORD"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#compare-dl-ua-aa-to-others",
    "href": "meetups/Meetup5/Meetup5.html#compare-dl-ua-aa-to-others",
    "title": "Meetup 5: Data Transformation",
    "section": "Compare DL, UA, AA to others",
    "text": "Compare DL, UA, AA to others\n\nCreate a logical variable that is TRUE when airline is in the group\nFALSE otherwise\n\n\nflights = nycflights13::flights |&gt; \n  mutate(IS_CARRIER_DL_UA_AA = carrier %in% c(\"DL\",\"UA\",\"AA\"))\nflights |&gt; \n  group_by(IS_CARRIER_DL_UA_AA) |&gt; \n  summarise(fraction_delayed = sum(arr_delay&gt;30,na.rm=TRUE)/\n              sum(is.finite(arr_delay),na.rm=TRUE)) |&gt; \n  print()\n\n# A tibble: 2 × 2\n  IS_CARRIER_DL_UA_AA fraction_delayed\n  &lt;lgl&gt;                          &lt;dbl&gt;\n1 FALSE                          0.180\n2 TRUE                           0.125"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#logical-subsetting",
    "href": "meetups/Meetup5/Meetup5.html#logical-subsetting",
    "title": "Meetup 5: Data Transformation",
    "section": "Logical Subsetting",
    "text": "Logical Subsetting\n\nPowerful way of manipulating Data Frames\nif_else and case_when: vector output\n\n\ndiamonds |&gt; \n  mutate( cut = if_else(\n      cut == \"Very Good\", \"Very_Good\", cut)\n    )"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#logical-subsetting-using-base",
    "href": "meetups/Meetup5/Meetup5.html#logical-subsetting-using-base",
    "title": "Meetup 5: Data Transformation",
    "section": "Logical Subsetting using Base",
    "text": "Logical Subsetting using Base\n\nCan use [] for other data types\n\n\ndiabetes = read_csv(\"../../assignments/labs/labData/diabetes.csv\")\ndiabetes\n\n# A tibble: 768 × 9\n   Pregnancies Glucose BloodPressure SkinThickness Insulin   BMI\n         &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1           6     148            72            35       0  33.6\n 2           1      85            66            29       0  26.6\n 3           8     183            64             0       0  23.3\n 4           1      89            66            23      94  28.1\n 5           0     137            40            35     168  43.1\n 6           5     116            74             0       0  25.6\n 7           3      78            50            32      88  31  \n 8          10     115             0             0       0  35.3\n 9           2     197            70            45     543  30.5\n10           8     125            96             0       0   0  \n# ℹ 758 more rows\n# ℹ 3 more variables: DiabetesPedigreeFunction &lt;dbl&gt;, Age &lt;dbl&gt;, Outcome &lt;dbl&gt;"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#logical-subsetting-using-base-1",
    "href": "meetups/Meetup5/Meetup5.html#logical-subsetting-using-base-1",
    "title": "Meetup 5: Data Transformation",
    "section": "Logical Subsetting using Base",
    "text": "Logical Subsetting using Base\n\nCan use [] for other data types\n\n\ndiabetes[2:6][diabetes[2:6]==0] = NA\ndiabetes\n\n# A tibble: 768 × 9\n   Pregnancies Glucose BloodPressure SkinThickness Insulin   BMI\n         &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1           6     148            72            35      NA  33.6\n 2           1      85            66            29      NA  26.6\n 3           8     183            64            NA      NA  23.3\n 4           1      89            66            23      94  28.1\n 5           0     137            40            35     168  43.1\n 6           5     116            74            NA      NA  25.6\n 7           3      78            50            32      88  31  \n 8          10     115            NA            NA      NA  35.3\n 9           2     197            70            45     543  30.5\n10           8     125            96            NA      NA  NA  \n# ℹ 758 more rows\n# ℹ 3 more variables: DiabetesPedigreeFunction &lt;dbl&gt;, Age &lt;dbl&gt;, Outcome &lt;dbl&gt;"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#numerical-transformations",
    "href": "meetups/Meetup5/Meetup5.html#numerical-transformations",
    "title": "Meetup 5: Data Transformation",
    "section": "Numerical Transformations",
    "text": "Numerical Transformations\n\nCan implement any mathematical formula you can imagine\nConstruct quantity of interest:\n\nDomain expertise\n\nAs part of EDA\n\nMake a complicated relationship look simpler or even linear\n\nTo boost other tools\n\nSome algorithms run better with standardized data"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#recycling-rules-for-aggregates",
    "href": "meetups/Meetup5/Meetup5.html#recycling-rules-for-aggregates",
    "title": "Meetup 5: Data Transformation",
    "section": "Recycling Rules for Aggregates",
    "text": "Recycling Rules for Aggregates\n\nR adjusts “boosts” numbers/vectors so that calculations work\nBehavior is very different from other languages:\n\n\nc(5,2,3,10)/c(1,9,1)\n\n[1]  5.0000000  0.2222222  3.0000000 10.0000000\n\n\n\nShorter vector/number is repeated to match the length of longer one\nCan be very unintuitive for when the vectors have lengths different from 1"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#recycling-fails",
    "href": "meetups/Meetup5/Meetup5.html#recycling-fails",
    "title": "Meetup 5: Data Transformation",
    "section": "Recycling Fails:",
    "text": "Recycling Fails:\n\nSource of silent bugs\n\n\nflights |&gt; \n  filter(month == c(1, 2)) |&gt; select(year, month, flight) |&gt; head(5)\n\n# A tibble: 5 × 3\n   year month flight\n  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1  2013     1   1545\n2  2013     1   1141\n3  2013     1    461\n4  2013     1    507\n5  2013     1     79\n\nflights |&gt; \n  filter(month %in% c(1, 2)) |&gt; select(year, month, flight) |&gt; head(5)\n\n# A tibble: 5 × 3\n   year month flight\n  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1  2013     1   1545\n2  2013     1   1714\n3  2013     1   1141\n4  2013     1    725\n5  2013     1    461"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#logarithms",
    "href": "meetups/Meetup5/Meetup5.html#logarithms",
    "title": "Meetup 5: Data Transformation",
    "section": "logarithms",
    "text": "logarithms\n\n\n\\(\\log(e^x) = x\\)\n\\(\\log(x_1 \\cdot x_2 \\cdot \\dots \\cdot x_n) = \\log(x_1) + \\log(x_2) + \\dots + \\log(x_n)\\)\n\\(\\log_{10}\\) counts the digits of a number"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#log-in-data-science-and-stats",
    "href": "meetups/Meetup5/Meetup5.html#log-in-data-science-and-stats",
    "title": "Meetup 5: Data Transformation",
    "section": "\\(\\log\\) in data science and stats",
    "text": "\\(\\log\\) in data science and stats\n\nStarting point when:\n\nPositive variables\nMultiplicative growth\nWide Range\n\nAvoiding overflow/underflow in probability models\n\nIn maximum likelihood focus on log likelihood\n\nSpecial case of Tukey’s Ladder:"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#two-approaches-to-transformations",
    "href": "meetups/Meetup5/Meetup5.html#two-approaches-to-transformations",
    "title": "Meetup 5: Data Transformation",
    "section": "Two Approaches to Transformations:",
    "text": "Two Approaches to Transformations:\n\nDomain Focused:\n\n\nConstruct quantities of interest\nTransformation always inspired by knowledge of domain\nPower transformation to scale physical values\n\n\nStats Focused:\n\n\nTransform with goal to make data more Normal, relations more linear\nExemplified by Box-Cox Transformation"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#window-functions-values",
    "href": "meetups/Meetup5/Meetup5.html#window-functions-values",
    "title": "Meetup 5: Data Transformation",
    "section": "Window Functions: Values",
    "text": "Window Functions: Values\n\nPerform transformations on “nearby” rows:\nlead Next value(s)\nlag Previous value(s)\n\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; lead() \n\n [1]  2  3  4  5  6  7  8  9 10 NA\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; lead(n=3) \n\n [1]  4  5  6  7  8  9 10 NA NA NA\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; lag() \n\n [1] NA  1  2  3  4  5  6  7  8  9\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; lag(n=3) \n\n [1] NA NA NA  1  2  3  4  5  6  7\n\n\n\nfirst() and last()"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#window-functions-ranks",
    "href": "meetups/Meetup5/Meetup5.html#window-functions-ranks",
    "title": "Meetup 5: Data Transformation",
    "section": "Window Functions: Ranks",
    "text": "Window Functions: Ranks\n\nPerform transformations on “nearby” rows:\npercent_rank()\nrank()\nntile()\n\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; percent_rank() \n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n [8] 0.7777778 0.8888889 1.0000000\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; rank() \n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; ntile(4) \n\n [1] 1 1 1 2 2 2 3 3 4 4"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates",
    "href": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates",
    "title": "Meetup 5: Data Transformation",
    "section": "Cumulative and Rolling Aggregates",
    "text": "Cumulative and Rolling Aggregates\n\ncumsum, cummean\nRolling window (several libraries including RcppRoll, TTR, slider)\n\n\nlibrary(TTR)\nlibrary(cranlogs)\npkgs = c(\n    \"tidyr\", \"lubridate\", \"dplyr\", \n    \"ggplot2\", \"purrr\", \n    \"stringr\", \"knitr\"\n    )\n\ntidyverse_downloads = cran_downloads(\n    packages = pkgs, \n    from     = \"2024-01-01\", \n    to       = \"2024-08-31\") %&gt;%\n    tibble::as_tibble() %&gt;%\n    group_by(package)"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates-1",
    "href": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates-1",
    "title": "Meetup 5: Data Transformation",
    "section": "Cumulative and Rolling Aggregates",
    "text": "Cumulative and Rolling Aggregates\n\ncumsum, cummean\nRolling window (several libraries including RcppRoll, TTR, slider)\n\n\ntidyverse_downloads |&gt; filter(package == \"lubridate\") |&gt; \n  ggplot(aes(x=date,y=count)) + geom_point()+\n  theme_bw(base_size = 16)"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates-2",
    "href": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates-2",
    "title": "Meetup 5: Data Transformation",
    "section": "Cumulative and Rolling Aggregates",
    "text": "Cumulative and Rolling Aggregates\n\nRolling window TTR:\n\n\ntidyverse_downloads |&gt; filter(package == \"lubridate\") |&gt; \n  ggplot(aes(x=date,y=count)) + geom_point()+\n  theme_bw(base_size = 16)"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates-3",
    "href": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates-3",
    "title": "Meetup 5: Data Transformation",
    "section": "Cumulative and Rolling Aggregates",
    "text": "Cumulative and Rolling Aggregates\n\ntidyverse_downloads |&gt; filter(package == \"lubridate\") |&gt; \n  mutate(rolling_count = runMean(count,n=7)) |&gt; \n  ggplot(aes(x=date,y=count)) + geom_point() + geom_line(aes(y=rolling_count)) +\n  theme_bw(base_size = 16)"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#caution-about-group-boundaries",
    "href": "meetups/Meetup5/Meetup5.html#caution-about-group-boundaries",
    "title": "Meetup 5: Data Transformation",
    "section": "Caution About Group Boundaries",
    "text": "Caution About Group Boundaries\n\ntidyverse_downloads |&gt;  ungroup() |&gt; \n  mutate(rolling_count = runMean(count,n=7)) |&gt; filter(package == \"lubridate\") |&gt; \n  ggplot(aes(x=date,y=count)) + geom_point() + geom_line(aes(y=rolling_count)) +\n  theme_bw(base_size = 16)"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#data-science-in-context-presentation",
    "href": "meetups/Meetup5/Meetup5.html#data-science-in-context-presentation",
    "title": "Meetup 5: Data Transformation",
    "section": "Data Science in Context Presentation",
    "text": "Data Science in Context Presentation"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#rank",
    "href": "meetups/Meetup5/Meetup5.html#rank",
    "title": "Meetup 5: Data Transformation",
    "section": "Rank",
    "text": "Rank\n\nrank transforms data from values to numerical rank:\n\n\nrandVec = rnorm(10)\nprint(randVec)\n\n [1]  1.2962539  0.8822177 -0.2722599 -0.1767018 -0.3723019 -0.9106871\n [7] -0.5472664  1.7013557 -0.5254146  1.0894533\n\nprint(randVec |&gt; rank())\n\n [1]  9  7  5  6  4  1  2 10  3  8\n\n\n\nRank preserved by all monotone transformations"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#rank-correlation-and-correlation-trick",
    "href": "meetups/Meetup5/Meetup5.html#rank-correlation-and-correlation-trick",
    "title": "Meetup 5: Data Transformation",
    "section": "Rank Correlation and Correlation Trick",
    "text": "Rank Correlation and Correlation Trick\n\nSpearman Correlation is correlation between ranks of two variables\n\n\nAnimals  |&gt; \n           summarise(\n  rank_corr = cor(body,brain,method='spearman'),\n  corr = cor(body,brain)\n)\n\n  rank_corr         corr\n1 0.7162994 -0.005341163\n\n\n\nCorrelation Trick: Look for Nonlinear Transformations when Rank-Correlation is much higher than Pearson Correlation"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#rank-correlation-graphical-version",
    "href": "meetups/Meetup5/Meetup5.html#rank-correlation-graphical-version",
    "title": "Meetup 5: Data Transformation",
    "section": "Rank Correlation Graphical Version",
    "text": "Rank Correlation Graphical Version"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#what-didnt-we-cover",
    "href": "meetups/Meetup5/Meetup5.html#what-didnt-we-cover",
    "title": "Meetup 5: Data Transformation",
    "section": "What didn’t we cover?",
    "text": "What didn’t we cover?\n\nlogit, probit, ReLU: Data transformations that go between a finite interval and the real line\nEnormous number of different ranking functions available in R\nLinear Algebra Based Methods, PCA, dimensionality reduction\nTools from the sciences: Fourier, Wavelets, etc"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup5/Meetup5.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 5: Data Transformation",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "assignments/labs/tbd.html",
    "href": "assignments/labs/tbd.html",
    "title": "Lab TBD",
    "section": "",
    "text": "Check back later\nThis assignment is not available to view yet. Please check back closer to the assignment due date/week of the module this assignment is based on."
  },
  {
    "objectID": "assignments/labs/Lab5.html",
    "href": "assignments/labs/Lab5.html",
    "title": "Lab 5: Working with Text and Strings",
    "section": "",
    "text": "In this lab you will practice perform a series of exercises that use text and string manipulation to either analyze data with text, manipulate data containing strings, apply regular expressions, or handle data files with unusual formats or text strings.\n\n\nProblem 1. Using the 173 majors listed in fivethirtyeight.com’s College Majors dataset, provide code that identifies the majors that contain either “DATA” or “STATISTICS”, case insensitive. You can find this dataset on R by installing the package fivethirtyeight and using the major column in either college_recent_grades, college_new_grads, or college_all_ages.\nProblem 2 Write code that transforms the data below:\n[1] \"bell pepper\" \"bilberry\" \"blackberry\" \"blood orange\"\n[5] \"blueberry\" \"cantaloupe\" \"chili pepper\" \"cloudberry\"\n[9] \"elderberry\" \"lime\" \"lychee\" \"mulberry\"\n[13] \"olive\"  \"salal berry\"\n\nInto a format like this:\nc(\"bell pepper\", \"bilberry\", \"blackberry\", \"blood orange\", \"blueberry\", \"cantaloupe\", \"chili pepper\", \"cloudberry\", \"elderberry\", \"lime\", \"lychee\", \"mulberry\", \"olive\", \"salal berry\")\nAs your starting point take the string defined in the following code chunk:\n\nmessyString = ' [1] \"bell pepper\" \"bilberry\" \"blackberry\" \"blood orange\" \\n\n [5] \"blueberry\" \"cantaloupe\" \"chili pepper\" \"cloudberry\" \\n\n [9] \"elderberry\" \"lime\" \"lychee\" \"mulberry\" \\n\n [13] \"olive\"  \"salal berry\" '\n\nHint: There are many different ways to solve this problem, but if you use str_extract_all a helpful flag that returns a character vector instead of a list is simplify=TRUE. Then you can apply other tools from stringr if needed.\nProblem 3 Describe, in words, what these regular expressions will match. Read carefully to see if each entry is a regular expression or a string that defines a regular expression.\n\n^.*$\n\"\\\\{.+\\\\}\"\n\\d{4}-\\d{2}-\\d{2}\n\"\\\\\\\\{4}\"\n\"(..)\\\\1\"\n\nProblem 4. Construct regular expressions to match words that:\n\nStart with “y”.\nHave seven letters or more.\nContain a vowel-consonant pair\nContain at least two vowel-consonant pairs in a row.\nContain the same vowel-consonant pair repeated twice in a row.\n\nFor each example, verify that they work by running them on the stringr::words dataset and show the first 10 results (hint: combine str_detect and logical subsetting).\nProblem 5 Consider the gss_cat data-frame discussed in Chapter 16 of R4DS (provided as part of the forcats package):\n\nCreate a new variable that describes whether the party-id of a survey respondent is “strong” if they are a strong republican or strong democrat, “weak” if they are a not strong democrat, not strong republican, or independent of any type, and “other” for the rest.\nCalculate the mean hours of TV watched by each of the groups “strong”, “weak”, and “other” and display it with a dot-plot (geom_point). Sort the levels in the dot-plot so that the group appears in order of most mean TV hours watched."
  },
  {
    "objectID": "assignments/labs/Lab5.html#problems",
    "href": "assignments/labs/Lab5.html#problems",
    "title": "Lab 5: Working with Text and Strings",
    "section": "",
    "text": "Problem 1. Using the 173 majors listed in fivethirtyeight.com’s College Majors dataset, provide code that identifies the majors that contain either “DATA” or “STATISTICS”, case insensitive. You can find this dataset on R by installing the package fivethirtyeight and using the major column in either college_recent_grades, college_new_grads, or college_all_ages.\nProblem 2 Write code that transforms the data below:\n[1] \"bell pepper\" \"bilberry\" \"blackberry\" \"blood orange\"\n[5] \"blueberry\" \"cantaloupe\" \"chili pepper\" \"cloudberry\"\n[9] \"elderberry\" \"lime\" \"lychee\" \"mulberry\"\n[13] \"olive\"  \"salal berry\"\n\nInto a format like this:\nc(\"bell pepper\", \"bilberry\", \"blackberry\", \"blood orange\", \"blueberry\", \"cantaloupe\", \"chili pepper\", \"cloudberry\", \"elderberry\", \"lime\", \"lychee\", \"mulberry\", \"olive\", \"salal berry\")\nAs your starting point take the string defined in the following code chunk:\n\nmessyString = ' [1] \"bell pepper\" \"bilberry\" \"blackberry\" \"blood orange\" \\n\n [5] \"blueberry\" \"cantaloupe\" \"chili pepper\" \"cloudberry\" \\n\n [9] \"elderberry\" \"lime\" \"lychee\" \"mulberry\" \\n\n [13] \"olive\"  \"salal berry\" '\n\nHint: There are many different ways to solve this problem, but if you use str_extract_all a helpful flag that returns a character vector instead of a list is simplify=TRUE. Then you can apply other tools from stringr if needed.\nProblem 3 Describe, in words, what these regular expressions will match. Read carefully to see if each entry is a regular expression or a string that defines a regular expression.\n\n^.*$\n\"\\\\{.+\\\\}\"\n\\d{4}-\\d{2}-\\d{2}\n\"\\\\\\\\{4}\"\n\"(..)\\\\1\"\n\nProblem 4. Construct regular expressions to match words that:\n\nStart with “y”.\nHave seven letters or more.\nContain a vowel-consonant pair\nContain at least two vowel-consonant pairs in a row.\nContain the same vowel-consonant pair repeated twice in a row.\n\nFor each example, verify that they work by running them on the stringr::words dataset and show the first 10 results (hint: combine str_detect and logical subsetting).\nProblem 5 Consider the gss_cat data-frame discussed in Chapter 16 of R4DS (provided as part of the forcats package):\n\nCreate a new variable that describes whether the party-id of a survey respondent is “strong” if they are a strong republican or strong democrat, “weak” if they are a not strong democrat, not strong republican, or independent of any type, and “other” for the rest.\nCalculate the mean hours of TV watched by each of the groups “strong”, “weak”, and “other” and display it with a dot-plot (geom_point). Sort the levels in the dot-plot so that the group appears in order of most mean TV hours watched."
  },
  {
    "objectID": "assignments/labs/Lab4.html",
    "href": "assignments/labs/Lab4.html",
    "title": "Lab 4: Data Transformations",
    "section": "",
    "text": "For this assignment we practice data transformation using a dataset of the daily prices and daily trading volumes of a group of large technology stocks that trade on US stock exchanges. Click here to download stocks.csv, which contains data going back to 2000. The dataset contains several variables, including:\n\nsymbol: which is the ticker symbol for the stock\ndate: which is the trading date\nopen, high, low, and close, which are the price at the start of trading, the high price during the day, the low price during the day, and the stock price at the close of trading (unit is USD)\nadjusted: which is the stock price at close adjusted for the financial effects of special events (such as dividends). Unit is USD\nvolume: which is the number of shares which traded during a given trading day.\n\n\nlibrary(tidyverse)\nlibrary(TTR)\nlibrary(kableExtra)\n\nstocks = read_csv(\"/home/georgehagstrom/work/Teaching/DATA607/website/assignments/labs/labData/stocks.csv\")\n\nstocks |&gt; head(10) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\nAAPL\n2000-01-03\n0.936384\n1.004464\n0.907924\n0.999442\n535796800\n0.8440041\n\n\nAAPL\n2000-01-04\n0.966518\n0.987723\n0.903460\n0.915179\n512377600\n0.7728457\n\n\nAAPL\n2000-01-05\n0.926339\n0.987165\n0.919643\n0.928571\n778321600\n0.7841553\n\n\nAAPL\n2000-01-06\n0.947545\n0.955357\n0.848214\n0.848214\n767972800\n0.7162958\n\n\nAAPL\n2000-01-07\n0.861607\n0.901786\n0.852679\n0.888393\n460734400\n0.7502258\n\n\nAAPL\n2000-01-10\n0.910714\n0.912946\n0.845982\n0.872768\n505064000\n0.7370310\n\n\nAAPL\n2000-01-11\n0.856585\n0.887277\n0.808036\n0.828125\n441548800\n0.6993311\n\n\nAAPL\n2000-01-12\n0.848214\n0.852679\n0.772321\n0.778460\n976068800\n0.6573902\n\n\nAAPL\n2000-01-13\n0.843610\n0.881696\n0.825893\n0.863839\n1032684800\n0.7294905\n\n\nAAPL\n2000-01-14\n0.892857\n0.912946\n0.887277\n0.896763\n390376000\n0.7572942\n\n\n\n\n\nAll of the the stock prices and the trading volume have been adjusted for stock splits, so that the data provide a continuous record of how prices and trading volume changed.\nThere are several important functions and packages that you will need to use to complete this exercise.\n\nWe will make a lot of use of window functions in dplyr, which are very helpful for making transformations (including lead, lag, percent_rank). The dplyr vignettes and articles are incredibly useful for learning about all the different functions available\nWe will use the TTR package (which is part of the tidyquant family of packages, see here). The main function we will use is called runMean. An alternative package that is also very nice but not part of the tidyverse is RcppRoll\nIf you aren’t very comfortable with logarithms, you should read more about them. They are one of the most important mathematical functions for data science. We aren’t using their mathematical properties much this week but they will be important throughout your data science journey. Khan Academy has a decent video, and this article in the journal nature has some more context.\nWe will calculate some correlation coefficients, using the cor function from base R (?cor to see how it is used). There is also a tidyverse package called corrr that is useful for calculating correlations on data frames, but we won’t use it for this lab.\nThe motivation for today’s assignment came from some news articles a few years ago about how big tech stocks collectively had a miniature meltdown after powering the stock market for several consecutive years, see this article at Morningstar\nThe wikipedia page on Market Impact has some references to Kyle’s \\(\\lambda\\), which we will calculate this week.\n\nProblem 1: The price of a stock on a given day only conveys information in relation to the stock price on other days. One useful measure is the daily return of the stock, which we will define as the ratio of the adjusted closing price on the current day of trading to the adjusted closing price on the previous day of trading. Read the following article on window functions in dplyr: window functions in dplyr.\n\nFind a function there that will help you calculate the daily return and use it along with mutate to add a return column to the data frame containing the daily return.\n\nHint: make sure to use group_by(symbol), otherwise your calculation might transpose prices from a different stock at the beginning of each time series.\nDifferences between the adjusted return and the return measured on the close price should indicate special corporate events such as dividends.\n\nCalculate the un-adjusted return using the same technique you used to calculate the return, but replacing the adjusted variable with the close variable, and find the datapoint in the dataset where the return exceeded the unadjusted return by the greatest margin. (Hint to check you have done it right: it happened in November 2004). The reason that the close price and the adjusted price differ is because stock prices typically decrease when a dividend is paid (to account for the cash paid out). The adjusted value has been modified from the beginning of the initial data record to increase adjusted to compensate for dividends. A dividend is just a payment that a company makes periodically to those who hold stock.\n\nIf you are curious: Look for an old news article describing the significance of that event and tell me what happened\nProblem 2: When working with stock price fluctuations or other processes where a quantity increases or decreases according to some multiplicative process like a growth rate (for example population growth) it is often better to work with the log of the growth rate rather than the growth rate itself. This allows standard summary statistics such as the mean to have a useful interpretation (otherwise you would have to use the geometric mean). Furthermore, the log transform is often useful to use on variables that are strictly positive, such as population growth rates or daily stock returns. To see why, consider a hypothetical stock which had a return of 0.5 (50% loss) on one day and 1.8 on the next day (80% gain). The mean of these two returns would be 1.075, or 7.5% per day. However, at the end of the two day period the stock would have lost 10% of its value (0.5*1.8 = 0.9). If we had computed the mean of the log(return) instead, we would have found that (log(0.5)+log(1.8))/2 = log(0.9^(1/2)), or approximately -5.2% per day, matching the observed price change.\n\nCreate a new variable called log_return which is the log of the return variable you calculated in the previous problem. Generate either a histogram or density plot of the distribution of log_return for the entire dataset. Then create a QQ-plot of log_return using geom_qq() and geom_qq_line(). What do you notice about the “tails” (right and left side/extreme edges) of the distribution from the QQ-plot? Are there visible signs of this in the density plot/histogram that you made?\n\nProblem 3: Volume measures how many shares were traded of a given stock over a set time period, and high volume days often associate with important events or market dynamics.\n\nMake a scatter plot of volume versus log_return, faceted by symbol to account for the fact that different stocks have different trading volumes. Do you see an association between volume and log_return in these scatter plots?\nUse the cor function to compute the pearson’s correlation coefficient between volume and log_return for each symbol. Why do you think the correlations are close to 0?\n\nHint: use it with summarize and don’t forget that cor is a base R function so you will either need to filter NA values for volume and log_return or appropriately choose the use flag in the argument- see ?cor for more info.\n\nNext compute the correlation in the same manner but this time transform log_return using the absolute value function. Recreate the faceted scatter-plots from the first part of the problem but with the absolute-value transformed log_return. How have the correlations changed from the previous summary?\n\nProblem 4: For this problem we will implement a more complicated mathematical transformation of data by calculating a measure of liquidity for each stock.\nLiquidity is defined loosely as the ability for a given asset to be bought or sold without a large impact on price. Liquid assets can be bought and sold quickly and easily, whereas illiquid assets have large increases or decreases in their price when someone tries to buy or sell them in large quantities. Liquidity is considered an important property of a well functioning financial market, and declines in liquidity have been blamed for worsening or triggering stock market crashes.\nMany methods have been invented to measure liquidity, but for this problem we will focus on a method called “Kyle’s \\(\\lambda\\)”. Kyle’s \\(\\lambda\\) estimates liquidity by using a linear regression between the absolute value daily return of a stock and the logarithm of the dollar volume of that stock. The time periods used to estimate this regression can vary, but here we will use daily returns and a one month time period (defined as 20 trading days). You will learn a lot about linear models in DATA 606 and other classes, but to be complete, \\(\\lambda\\) is a coefficient in the following linear model: \\[\n|R_t-1| = c + \\lambda \\log((\\mathrm{Volume})_t (\\mathrm{close})_t) + \\epsilon_t\n\\] where the coefficients \\(c\\) and \\(\\lambda\\) will be calculated to minimize the error \\(\\epsilon_t\\) over the past 20 trading days.\n\\(\\lambda\\) stands for the amount that the stock price will move in units of basis points for a given \\(\\log\\) dollar volume of trade. A small \\(\\lambda\\) indicates high liquidity, and a high \\(\\lambda\\) indicates low liquidity.\n\\(\\lambda\\) can be be calculated using rolling averages on the time series data with the TTR package, specifically the function runMean which when used within a dplyr pipeline will calculate the mean over the past \\(n\\) data points. For example, the command:\n\nlibrary(TTR)\nstocks |&gt; group_by(symbol) |&gt;  mutate(log_return_20d = runMean(log_return,n=20))\n\nadds a new variable which is equal to the mean of the log_return over the past 20 days. The mathematical formula for \\(\\lambda\\) is: \\[\n\\lambda = \\frac{\\mathrm{mean}(R_a\\log( p_c V ))\n- \\mathrm{mean}\\left(R_a\\right) \\mathrm{mean}\\left(\\log\\left(p_c V\\right) \\right) }\n{\\mathrm{mean}\\left(\\log\\left( p_c V \\right)^2\\right)\n-\\mathrm{mean}\\left(\\log(p_c V)\\right)^2 }\n\\] where to make the formula easier to read we have defined \\(R_a = |\\mathrm{return} -1|\\), \\(p_c = \\mathrm{close}\\) and \\(V = \\mathrm{volume}\\), and the averages have been taken over the past 20 days of data.\n\nAdd a new variable called kyle to the data frame by implementing the above formula for \\(\\lambda\\). Make sure to read and implement the formula very carefully, and to use the runMean function to calculate the rolling average correctly.\nPlot Kyle’s lambda for each stock over time (I would use a faceted scatterplot). What do you notice about how this measure of liquidity behaves (remember liquidity is high when \\(\\lambda\\) is small)?\nNext add a new variable to the dataframe called extreme which is true when the log_return for a given stock is either greater than 95% of other values of the log_return or less than 95% of all values of log_return. Use the percent_rank dplyr window function along with logical operators to create this variable. Then for each stock calculate the mean value of Kyle’s lambda for the days when the log_return had extreme values and for when it didn’t (as identified by the extreme variable). What do your calculations and figures indicate about liquidity during extreme events?"
  },
  {
    "objectID": "assignments/labs/Lab4.html#overview-large-technology-stocks",
    "href": "assignments/labs/Lab4.html#overview-large-technology-stocks",
    "title": "Lab 4: Data Transformations",
    "section": "",
    "text": "For this assignment we practice data transformation using a dataset of the daily prices and daily trading volumes of a group of large technology stocks that trade on US stock exchanges. Click here to download stocks.csv, which contains data going back to 2000. The dataset contains several variables, including:\n\nsymbol: which is the ticker symbol for the stock\ndate: which is the trading date\nopen, high, low, and close, which are the price at the start of trading, the high price during the day, the low price during the day, and the stock price at the close of trading (unit is USD)\nadjusted: which is the stock price at close adjusted for the financial effects of special events (such as dividends). Unit is USD\nvolume: which is the number of shares which traded during a given trading day.\n\n\nlibrary(tidyverse)\nlibrary(TTR)\nlibrary(kableExtra)\n\nstocks = read_csv(\"/home/georgehagstrom/work/Teaching/DATA607/website/assignments/labs/labData/stocks.csv\")\n\nstocks |&gt; head(10) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\nAAPL\n2000-01-03\n0.936384\n1.004464\n0.907924\n0.999442\n535796800\n0.8440041\n\n\nAAPL\n2000-01-04\n0.966518\n0.987723\n0.903460\n0.915179\n512377600\n0.7728457\n\n\nAAPL\n2000-01-05\n0.926339\n0.987165\n0.919643\n0.928571\n778321600\n0.7841553\n\n\nAAPL\n2000-01-06\n0.947545\n0.955357\n0.848214\n0.848214\n767972800\n0.7162958\n\n\nAAPL\n2000-01-07\n0.861607\n0.901786\n0.852679\n0.888393\n460734400\n0.7502258\n\n\nAAPL\n2000-01-10\n0.910714\n0.912946\n0.845982\n0.872768\n505064000\n0.7370310\n\n\nAAPL\n2000-01-11\n0.856585\n0.887277\n0.808036\n0.828125\n441548800\n0.6993311\n\n\nAAPL\n2000-01-12\n0.848214\n0.852679\n0.772321\n0.778460\n976068800\n0.6573902\n\n\nAAPL\n2000-01-13\n0.843610\n0.881696\n0.825893\n0.863839\n1032684800\n0.7294905\n\n\nAAPL\n2000-01-14\n0.892857\n0.912946\n0.887277\n0.896763\n390376000\n0.7572942\n\n\n\n\n\nAll of the the stock prices and the trading volume have been adjusted for stock splits, so that the data provide a continuous record of how prices and trading volume changed.\nThere are several important functions and packages that you will need to use to complete this exercise.\n\nWe will make a lot of use of window functions in dplyr, which are very helpful for making transformations (including lead, lag, percent_rank). The dplyr vignettes and articles are incredibly useful for learning about all the different functions available\nWe will use the TTR package (which is part of the tidyquant family of packages, see here). The main function we will use is called runMean. An alternative package that is also very nice but not part of the tidyverse is RcppRoll\nIf you aren’t very comfortable with logarithms, you should read more about them. They are one of the most important mathematical functions for data science. We aren’t using their mathematical properties much this week but they will be important throughout your data science journey. Khan Academy has a decent video, and this article in the journal nature has some more context.\nWe will calculate some correlation coefficients, using the cor function from base R (?cor to see how it is used). There is also a tidyverse package called corrr that is useful for calculating correlations on data frames, but we won’t use it for this lab.\nThe motivation for today’s assignment came from some news articles a few years ago about how big tech stocks collectively had a miniature meltdown after powering the stock market for several consecutive years, see this article at Morningstar\nThe wikipedia page on Market Impact has some references to Kyle’s \\(\\lambda\\), which we will calculate this week.\n\nProblem 1: The price of a stock on a given day only conveys information in relation to the stock price on other days. One useful measure is the daily return of the stock, which we will define as the ratio of the adjusted closing price on the current day of trading to the adjusted closing price on the previous day of trading. Read the following article on window functions in dplyr: window functions in dplyr.\n\nFind a function there that will help you calculate the daily return and use it along with mutate to add a return column to the data frame containing the daily return.\n\nHint: make sure to use group_by(symbol), otherwise your calculation might transpose prices from a different stock at the beginning of each time series.\nDifferences between the adjusted return and the return measured on the close price should indicate special corporate events such as dividends.\n\nCalculate the un-adjusted return using the same technique you used to calculate the return, but replacing the adjusted variable with the close variable, and find the datapoint in the dataset where the return exceeded the unadjusted return by the greatest margin. (Hint to check you have done it right: it happened in November 2004). The reason that the close price and the adjusted price differ is because stock prices typically decrease when a dividend is paid (to account for the cash paid out). The adjusted value has been modified from the beginning of the initial data record to increase adjusted to compensate for dividends. A dividend is just a payment that a company makes periodically to those who hold stock.\n\nIf you are curious: Look for an old news article describing the significance of that event and tell me what happened\nProblem 2: When working with stock price fluctuations or other processes where a quantity increases or decreases according to some multiplicative process like a growth rate (for example population growth) it is often better to work with the log of the growth rate rather than the growth rate itself. This allows standard summary statistics such as the mean to have a useful interpretation (otherwise you would have to use the geometric mean). Furthermore, the log transform is often useful to use on variables that are strictly positive, such as population growth rates or daily stock returns. To see why, consider a hypothetical stock which had a return of 0.5 (50% loss) on one day and 1.8 on the next day (80% gain). The mean of these two returns would be 1.075, or 7.5% per day. However, at the end of the two day period the stock would have lost 10% of its value (0.5*1.8 = 0.9). If we had computed the mean of the log(return) instead, we would have found that (log(0.5)+log(1.8))/2 = log(0.9^(1/2)), or approximately -5.2% per day, matching the observed price change.\n\nCreate a new variable called log_return which is the log of the return variable you calculated in the previous problem. Generate either a histogram or density plot of the distribution of log_return for the entire dataset. Then create a QQ-plot of log_return using geom_qq() and geom_qq_line(). What do you notice about the “tails” (right and left side/extreme edges) of the distribution from the QQ-plot? Are there visible signs of this in the density plot/histogram that you made?\n\nProblem 3: Volume measures how many shares were traded of a given stock over a set time period, and high volume days often associate with important events or market dynamics.\n\nMake a scatter plot of volume versus log_return, faceted by symbol to account for the fact that different stocks have different trading volumes. Do you see an association between volume and log_return in these scatter plots?\nUse the cor function to compute the pearson’s correlation coefficient between volume and log_return for each symbol. Why do you think the correlations are close to 0?\n\nHint: use it with summarize and don’t forget that cor is a base R function so you will either need to filter NA values for volume and log_return or appropriately choose the use flag in the argument- see ?cor for more info.\n\nNext compute the correlation in the same manner but this time transform log_return using the absolute value function. Recreate the faceted scatter-plots from the first part of the problem but with the absolute-value transformed log_return. How have the correlations changed from the previous summary?\n\nProblem 4: For this problem we will implement a more complicated mathematical transformation of data by calculating a measure of liquidity for each stock.\nLiquidity is defined loosely as the ability for a given asset to be bought or sold without a large impact on price. Liquid assets can be bought and sold quickly and easily, whereas illiquid assets have large increases or decreases in their price when someone tries to buy or sell them in large quantities. Liquidity is considered an important property of a well functioning financial market, and declines in liquidity have been blamed for worsening or triggering stock market crashes.\nMany methods have been invented to measure liquidity, but for this problem we will focus on a method called “Kyle’s \\(\\lambda\\)”. Kyle’s \\(\\lambda\\) estimates liquidity by using a linear regression between the absolute value daily return of a stock and the logarithm of the dollar volume of that stock. The time periods used to estimate this regression can vary, but here we will use daily returns and a one month time period (defined as 20 trading days). You will learn a lot about linear models in DATA 606 and other classes, but to be complete, \\(\\lambda\\) is a coefficient in the following linear model: \\[\n|R_t-1| = c + \\lambda \\log((\\mathrm{Volume})_t (\\mathrm{close})_t) + \\epsilon_t\n\\] where the coefficients \\(c\\) and \\(\\lambda\\) will be calculated to minimize the error \\(\\epsilon_t\\) over the past 20 trading days.\n\\(\\lambda\\) stands for the amount that the stock price will move in units of basis points for a given \\(\\log\\) dollar volume of trade. A small \\(\\lambda\\) indicates high liquidity, and a high \\(\\lambda\\) indicates low liquidity.\n\\(\\lambda\\) can be be calculated using rolling averages on the time series data with the TTR package, specifically the function runMean which when used within a dplyr pipeline will calculate the mean over the past \\(n\\) data points. For example, the command:\n\nlibrary(TTR)\nstocks |&gt; group_by(symbol) |&gt;  mutate(log_return_20d = runMean(log_return,n=20))\n\nadds a new variable which is equal to the mean of the log_return over the past 20 days. The mathematical formula for \\(\\lambda\\) is: \\[\n\\lambda = \\frac{\\mathrm{mean}(R_a\\log( p_c V ))\n- \\mathrm{mean}\\left(R_a\\right) \\mathrm{mean}\\left(\\log\\left(p_c V\\right) \\right) }\n{\\mathrm{mean}\\left(\\log\\left( p_c V \\right)^2\\right)\n-\\mathrm{mean}\\left(\\log(p_c V)\\right)^2 }\n\\] where to make the formula easier to read we have defined \\(R_a = |\\mathrm{return} -1|\\), \\(p_c = \\mathrm{close}\\) and \\(V = \\mathrm{volume}\\), and the averages have been taken over the past 20 days of data.\n\nAdd a new variable called kyle to the data frame by implementing the above formula for \\(\\lambda\\). Make sure to read and implement the formula very carefully, and to use the runMean function to calculate the rolling average correctly.\nPlot Kyle’s lambda for each stock over time (I would use a faceted scatterplot). What do you notice about how this measure of liquidity behaves (remember liquidity is high when \\(\\lambda\\) is small)?\nNext add a new variable to the dataframe called extreme which is true when the log_return for a given stock is either greater than 95% of other values of the log_return or less than 95% of all values of log_return. Use the percent_rank dplyr window function along with logical operators to create this variable. Then for each stock calculate the mean value of Kyle’s lambda for the days when the log_return had extreme values and for when it didn’t (as identified by the extreme variable). What do your calculations and figures indicate about liquidity during extreme events?"
  },
  {
    "objectID": "assignments/labs/Lab7.html",
    "href": "assignments/labs/Lab7.html",
    "title": "Lab 7: Webscraping and APIs",
    "section": "",
    "text": "Overview\nThis is a two part assignment. In the first part you will use the webscraping package rvest along with functions and iteration to download data from the internet on political contributions. In the second part, you will use httr to interact with the New York Times website apis.\n\n\nWebscraping Open Secrets\nIn this assignment we will scrape and work with data foreign connected PACs that donate to US political campaigns. In the United States, only American citizens and green card holders can contribute to federal elections, but the American divisions of foreign companies can form political action committees (PACs) and collect contributions from their American employees.\nFirst, we will get data foreign connected PAC contributions in the 2022 election cycle. Then, you will use a similar approach to get data such contributions from previous years so that we can examine trends over time.\nIn order to complete this assignment you will need a Chrome browser with the Selector Gadget extension installed.\nIn addition to tidyverse, you will need to install and load the packages robotstxt, rvest, and scales.\nProblem 1: Check that open secrets allows you to webscrape by running the paths_allowed function on the url https://www.opensecrets.org. Then write a function called scrape_pac() that scrapes information from the Open Secrets webpage for foreign connected PAC contributions in a given year:\n\nThis function should take the url of the webpage as its only input and should output a data frame\nIt should rename scraped variables using the “snake_case” naming convention\nclean up the Country of Origin/Parent Company variable with str_squish().\nadd a new column to the data frame for year. We will want this information when we ultimately have data from all years, so this is a good time to keep track of it. Our function doesn’t take a year argument, but the year is embedded in the URL, so we can extract it out of there, and add it as a new column. Use the str_sub() function to extract the last 4 characters from the URL. You will probably want to look at the help for this function to figure out how to specify “last 4 characters”.\n\nDefine the URLs for 2022, 2020, and 2000 contributions. Then, test your function using these URLs as inputs. Does the function seem to do what you expected it to do?\nProblem 2: Construct a vector called urls that contains the URLs for each webpage that contains information on foreign-connected PAC contributions for a given year. Map the scrape_pac() function over urls in a way that will result in a data frame called pac_all. Write the data frame to a csv file called pac-all.csv in the data folder.\nProblem 3: Clean your downloaded data file for visualization.\n\nFirst report the number of observations and variables that your scraped datasset contains.\nSeparate the country_parent into two such that country and parent company appear in different columns for country-level analysis.\nConvert contribution amounts in total, dems, and repubs from character strings to numeric values.\nPrint out the top 10 rows of your data frame after completing these steps.\n\nProblem 4: Create a line plot of total contributions from all foreign-connected PACs in the Canada and Mexico over the years. Once you have made the plot, write a brief interpretation of what the graph reveals. Few hints to help you out:\nProblem 5: Create a line plot of contributions from UK companies to each of the Democratic and Republican parties over time, with the lines colored according to party to differentiate them.\n\n\nUsing the NY Times API\nProblem 6: The New York Times web site provides a rich set of APIs, as described here . You’ll need to start by signing up for an API key. Your task is to choose one of the New York Times APIs, construct an interface in R to read in the JSON data, and transform it into an R DataFrame."
  },
  {
    "objectID": "assignments/tidygit/tidyextend.html",
    "href": "assignments/tidygit/tidyextend.html",
    "title": "Tidyverse Extend",
    "section": "",
    "text": "Overview\nIn this assignment, you’ll practice collaborating around a code project with GitHub. You could consider our collective work as building out a book of examples on how to use TidyVerse functions.\nGitHub repository: https://github.com/georgehagstrom/Fall2024TIDYVERSE\nYour task here is to Extend an Existing Example. Using one of your classmate’s examples (as created above), extend his or her example with additional annotated code.\nYou should clone the provided repository. Once you have code to submit, you should make a pull request on the shared repository. You should also update the README.md file with your example.\nAfter you’ve extended your classmate’s vignette, please submit your GitHub handle name on brightspace.\nYou should complete your submission on the schedule stated in the course syllabus."
  },
  {
    "objectID": "assignments/participation.html",
    "href": "assignments/participation.html",
    "title": "Participation",
    "section": "",
    "text": "One Minute Papers\nA “one minute paper” (Angelo & Cross, 1993) is a short written reflection to be completed after each class meetup. You are to answer two questions: 1) What was the most important thing you learned during this class? and 2) What important question remains unanswered for you? Our goal is to give you a moment to reflect on the most important concepts presented were and to provide me with information about what concepts are still unclear. At the completion of each meetup (whether attended live or after watching the recording), complete the Google Form linked from the last slide.\n\n\nSlack\nPlease be an active participant in the Slack channel. In addition to being a good resource for asking and answer questions, we hope you begin to make connections with other students. Contribute at least one resource related to data science that you find and think would be useful to your fellow students. Please post it in the #resource channel.",
    "crumbs": [
      "Assignments",
      "Participation"
    ]
  },
  {
    "objectID": "assignments/project.html",
    "href": "assignments/project.html",
    "title": "Data Project",
    "section": "",
    "text": "The idea of this project is for you to find data and do something interesting with it that relates to the skills you have developed in this course.\nThe project is intentionally open ended- this is an opportunity for you to explore your interests, through using techniques or technologies that branch off those that were introduced in this class, datasets related to problems you face in your career or that you are curious about, or both.\nA great project will demonstrate a complete “Data Science Workflow”, such as the one introduced at the beginning of the textbook, starting from multiple distinct datasets from public sources (if you have a private dataset that you want to use you may discuss that with me), data tidying, exploratory data analysis, cleaning, hypothesis generation, and lastly modeling. This course has not focused on modeling and the mathematical sophistication of your model will not factor heavily in your evaluation, but by this point in the semester you will have been exposed to enough modeling techniques in other courses to incorporate models within your project.\nThe output of your project should software that allows me to reproduce your work and a research report which describes what you did with the data and your findings, which could be contained within a github repository. You will also present your project in a short presentation in the final week of the semester.\n\n\nThe first step of your project is to submit an initial proposal, which will help to ensure that your project is both worthwhile and feasible in the time frame of the last half of the semester. Your proposal should describe the area that you are interested in (and if you have a specific question already you can describe that too), the datasets you are going to use, and your data analysis plan. Your proposal should be a maximum of 2 pages (excluding figures and references), and should include three sections (each with target length 1-2 paragraphs):\n\nSection 1 - Introduction: The introduction should describe the motivation for your project, the subject matter area of your dataset, and any general research questions you may already have.\nSection 2 - Data: At the time of submitting the proposal you should have already identified some datasets that you plan to use in your analysis. Give a quick description of those datasets here, describe where the data can be found, the number of variables and observations, and/or the output of the glimpse() or skim() functions on the data.\nSection 3 - Data analysis plan: Describe your initial approach to analyzing your data. How will your datasets need to be processed in for your to make use of all of them in the analysis? Which sort of comparisons do you plan to make in your exploratory data analysis (you can share early visualizations of this is you have them)? What statistical methods do you think are appropriate to address the questions that motivated your interest?\n\nThe proposal is preliminary, and you can include additional data or a different analysis as needs require.\n\n\n\nWhen writing proposals, I’ve found the following set of considerations, called “Heilmeier's Questions”, to be helpful to consider. Although this is just a short project, you might find them helpful as well, and useful for formulating future projects and proposals that you might have to complete. These questions were developed by Dr. George Heilmeier, who was the director of DARPA from 1975-1977. Heilmeier said that every proposal to DARPA needed to answer these questions clearly and completely in order to receive funding:\n\nWhat are you trying to do? Articulate your objectives using absolutely no jargon. What is the problem? Why is it hard?\nHow is it done today, and what are the limits of current practice?\nWhat is new in your approach, and why do you think it will be successful?\nWho cares?\nIf you’re successful, what difference will it make? What applications are enabled as a result?\nWhat are the risks?\nHow much will it cost?\nHow long will it take?\nWhat are the midterm and final “exams” to check for success? How will progress be measured?\n\n\n\n\nThe emphasis of class has been working with datasets that have different types of variables and which are messy or often heterogeneous. For this project you need to select data that comes from multiple sources (there should be at least two datasets). You should be able to access the data and it should be large enough and contain enough variables so that multiple interesting relationships can be explored. A good starting point is that across your dataset, there should be at least 50 observations and more than 10 variables (exceptions can be made but you must speak with me first). The dataset’s variables should include categorical variables, discrete numerical variables, and continuous numerical variables.\nNote on reusing datasets from class: Do not reuse datasets used in examples, homework assignments, or labs in the class. Also do not use datasets exclusively from Kaggle or other sources of curated data for data science competitions (these have been made clean and tidy already).\nBelow are a list of data repositories that might be of interest to browse. You're not limited to these resources, and in fact you're encouraged to venture beyond them. But you might find something interesting there:\n\nNew York City Open Data\nFiveThirtyEight https://github.com/fivethirtyeight/data\nRStudio data sources http://blog.rstudio.org/2014/07/23/new-data-packages/\nAnalyze Survey Data for Free (ASDFree) has many open data sources that can be used http://www.asdfree.com/\nThe World Bank Data Catalog http://datacatalog.worldbank.org/\nGoogle Public Data search engine http://www.google.com/publicdata/directory\nVanderbilt data sources http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets\nProgramme of International Student Assessment (PISA) http://www.oecd.org/pisa/\nBehavioral Risk Factor Surveillance System (BRFSS) http://www.cdc.gov/brfss/\nWorld Values Survey http://www.worldvaluessurvey.org/wvs.jsp\nAmerican National Election Survey (ANES) http://www.electionstudies.org/\nGeneral Social Survey (GSS) http://www3.norc.org/GSS+Website/\nIntegrated Postsecondary Education Data System (IPEDS) https://nces.ed.gov/ipeds/\nU.S. Census and American Community Survey https://cran.r-project.org/web/packages/acs/index.html\n10 Standard Datasets for Practicing Applied Machine Learning\nAwesome Public Datasets\nUCI Machine Learning Repository - See also this R package: https://github.com/tyluRp/ucimlr\nOpenML\nBikeshare data portal\nUK Gov Data\nYouth Risk Behavior Surveillance System (YRBSS)\nPRISM Data Archive Project\nHarvard Dataverse\n\n\n\n\nThe project report should include an Abstract which gives a summary of your project in under 300 words, and Introduction, a section on the Data which describes the cleaning, tidying, and exploratory data analysis steps you took and what hypotheses you generated, a section on the Data Analysis, and a Conclusion section. All of the code you used to perform your cleaning and analysis should be included either in a github repository or in your report (if you submit in a quarto format for example). You will be evaluated using the following rubric:\n\n\n\nYou will record a short video presentation with a slide-deck presenting your project. It should cover the motivation for your project, describe the data and analysis, and show visualizations of your main conclusions. Each student is required to watch two of the uploaded presentations and provided anonymous feedback on the presentations.\n\n\n\n\n\n\n\n\n\n\n\n\nDomain\nAccomplished (100%)\nProficient (80%)\nNeeds Improvement (60%)\n\n\n\n\nProposal (20 pts)\nMotivation explained well, data science workflow clearly present, datasets and proposed analysis appropriate\nDescription of motivation, workflow, datasets, and analysis present but a few are not appropriate or unclear\nSeveral of motivation, workflow, datasets, and analysis are missing or inappropriate\n\n\nAbstract (5 pts)\nAbstract is less than 300 words, free of grammatical errors, summarizes the project analysis, conclusions, and implications\nNA\nNA\n\n\nIntroduction (20 pts)\nClear explanation of motivation for the project, choice of data, and analysis methods/workflow.\nRationale for the project, analysis/workflow, or choice of data is present but unclear.\nRationale is unstated.\n\n\nDatasets and Wrangling (30 pts)\nMultiple datasets used with different data types, data is properly tidied, reproducible code for tidying\nOne of the previous criteria is missing.\nSeveral criteria not met: insufficient datasets and types, incorrect tidying, non-reproducible code\n\n\nExploratory Data Analysis (30 pts)\nAppropriate summary statistics and visualization of distributions/covariance, hypotheses, and treatment of missing values/outliers, code reproducible\nEDA completed but some of summary statistics, visualizations, hypotheses, and missing data treatment not appropriate\nEDA substantially inappropriate or has several aspects missing\n\n\nData Display (20 pts)\nIncludes appropriate, well-labeled, accurate displays (graphs and tables) of the data.\nIncludes appropriate, accurate displays of the data.\nIncludes appropriate but no accurate displays of the data.\n\n\nStatistical Model (5 pts)\nstatistical test(s) was used for the data and interpretation was clear.\nThe appropriate statistical test(s) was used but interpretation was not fully clear or well articulated.\nThe incorrect statistical test was used an/or not justified for the data as presented.\n\n\nConclusion (20 pts)\nConclusion includes a clear description of results consistent with the EDA and statistical modeling and discusses limitations of analysis and how to go further.\nConclusion describes results, limitations, and potential future steps but some descriptions inappropriate or unclear\nDescription of results, limitations, and future steps missing and/or unclear\n\n\nOverall Presentation (20 pts)\nAttractive, well-organized, well-written presentation\nPresentation has two of the three qualities: attractive, well-organized, well-written.\nPresentation is not attractive, organized, or written. There are numerous errors throughout.",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#heilmeiers-questions",
    "href": "assignments/project.html#heilmeiers-questions",
    "title": "Data Project",
    "section": "",
    "text": "When writing proposals, I’ve found the following set of considerations, called “Heilmeier's Questions”, to be helpful to consider. Although this is just a short project, you might find them helpful as well, and useful for formulating future projects and proposals that you might have to complete. These questions were developed by Dr. George Heilmeier, who was the director of DARPA from 1975-1977. Heilmeier said that every proposal to DARPA needed to answer these questions clearly and completely in order to receive funding:\n\nWhat are you trying to do? Articulate your objectives using absolutely no jargon. What is the problem? Why is it hard?\nHow is it done today, and what are the limits of current practice?\nWhat is new in your approach, and why do you think it will be successful?\nWho cares?\nIf you’re successful, what difference will it make? What applications are enabled as a result?\nWhat are the risks?\nHow much will it cost?\nHow long will it take?\nWhat are the midterm and final “exams” to check for success? How will progress be measured?",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#data",
    "href": "assignments/project.html#data",
    "title": "Data Project",
    "section": "",
    "text": "The emphasis of class has been working with datasets that have different types of variables and which are messy or often heterogeneous. For this project you need to select data that comes from multiple sources (there should be at least two datasets). You should be able to access the data and it should be large enough and contain enough variables so that multiple interesting relationships can be explored. A good starting point is that across your dataset, there should be at least 50 observations and more than 10 variables (exceptions can be made but you must speak with me first). The dataset’s variables should include categorical variables, discrete numerical variables, and continuous numerical variables.\nNote on reusing datasets from class: Do not reuse datasets used in examples, homework assignments, or labs in the class. Also do not use datasets exclusively from Kaggle or other sources of curated data for data science competitions (these have been made clean and tidy already).\nBelow are a list of data repositories that might be of interest to browse. You're not limited to these resources, and in fact you're encouraged to venture beyond them. But you might find something interesting there:\n\nNew York City Open Data\nFiveThirtyEight https://github.com/fivethirtyeight/data\nRStudio data sources http://blog.rstudio.org/2014/07/23/new-data-packages/\nAnalyze Survey Data for Free (ASDFree) has many open data sources that can be used http://www.asdfree.com/\nThe World Bank Data Catalog http://datacatalog.worldbank.org/\nGoogle Public Data search engine http://www.google.com/publicdata/directory\nVanderbilt data sources http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets\nProgramme of International Student Assessment (PISA) http://www.oecd.org/pisa/\nBehavioral Risk Factor Surveillance System (BRFSS) http://www.cdc.gov/brfss/\nWorld Values Survey http://www.worldvaluessurvey.org/wvs.jsp\nAmerican National Election Survey (ANES) http://www.electionstudies.org/\nGeneral Social Survey (GSS) http://www3.norc.org/GSS+Website/\nIntegrated Postsecondary Education Data System (IPEDS) https://nces.ed.gov/ipeds/\nU.S. Census and American Community Survey https://cran.r-project.org/web/packages/acs/index.html\n10 Standard Datasets for Practicing Applied Machine Learning\nAwesome Public Datasets\nUCI Machine Learning Repository - See also this R package: https://github.com/tyluRp/ucimlr\nOpenML\nBikeshare data portal\nUK Gov Data\nYouth Risk Behavior Surveillance System (YRBSS)\nPRISM Data Archive Project\nHarvard Dataverse",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#project-report-and-code-repository",
    "href": "assignments/project.html#project-report-and-code-repository",
    "title": "Data Project",
    "section": "",
    "text": "The project report should include an Abstract which gives a summary of your project in under 300 words, and Introduction, a section on the Data which describes the cleaning, tidying, and exploratory data analysis steps you took and what hypotheses you generated, a section on the Data Analysis, and a Conclusion section. All of the code you used to perform your cleaning and analysis should be included either in a github repository or in your report (if you submit in a quarto format for example). You will be evaluated using the following rubric:",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#presentation",
    "href": "assignments/project.html#presentation",
    "title": "Data Project",
    "section": "",
    "text": "You will record a short video presentation with a slide-deck presenting your project. It should cover the motivation for your project, describe the data and analysis, and show visualizations of your main conclusions. Each student is required to watch two of the uploaded presentations and provided anonymous feedback on the presentations.",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#overall-grading-rubric",
    "href": "assignments/project.html#overall-grading-rubric",
    "title": "Data Project",
    "section": "",
    "text": "Domain\nAccomplished (100%)\nProficient (80%)\nNeeds Improvement (60%)\n\n\n\n\nProposal (20 pts)\nMotivation explained well, data science workflow clearly present, datasets and proposed analysis appropriate\nDescription of motivation, workflow, datasets, and analysis present but a few are not appropriate or unclear\nSeveral of motivation, workflow, datasets, and analysis are missing or inappropriate\n\n\nAbstract (5 pts)\nAbstract is less than 300 words, free of grammatical errors, summarizes the project analysis, conclusions, and implications\nNA\nNA\n\n\nIntroduction (20 pts)\nClear explanation of motivation for the project, choice of data, and analysis methods/workflow.\nRationale for the project, analysis/workflow, or choice of data is present but unclear.\nRationale is unstated.\n\n\nDatasets and Wrangling (30 pts)\nMultiple datasets used with different data types, data is properly tidied, reproducible code for tidying\nOne of the previous criteria is missing.\nSeveral criteria not met: insufficient datasets and types, incorrect tidying, non-reproducible code\n\n\nExploratory Data Analysis (30 pts)\nAppropriate summary statistics and visualization of distributions/covariance, hypotheses, and treatment of missing values/outliers, code reproducible\nEDA completed but some of summary statistics, visualizations, hypotheses, and missing data treatment not appropriate\nEDA substantially inappropriate or has several aspects missing\n\n\nData Display (20 pts)\nIncludes appropriate, well-labeled, accurate displays (graphs and tables) of the data.\nIncludes appropriate, accurate displays of the data.\nIncludes appropriate but no accurate displays of the data.\n\n\nStatistical Model (5 pts)\nstatistical test(s) was used for the data and interpretation was clear.\nThe appropriate statistical test(s) was used but interpretation was not fully clear or well articulated.\nThe incorrect statistical test was used an/or not justified for the data as presented.\n\n\nConclusion (20 pts)\nConclusion includes a clear description of results consistent with the EDA and statistical modeling and discusses limitations of analysis and how to go further.\nConclusion describes results, limitations, and potential future steps but some descriptions inappropriate or unclear\nDescription of results, limitations, and future steps missing and/or unclear\n\n\nOverall Presentation (20 pts)\nAttractive, well-organized, well-written presentation\nPresentation has two of the three qualities: attractive, well-organized, well-written.\nPresentation is not attractive, organized, or written. There are numerous errors throughout.",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Announcements",
    "section": "",
    "text": "Be sure to check this page regularly for updates. In addition to periodic updates about the course, slides and recordings of the weekly meetups will be posted here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 8: Functions and Iteration\n\n\n\n\n\nClick here for meetup 8 slides and Zoom link\n\n\n\n\n\nOct 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 Info: Advanced R Programming and Project Proposals\n\n\n\n\n\nClick here for info on Week 8\n\n\n\n\n\nOct 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Databases\n\n\n\n\n\nClick here for a discussion of databases, SQL, and dbplyr\n\n\n\n\n\nOct 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Create a Database from Within R using DBI\n\n\n\n\n\nClick here for a link to a code vignette and a youtube video of a coding demo on databases\n\n\n\n\n\nOct 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Vignette: Joins and Lahman Batting Database\n\n\n\n\n\nClick here for a link to a code vignette and a youtube video of a coding demo focused on debugging and reprex\n\n\n\n\n\nOct 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 7: R and SQL\n\n\n\n\n\nClick here for meetup 7 slides and Zoom link\n\n\n\n\n\nOct 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7 Info: Joins and Databases\n\n\n\n\n\nClick here for info on Week 7\n\n\n\n\n\nOct 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Changes in US Baby Names Using stringr, forcats, and regex\n\n\n\n\n\nClick here for a link to a code vignette and a youtube video which illustrates the use of the stringr and forcats libraries as well as regular expressions to explore the variation of baby names in the US over time\n\n\n\n\n\nOct 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 6: Working With Text and Strings\n\n\n\n\n\nClick here for meetup 5 slides and Zoom link\n\n\n\n\n\nOct 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6 Info: Working With Text and Strings\n\n\n\n\n\nClick here for info on Week 6\n\n\n\n\n\nSep 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Correlation Trick (End of Meetup 5)\n\n\n\n\n\nClick here for a link to a youtube video covering a few extra slides from Meetup 5\n\n\n\n\n\nSep 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData Transformation Coding Demo\n\n\n\n\n\nClick here for a link to a code vignette and a youtube video of a coding demo\n\n\n\n\n\nSep 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 5: Data Transformations\n\n\n\n\n\nClick here for meetup 5 slides and Zoom link\n\n\n\n\n\nSep 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5 Info: Data Transformations\n\n\n\n\n\nClick here for info on Week 5\n\n\n\n\n\nSep 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow to find help and make a reproducible example (Debugging)\n\n\n\n\n\nClick here for a link to a code vignette and a youtube video of a coding demo focused on debugging and reprex\n\n\n\n\n\nSep 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 4: Exploratory Data Analysis\n\n\n\n\n\nClick here for meetup 4 slides and Zoom link\n\n\n\n\n\nSep 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4 Info: Exploratory Data Analysis\n\n\n\n\n\nClick here for info on Week 4\n\n\n\n\n\nSep 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPivot Coding Demo\n\n\n\n\n\nClick here for a link to a code vignette and a youtube video of a coding demo\n\n\n\n\n\nSep 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 3: Tidy Data\n\n\n\n\n\nClick here for meetup 2 slides and Zoom link\n\n\n\n\n\nSep 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3 Info: Data Tidying\n\n\n\n\n\nClick here for info on Week 3\n\n\n\n\n\nSep 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2 Info: Data Visualizations and Transformations\n\n\n\n\n\nClick here for info on Week 2\n\n\n\n\n\nSep 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 1: Data Science Workflow and Toolkit\n\n\n\n\n\nClick here for meetup 1 slides and Zoom link\n\n\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to DATA 607\n\n\n\n\n\nImportant information on how to get started with this course. Click this post now for instructions on getting started.\n\n\n\n\n\nAug 17, 2024\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Announcements"
    ]
  },
  {
    "objectID": "assignments/labs.html",
    "href": "assignments/labs.html",
    "title": "Labs",
    "section": "",
    "text": "The primary homework assignments in this course are lab assignments where you will use R and occassionally other software to acquire, explore, wrangle, and manage different data sets. Please submit a PDF (preferred) or HTML file along with your qmd file. Labs should be submitted on Blackboard.\n\n\nIntroduction to Data Visualization using ggplot (Template)\n\n\nTidy Data (Template)\n\n\nExploratory Data Analysis (Template)\n\n\nData Transformations (Template)\n\n\nProcesing Text and Strings (Template)\n\n\nR and SQL (Template)\n\n\nWeb Scraping and APIs (Template)\n\n\nText Mining and NLP (Template)\n\n\nGraphs and Graph Data (Template)\n\n\nLarge Datasets and Cloud Computing (Template)",
    "crumbs": [
      "Assignments",
      "Labs"
    ]
  },
  {
    "objectID": "assignments/dsincontext.html",
    "href": "assignments/dsincontext.html",
    "title": "Data Science in Context Presentations",
    "section": "",
    "text": "Overview\nEach student needs to complete one data science in context presentation. These are short presentations with a target length of 5-7 minutes. At the beginning of the semester you should sign up for a presentation slot using this sign up sheet. The presentations will be made live in class, with an opportunity for a few minutes of question or discussion. You can pick any topic that interests you and is related to data science. If you have a strong preference not to present live, you may record and upload your presentation separately."
  },
  {
    "objectID": "assignments/tidygit/tidycreate.html",
    "href": "assignments/tidygit/tidycreate.html",
    "title": "Tidyverse Create",
    "section": "",
    "text": "Overview\nIn this assignment, you’ll practice collaborating around a code project with GitHub. You could consider our collective work as building out a book of examples on how to use TidyVerse functions.\nGitHub repository: https://github.com/georgehagstrom/Fall2024TIDYVERSE\nYour task here is to Create an Example Using one or more TidyVerse packages, and any dataset from fivethirtyeight.com or Kaggle, create a programming sample “vignette” that demonstrates how to use one or more of the capabilities of the selected TidyVerse package with your selected dataset.\nLater, you’ll be asked to extend an existing vignette.  Using one of your classmate’s examples (as created above), you’ll then extend his or her example with additional annotated code.\nYou should clone the provided repository. Once you have code to submit, you should make a pull request on the shared repository. You should also update the README.md file with your example.\nAfter you’ve created your vignette, please submit your GitHub handle name in the submission link provided below. .\nYou should complete your submission on the schedule stated in the course syllabus."
  },
  {
    "objectID": "assignments/labs/Lab3.html",
    "href": "assignments/labs/Lab3.html",
    "title": "Lab3: Exploratory Data Analysis",
    "section": "",
    "text": "This is a two part lab where each part will focus on a different dataset: the first part will use a dataset containing a series of diagnostic measurements taken on members of the Akimel O’odham people (an indigenous group living in the Southwestern United States who are also called the Pima) to understand diabetes risk (click here to download diabetes.csv), and the second dataset contains information on traffic accidents in New York City in the months of July and August of this year, and was compiled by NYC Open Data (click here to download crashes.csv).\nFor this problem set you will need to install the skimr and GGally packages, and in particular the functions skim and ggpairs.\nWe will also explore the concept of an inlier, which is an erroneous value that occurs in the interior of the distribution of a variable, rather than in the tails of the variable. The US Census published an article on the problem of inliers here"
  },
  {
    "objectID": "assignments/labs/Lab3.html#overview",
    "href": "assignments/labs/Lab3.html#overview",
    "title": "Lab3: Exploratory Data Analysis",
    "section": "",
    "text": "This is a two part lab where each part will focus on a different dataset: the first part will use a dataset containing a series of diagnostic measurements taken on members of the Akimel O’odham people (an indigenous group living in the Southwestern United States who are also called the Pima) to understand diabetes risk (click here to download diabetes.csv), and the second dataset contains information on traffic accidents in New York City in the months of July and August of this year, and was compiled by NYC Open Data (click here to download crashes.csv).\nFor this problem set you will need to install the skimr and GGally packages, and in particular the functions skim and ggpairs.\nWe will also explore the concept of an inlier, which is an erroneous value that occurs in the interior of the distribution of a variable, rather than in the tails of the variable. The US Census published an article on the problem of inliers here"
  },
  {
    "objectID": "assignments/labs/Lab3.html#part-1-health-diagnostics-and-diabetes-incidence",
    "href": "assignments/labs/Lab3.html#part-1-health-diagnostics-and-diabetes-incidence",
    "title": "Lab3: Exploratory Data Analysis",
    "section": "Part 1: Health Diagnostics and Diabetes Incidence",
    "text": "Part 1: Health Diagnostics and Diabetes Incidence\nProblem 1: Data Description and Outliers.\nLoad diabetes.csv into R and take a look at the data using the skimr package (make sure to install it if you don’t have it). Skimr provides a tidy summary function called skim. Use skim on the data frame that you loaded from diabetes.csv.\nSkim will list several variables. Pregnancies is the past number of pregnancies (this dataset includes women 21 years or older), glucose describes the concentration of glucose in the blood after an oral glucose tolerance test (drinking a sugary drink and measuring two hours later), skin thickness is the result of a skinfold thickness test taken at the triceps (upper arm), Insulin is the insulin concentration in the blood taken at the same time as the glucose measurement (Insulin is a hormone that transports glucose into cells), BMI is “Body Mass Index”, Diabetes Pedigree Function is a measure of diabetes risk based on the family history of diabetes for each patient (this is an engineered feature) and outcome is equal to 1 if the patient was diagnosed with diabetes with 5 years and 0 otherwise.\n\nSkim should show no missing data, but should indicate potential data issues. Do any of the percentile ranges (p0, p25, p50, p75, or p100) for the reported variables suggest a potential problem?\nFurther investigate the dataset to find potentially problematic variables using a qq-plot (geom_qq) or group_by combined with count and arrange. For which variables do you find repeated values and what are those values? Do you believe these values represent real measurements or could they correspond to missing data? Do the repeated variables occur in the same rows or different rows?\n\nWrite an overview of which values are missing and replace all missing values with NA for the next stage of analysis.\n\nPerform Tukey Box plots on each variable to identify potential outliers. Which variables have the most outliers? Are there any outliers that you think come from measurement error? If so remove them.\n\nProblem 2: Pair Plots\nUse the GGally package and its function ggpair on both the original dataset and the cleaned dataset. Which correlations change the most? What are the strongest correlations between variables overall and with the Outcome?\n\nRemark: This dataset has been used an model dataset for the construction of binary classifiers using machine learning and there are a large number of published studies showing these analyses. However, many of these analyses did not exclude the missing values erroneously coded as zero, as is discussed in this interesting paper by Breault, leading to highly degraded accuracy."
  },
  {
    "objectID": "assignments/labs/Lab3.html#part-2-car-crashes-in-nyc",
    "href": "assignments/labs/Lab3.html#part-2-car-crashes-in-nyc",
    "title": "Lab3: Exploratory Data Analysis",
    "section": "Part 2: Car Crashes in NYC",
    "text": "Part 2: Car Crashes in NYC\nProblem 3: Finding Inliers and Missing Data\nLoad the NYC car crash dataset using read_csv. You can download the data from the course website by clicking here.\n\nWhich variables have missing data (use skim or another tool of your choosing)? Some missing values have a different interpretation than others- what does it mean when VEHICLE TYPE CODE 2 is missing compared to LATITUDE?\nLatitude and Longitude have the same number of missing values. Verify that they always occur in the same row. Check the counts of latitude and longitude values- do you find any hidden missing values? If so recode them as NA.\nMany of the geographic values are missing, but geographic information is redundant in multiple variables in the dataset. For example, with effort you could determine the borough of an accident from the zip code, the latitude and longitude, or the streets (not part of the assignment for this week). Consider the borough variable- what percentage of the missing values of borough have values present of at least one of zip code or latitude. What about if we include all the street name variables? What fraction of rows don’t have any detailed location information (latitude, zip code, or street names)?\nThe CRASH TIME variable has no missing values. Compute the count of how many times each individual time occurs in the crash data set. This will suggest that there are some inliers in the data. Compute summary statistics on the count data, and determine how many inliers there are (define an inlier as a data value where the count is an outlier, i.e. the count of that value is greater than 1.5*IQR + P75, i.e. 1.5 times the interquartile range past the 75th percentile for the distribution of counts for values of that variable.) For which inliers do you believe the time is most likely to be accurate? For which is it least likely to be accurate and why do you think so?\n\nProblem 4: Finding Patterns in the Data\nFormulate a question about crash data in NYC and make visualizations to explore your question. It could be related to the geographical distribution of accidents, the timing of accidents, which types of vehicles lead to more or less dangerous accidents, or anything else you want. Write comments/notes describing your observations in each visualizations you create and mention how these observations impact your initial hypotheses.\nUseful questions to consider when you observe a pattern:\n\nCould this pattern be due to coincidence (i.e. random chance)?\nHow can you describe the relationship implied by the pattern?\nHow strong is the relationship implied by the pattern?\nWhat other variables might affect the relationship?\nDoes the relationship change if you look at individual subgroups of the data?"
  },
  {
    "objectID": "assignments/labs/labData/sports_program_data_dictionary.html",
    "href": "assignments/labs/labData/sports_program_data_dictionary.html",
    "title": "Sports Program Data Dictionary",
    "section": "",
    "text": "Year Year data was collected\nunitid Unique code which identifies institutions\ninstitution_name Name of institution\ncity_txt City in which institution is located\nstate_cd 2 digit state code identifying the university\nzip_text 5 digit zip code where the university is located\nclassification_code Numerical code which identifies the sports division in which the school belongs. Note that these are structured by the overall division (Division I, II, III etc) and also some other features such as whether there is an American Football team and which grouping that football team competes in.\nclassification_name Name of the classification the university sports teams belong to\nclassification_other Description of classification if university belongs to a different sports league than the one enumerated previously\nef_male_count Count of males in the student body\nef_female_count Count of females in the student body\nef_total_count Total number of students at the institution\nsector_cd Numeric code identifying the type of university\nsector_name Description of university type\nsportscode Numeric code identifying the sport that the team plays\npartic_men Number of men participating on men’s team\npartic_women Number of women participating on women’s team\npartic_coed_men Number of men participating on coed team\npartic_coed_women Number of women participating on coed team\nsum_partic_men Sum of men participating in this sport\nsum_partic_women Sum of women participating in this sport\nrev_men Revenue generated by Men’s Team\nrev_women Revenue generated by Women’s Team\ntotal_rev_men_women Revenue across all teams\nexp_men Expenses for men’s team\nexp_women Expenses for women’s team\ntotal_exp_menwomen Expenses across both teams\nsports Name of sport"
  },
  {
    "objectID": "assignments/labs/labData/sports_program_data_dictionary.html#list-of-variables",
    "href": "assignments/labs/labData/sports_program_data_dictionary.html#list-of-variables",
    "title": "Sports Program Data Dictionary",
    "section": "",
    "text": "Year Year data was collected\nunitid Unique code which identifies institutions\ninstitution_name Name of institution\ncity_txt City in which institution is located\nstate_cd 2 digit state code identifying the university\nzip_text 5 digit zip code where the university is located\nclassification_code Numerical code which identifies the sports division in which the school belongs. Note that these are structured by the overall division (Division I, II, III etc) and also some other features such as whether there is an American Football team and which grouping that football team competes in.\nclassification_name Name of the classification the university sports teams belong to\nclassification_other Description of classification if university belongs to a different sports league than the one enumerated previously\nef_male_count Count of males in the student body\nef_female_count Count of females in the student body\nef_total_count Total number of students at the institution\nsector_cd Numeric code identifying the type of university\nsector_name Description of university type\nsportscode Numeric code identifying the sport that the team plays\npartic_men Number of men participating on men’s team\npartic_women Number of women participating on women’s team\npartic_coed_men Number of men participating on coed team\npartic_coed_women Number of women participating on coed team\nsum_partic_men Sum of men participating in this sport\nsum_partic_women Sum of women participating in this sport\nrev_men Revenue generated by Men’s Team\nrev_women Revenue generated by Women’s Team\ntotal_rev_men_women Revenue across all teams\nexp_men Expenses for men’s team\nexp_women Expenses for women’s team\ntotal_exp_menwomen Expenses across both teams\nsports Name of sport"
  },
  {
    "objectID": "assignments/labs/Lab6.html",
    "href": "assignments/labs/Lab6.html",
    "title": "Lab 6: R and SQL",
    "section": "",
    "text": "Overview\nThis lab is divided into two parts. In the first part you will practice using joins for data wrangling and analysis on the nycflights dataset. Some of the problems come from Chapter 19 of your book. For the second part, you will download a dataset on the budgets of college sports programs and process it for storage in a relational database (I strongly recommend using duckdb which can be installed using install.packages(\"duckdb\")- duckdb is highly performant, self-contained, and ideally suited both to learning SQL and performing data analysis). Then you will load this database and use dbplyr to perform an analysis. You will also practice using forcats to recode some of the variables as factors (which are supported by duckdb) and using separate_wider_delim to split columns of text data.\nYou will need to have installed and to the following libraries:\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(nycflights13)\n\n\n\nProblems\nPart I: Airline Flight Delays\nFor the first part of this lab exercise, we will be using the nycflights library, which contains several different built in datasets including planes, which has information on each plane that appears in the data; flights, which has information on individual flights; airports, which has information on individual airports; and weather, which has information on the weather that the origin airports. In order to do this set of lab exercises, you will need to use different types of joins to combine variables in each data frame.\nProblem 1\n\nUse the flights and planes tables to compute the mean departure delay of each aircraft that has more than 30 recorded flights in the dataset. Hint: Make note of the fact that the variable year appears in both flights and planes but means different things in each before performing any joins.\nUse anti-join to identify flights where tailnum does not have a match in plane. Determine the carriers for which this problem is the most common.\nFind the airplane model which made the most flights in the dataset, and filter the dataset to contain only flights flown by airplanes of that model, adding a variable which corresponds to the year each those airplanes were built. Then compute the average departure delay for each year of origin and plot the data. Is there any evidence that older planes have more greater departure delays?\n\nProblem 2\n\nCompute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States:\n\n\nairports |&gt;\n  semi_join(flights, join_by(faa == dest)) |&gt;\n  ggplot(aes(x = lon, y = lat)) +\n    borders(\"state\") +\n    geom_point() +\n    coord_quickmap()\n\nYou might want to use the size or color of the points to display the average delay for each airport.\nPart II: Creating and Accessing a Database \nIn this exercise we will begin with a flat file which contains data on college sports programs throughout the country. The source of the data is a government run database called Equity in Athletics Data Analysis, though we are working with just a small subset here. You can download this file by clicking here: sports_program_costs.csv. I have also included a data dictionary which gives a quick description of the dataset, which can be downloaded from here: sports_program_data_dictionary.qmd. This file contains information on two types of entities: sports teams and universities, however the information on both entities is combined into a single table, creating substantial redundancies. This exercise has several goals:\n\nLoad this data into R, split the dataframe into two dataframes, one corresponding to colleges and another corresponding to sports teams, related to each other by common keys. Many databases are stored according to normalization rules, which are designed to limit redundancy and to make it easier to both work with the data and make changes to it. By splitting the data frame we will partially normalize it (but won’t go too far).\nCreate a relational database using duckdb which contains these two tables.\nRead this database into R and a/an SQL query/queries to perform an analysis.\n\nProblem 3:\n\nsports_program_data.csv contains variables which either describe properties of a sports team or a college. Split sports_programs_data into two data frames, one called colleges and another called teams. How can you tell which variables describe colleges and which describe teams? Use the data dictionary and observations of how the values vary as you move from college to college to help make the decision easier. Make sure there are primary keys for both the colleges and teams data frames (verify with count)- what are the primary keys in each case and are they simple keys (one variable) or compound keys (require multiple variables). One of these data-sets should contain a foreign key- which one has it and what variables comprise it?\nThe variable sector_name contains information about whether a college is public, private, non-profit, for-profit, a 2-year college, or a 4-year + college. Split this variable (using separate_wider_delim) into two variables, one of which describes whether the college is a Public, Private nonprofit, or private for-profit, and another which describes how many years the college programs run.\nSeveral variables are candidates to be recoded as factors, for example state_cd, zip_text, classification_name, sports, and the sector variables you just created for the previous part. Recode these variables as categorical variables. For the classification variable, use the classification_code to order the factors according to the numeric code.\n\nProblem 4\n\nUsing DBI, duckdb, and dbplyr, create a relational database with two tables, writing the sports data frame you created in problem 3 to one and the colleges data frame (also from problem 3) to the other. Write this database to disk. How does the size of the database file compare to the original csv?\nUse dbplyr to write a query to this database that calculates the top 10 colleges ranked by the average profit (defined as revenue - expenses) of their american football team over the years of data. Print the SQL query that results from your R pipeline using show_query() and then use collect() to show the results of this query."
  },
  {
    "objectID": "assignments/labs/Lab8.html",
    "href": "assignments/labs/Lab8.html",
    "title": "Lab 8: Text Mining and NLP",
    "section": "",
    "text": "Overview\nIn this lab, we will use the advanced R programming concepts that we studied to process a complex text file for analysis.\nProblem 1:\nIn Text Mining with R, Chapter 2 looks at Sentiment Analysis. In this assignment, you should start by getting the primary example code from chapter 2 working in an R Markdown document. You should provide a citation to this base code.\nProblem 2 Extend this base code in two ways:\n\nWork with a different corpus of your choosing\nIncorporate at least one additional sentiment lexicon (possibly from another R package that you’ve found through research)."
  },
  {
    "objectID": "assignments/labs/Lab1.html",
    "href": "assignments/labs/Lab1.html",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "",
    "text": "Airbnb has had a disruptive effect on the hotel, rental home, and vacation industry throughout the world. The success of Airbnb has not come without controversies, with critics arguing that Airbnb has adverse impacts on housing and rental prices and also on the daily lives of people living in neighborhoods where Airbnb is popular. This controversy has been particularly intense in NYC, where the debate been Airbnb proponents and detractors eventually led to the city imposing strong restrictions on the use of Airbnb. If you find this issue interesting and want to go deeper, there is the potential for an interesting project that brings in hotels (which have interesting regulations in NYC), hotel price data, and rental data and looks at these things together.\nBecause Airbnb listings are available online through their website and app, it is possible for us to acquire and visualize the impacts of Airbnb on different cities, including New York City. This is possible through the work of an organization called inside airbnb"
  },
  {
    "objectID": "assignments/labs/Lab1.html#airbnb-in-nyc-or-your-city",
    "href": "assignments/labs/Lab1.html#airbnb-in-nyc-or-your-city",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "",
    "text": "Airbnb has had a disruptive effect on the hotel, rental home, and vacation industry throughout the world. The success of Airbnb has not come without controversies, with critics arguing that Airbnb has adverse impacts on housing and rental prices and also on the daily lives of people living in neighborhoods where Airbnb is popular. This controversy has been particularly intense in NYC, where the debate been Airbnb proponents and detractors eventually led to the city imposing strong restrictions on the use of Airbnb. If you find this issue interesting and want to go deeper, there is the potential for an interesting project that brings in hotels (which have interesting regulations in NYC), hotel price data, and rental data and looks at these things together.\nBecause Airbnb listings are available online through their website and app, it is possible for us to acquire and visualize the impacts of Airbnb on different cities, including New York City. This is possible through the work of an organization called inside airbnb"
  },
  {
    "objectID": "assignments/labs/Lab1.html#github-instructions",
    "href": "assignments/labs/Lab1.html#github-instructions",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "Github Instructions",
    "text": "Github Instructions\nBefore we introduce the data and the main assignment, let’s begin with a few key steps to configure the file and create a github repository for your first assignment. This is optional but I think it is a good idea to start getting familiar with github tools. These steps come from the happygitwithr tutorial.\n\nStart a new github repository in your account, clone it to your computer (using RStudio to start a new project from a repository or any other way)\nUpdate the YAML, changing the author name to your name, and Render the document.\nCommit your changes with a meaningful commit message.\nPush your changes to GitHub.\nGo to your repo on GitHub and confirm that your changes are visible in your Qmd and md files. If anything is missing, commit and push again."
  },
  {
    "objectID": "assignments/labs/Lab1.html#packages",
    "href": "assignments/labs/Lab1.html#packages",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation, and the ggridges package to make a ridge plot in the last exercise. You may need to install ggridges if you haven’t already, you can do that using:\n\ninstall.packages(\"ggridges\")\n\nThen make sure to load both packages:"
  },
  {
    "objectID": "assignments/labs/Lab1.html#data",
    "href": "assignments/labs/Lab1.html#data",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "Data",
    "text": "Data\nThe data for this assignment can be found on my github page by clicking here and downloading nycbnb.csv\nIf you are adventurous and want to perform this assignment for a different city you can choose one from inside airbnb). If you go that route, make sure to download the file listings.csv.gz for the city you selected (gz is an archive format which you should be able to expand), and you will only need to keep the following columns for this analysis:\n\n  nycbnb = nycbnb |&gt; \n    select(\n      id, \n      price, \n      neighbourhood_cleansed,\n      neighbourhood_group_cleansed,\n      accommodates, \n      bathrooms, \n      bedrooms, \n      beds, \n      review_scores_rating, \n      number_of_reviews, \n      listing_url )\n\nYou will also need to do some post-processing, including changing the price column from a string to a numerical variable. If you decide to go this custom route let me know and make sure to share your data, but I recommend sticking with the data I provided.\nYou can read the data into R using the command:\n\nnycbnb = read_csv(\"/home/georgehagstrom/work/Teaching/DATA607/website/data/nycbnb.csv\")\n\nwhere you should replace /home/georgehagstrom/work/Teaching/DATA607/website/data/nycbnb.csv\"nycbnb.csv with the local path to your file.\nImportant note: It is generally not wise to include datasets in github repositories, especially if they are large and can change frequently.\nYou can view the dataset as a spreadsheet using the View() function. Note that you should not put this function in your R Markdown document, but instead type it directly in the Console, as it pops open a new window (and the concept of popping open a window in a static document doesn’t really make sense…). When you run this in the console, you’ll see the following data viewer window pop up."
  },
  {
    "objectID": "assignments/tidygit.html",
    "href": "assignments/tidygit.html",
    "title": "GitHub/TidyVerse Create and Extend",
    "section": "",
    "text": "The primary homework assignments in this course are lab assignments where you will use R and occassionally other software to acquire, explore, wrangle, and manage different data sets. Please submit a PDF (preferred) or HTML file along with your Rmarkdown file. Be sure to answer all questions in lab, not just the on your own section. Labs should be submitted on Blackboard.\n\n\nTidyverse Create (Template)\n\n\nTidyverse Extend (Template)",
    "crumbs": [
      "Assignments",
      "GitHub/Tidyverse Recipes"
    ]
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#what-is-data-science",
    "href": "meetups/Meetup1/Meetup1.html#what-is-data-science",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "What is Data Science?",
    "text": "What is Data Science?\n\n\nData science is a “discipline that allows you to transform raw data into understanding, insight, and knowledge”\n\n\n\n\nI hear often: “Data Science is just statistics with a clever brand name”\n\n\n\n\nIs this a misconception?"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from text"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-1",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-1",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textLoad the data from files into software"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-2",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-2",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textTidy the data so it is stored in a consistent way"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-3",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-3",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textTransform the data to focus our analysis on observations of interest"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-4",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-4",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textVisualize the data to find relationships, problems, and pose questions"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-5",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-5",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textModel the data to answer questions precisely using statistics"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-6",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-6",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textCommunicate to share results with others"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-7",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-7",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textThis class will focus on everything but modeling, i.e. the part of Data Science that isn’t statistics"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#modeling-can-be-small-part-of-data-science-projects",
    "href": "meetups/Meetup1/Meetup1.html#modeling-can-be-small-part-of-data-science-projects",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Modeling can be small part of Data Science projects",
    "text": "Modeling can be small part of Data Science projects\nIt is said that 80% of time in data science projects is spent on data mining, cleaning, tidying, exploratory data analysis, etc\n\nFigure from ForbesPlease forgive the Pie Chart"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#introcase-study",
    "href": "meetups/Meetup1/Meetup1.html#introcase-study",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Intro/Case Study",
    "text": "Intro/Case Study\n\n\n\n\n\nTrait Correlations in Marine Bacteria\n\n\n\n\nData on how bacteria get their food in the ocean\nGetting data for this plot took months…..\nMany sources, data formats, quality issues, processing"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#learning-objectives",
    "href": "meetups/Meetup1/Meetup1.html#learning-objectives",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the course, you will have a foundation of skills in the Data Science Workflow\n\nFind data you need and do all steps to prep it for analysis\nBuild expertise in R and the tidyverse\nUse and understand relational databases and SQL\nCollaborate with Git and GitHub\nIntroduce you to distributed computing and other tools for large datasets\nImprove your programming ability"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#vignette-electricity-and-co2",
    "href": "meetups/Meetup1/Meetup1.html#vignette-electricity-and-co2",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Vignette: Electricity and CO2",
    "text": "Vignette: Electricity and CO2\n\nSources of Power, refs last slide"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#electricity-generation-over-time",
    "href": "meetups/Meetup1/Meetup1.html#electricity-generation-over-time",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Electricity Generation Over Time",
    "text": "Electricity Generation Over Time\n\nSource: Our World in Data"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#carbon-intensity-of-electricity",
    "href": "meetups/Meetup1/Meetup1.html#carbon-intensity-of-electricity",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Carbon Intensity of Electricity",
    "text": "Carbon Intensity of Electricity\n\nSource: Our World in Data"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#controls-on-carbon-intensity",
    "href": "meetups/Meetup1/Meetup1.html#controls-on-carbon-intensity",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Controls on Carbon Intensity",
    "text": "Controls on Carbon Intensity\n\nSource: Our World in Data"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#link-to-the-vignette",
    "href": "meetups/Meetup1/Meetup1.html#link-to-the-vignette",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Link to the Vignette",
    "text": "Link to the Vignette\nYou can download the vignette from my github by clicking here\nRemember to download the data if you want to render the file."
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#syllabus-and-course-site",
    "href": "meetups/Meetup1/Meetup1.html#syllabus-and-course-site",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Syllabus and Course Site",
    "text": "Syllabus and Course Site\n\nFull Syllabus on the course website:\n\nhttps://georgehagstrom.github.io/DATA607/\nCourse website contains links to weekly reading and homework assignments, meetup videos, course schedule, and other course materials\n\nUse the Brightspace page to submit assignments, either in pdf format or a link to an html on some site I can access (ie github or rpubs)"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#meetups",
    "href": "meetups/Meetup1/Meetup1.html#meetups",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Meetups",
    "text": "Meetups\n\n6:45-7:45 on Wednesday evening. Attending live preferred, watch video after if you can’t\nOffice Hours: On Zoom by appointment\nCommunication and collaboration: https://fall2024data607.slack.com"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#assignments",
    "href": "meetups/Meetup1/Meetup1.html#assignments",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Assignments",
    "text": "Assignments\n\nLabs (50%): Weekly Programming assignments\nTidyVerse Recipes (10%): Collaborative intro to Git\nProject (25%)\n\nAssemble and explore a data set of your choosing\nExplore your interests, build your portfolio!\n\nData Science in Context Presentation (5%)\n\nOne 5 minute presentation, sign up for your presentation slot asap!\n\nMeetup Reflections and Introduction (10%)"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#schedule",
    "href": "meetups/Meetup1/Meetup1.html#schedule",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Schedule",
    "text": "Schedule"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#textbooks",
    "href": "meetups/Meetup1/Meetup1.html#textbooks",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Textbooks",
    "text": "Textbooks\n\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2023). R for Data Science (2e). O’Reilly\nJennifer Bryan. Happy Git and GitHub for the R User.\nJulia Silge and David Robinson (2017). Text Mining with R. O’Reilly\n\nRecommended: Wickham, H. Advanced R. Baca Raton, FL: Taylor & Francis Group."
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#tidyverse-opinionated-ecosystem",
    "href": "meetups/Meetup1/Meetup1.html#tidyverse-opinionated-ecosystem",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Tidyverse: Opinionated Ecosystem",
    "text": "Tidyverse: Opinionated Ecosystem\n\n\n\n\n\nCollection of compatible packages\nShared philosophy, common grammar\nStrong Core, Many Extensions\nAdvantages and Disadvantages"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#what-to-do-this-week",
    "href": "meetups/Meetup1/Meetup1.html#what-to-do-this-week",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "What to do this week?",
    "text": "What to do this week?\n\nReadings:\n\nIntro and Chapter 28 of R4DS\nSections 1-15 of Happy Git\nQuarto Tutorial\nAppendix on R Help Files\n\nGet software installed and configured:\n\n\n\nR, RStudio, git, latex\n\n\n\nWrite a post introducing yourself on Brightspace Discussions\nSign up for your Data Science in Context Presentation"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#image-references",
    "href": "meetups/Meetup1/Meetup1.html#image-references",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Image References",
    "text": "Image References\n\nCoal: By Morgre - Own work, CC BY-SA 3.0\nGas/Methane: By Georg Slickers - Self-photographed, CC BY-SA 3.0\nHydro: By Source file: Le Grand PortageDerivative work: Rehman - File:Three_Gorges_Dam,_Yangtze_River,_China.jpg, CC BY 2.0\nSolar: By Parabel GmbH - Own work, CC BY-SA 3.0\nWind: By Erik Wilde from Berkeley, CA, USA - harvesting wind, CC BY-SA 2.0\n\n\n\n\n\nDATA 607"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#week-overview",
    "href": "meetups/Meetup4/Meetup4.html#week-overview",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Week Overview",
    "text": "Week Overview\n\nGraded Most of Lab 2\n\nPlease submit pdf and wrap your code in code chunks!\n\nEDA Lab due Sunday at midnight\nPosted coding demo on reprex/debugging\nHadley Wickham EDA Demo\nSign up for Data Science in Context!\nToday’s focus is EDA (Chapter 10)\n\nVast subject, entire books on it\nNot from book: Inliers, QQ-Plots, Dot-Plots, Pair Plots"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#case-study-barley-yields-in-mn",
    "href": "meetups/Meetup4/Meetup4.html#case-study-barley-yields-in-mn",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Case Study: Barley Yields in MN",
    "text": "Case Study: Barley Yields in MN\n\n10 Varieties of Barley\nGrown at 6 Different Sites\nYield Measured in 1931 and 1932\nData Used as an Exemplar in Stats Literature"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#barley-data",
    "href": "meetups/Meetup4/Meetup4.html#barley-data",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Barley Data",
    "text": "Barley Data\n\n\n\n\n\nsite\nyear\nvariety\nyield\n\n\n\n\nUniversity Farm\n1931\nManchuria\n27.00000\n\n\nWaseca\n1931\nManchuria\n48.86667\n\n\nMorris\n1931\nManchuria\n27.43334\n\n\nCrookston\n1931\nManchuria\n39.93333\n\n\nGrand Rapids\n1931\nManchuria\n32.96667\n\n\nDuluth\n1931\nManchuria\n28.96667\n\n\nUniversity Farm\n1931\nGlabron\n43.06666\n\n\nWaseca\n1931\nGlabron\n55.20000\n\n\nMorris\n1931\nGlabron\n28.76667\n\n\nCrookston\n1931\nGlabron\n38.13333"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#visualize-the-data",
    "href": "meetups/Meetup4/Meetup4.html#visualize-the-data",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Visualize the Data",
    "text": "Visualize the Data"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#swapped-years-at-morris",
    "href": "meetups/Meetup4/Meetup4.html#swapped-years-at-morris",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Swapped Years at Morris?",
    "text": "Swapped Years at Morris?"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#years-later-investigation-commences",
    "href": "meetups/Meetup4/Meetup4.html#years-later-investigation-commences",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "80 Years Later… Investigation Commences",
    "text": "80 Years Later… Investigation Commences"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#evidence-emerges",
    "href": "meetups/Meetup4/Meetup4.html#evidence-emerges",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Evidence Emerges",
    "text": "Evidence Emerges\n\nAmbiguity in years of study noticed (1930 and 1931 versus 1931 and 1932)\nSwapping of a different sample discovered\nStatistical fits much more parsimonious on swapped data\nI would bet on data error, but we may never know without more experiments"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#what-is-exploratory-data-analysis",
    "href": "meetups/Meetup4/Meetup4.html#what-is-exploratory-data-analysis",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "What is Exploratory Data Analysis",
    "text": "What is Exploratory Data Analysis\nExploratory Data Analysis is the art of looking at data in a systematic way in order to understand the the underlying structure of the data.\nTwo main goals:\n\nEnsure data quality\nUncover patterns to guide future analysis\n\nEDA is detective work- you ask and answer questions, which inspires more questions"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#eda-steps",
    "href": "meetups/Meetup4/Meetup4.html#eda-steps",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "EDA Steps",
    "text": "EDA Steps\n\nGeneral Characteristics of Data and Descriptive Stats (summary, skim, counts)\nVisualize Variation (Histograms, QQ Plots, Box plots, …)\nDeal with outliers, inliers, missing data\nVisualize relationships (bivariate and multivariate plots)\nIterate"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#quantile-quantile-qq-plots",
    "href": "meetups/Meetup4/Meetup4.html#quantile-quantile-qq-plots",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Quantile-Quantile (QQ) Plots",
    "text": "Quantile-Quantile (QQ) Plots\n\nSorts data in increasing order, plots against sorted data from a probability distribution (usually Gaussian) or other dataset\nMost powerful/underused univariate visualization\n\nCreate percentile vec seq(0,1,1/(N+1))\nDrop ends and calculate data using qnorm(seq)\nScatterplot of Gaussian quantiles on x axis, your data on y\ngeom_qq, stat_qq do this for you"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#qq-plot-for-us-cereals-data",
    "href": "meetups/Meetup4/Meetup4.html#qq-plot-for-us-cereals-data",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "QQ Plot for US Cereals Data",
    "text": "QQ Plot for US Cereals Data"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#qq-plot-for-us-cereals-data-1",
    "href": "meetups/Meetup4/Meetup4.html#qq-plot-for-us-cereals-data-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "QQ Plot for US Cereals Data",
    "text": "QQ Plot for US Cereals Data"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#qq-plot-for-us-cereals-data-2",
    "href": "meetups/Meetup4/Meetup4.html#qq-plot-for-us-cereals-data-2",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "QQ Plot for US Cereals Data",
    "text": "QQ Plot for US Cereals Data"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#comparing-visualizations",
    "href": "meetups/Meetup4/Meetup4.html#comparing-visualizations",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Comparing visualizations",
    "text": "Comparing visualizations"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#outliers",
    "href": "meetups/Meetup4/Meetup4.html#outliers",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Outliers",
    "text": "Outliers\n\nOutlier is a data point you suspect was generated by a different mechanism than the rest of your dataset\nCould be a spurrious value or a major discovery\nHeuristic for suspecting outliers:\n\n\\[ y &gt; p_{75} + k \\mathrm{IQR}\\] Common to pick \\(k=1.5\\)\n\nYour domain specific expertise tells you what to do with outliers!"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#inliers",
    "href": "meetups/Meetup4/Meetup4.html#inliers",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Inliers",
    "text": "Inliers\n\nInliers: Value in the interior of the distribution of your data that is in error\nMuch more subtle and pernicious than outliers\nOften Disguised Missing Data\n\nNA values systematically coded as 0 or some other default\nBusiness or government transactions that require a some information for a form to be filled out that isn’t always available\n\nCommon Manifestation in Repated Values"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#check-for-inliers",
    "href": "meetups/Meetup4/Meetup4.html#check-for-inliers",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Check for Inliers",
    "text": "Check for Inliers\n\nFlat regions of your QQ-Plot\nLook for outliers of your count data:\n\n\nUScereal |&gt; \n  group_by(fibre) |&gt;\n  summarise(count = n()) |&gt; \n  arrange(desc(count)) |&gt; \n  ggplot(aes(x=fibre,y=count)) +\n  geom_point()\n\n\nOnce you find them investigate\nCan also use boxplots, look at your data, or compute summary statistics"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#visualizing-covariation",
    "href": "meetups/Meetup4/Meetup4.html#visualizing-covariation",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Visualizing Covariation",
    "text": "Visualizing Covariation\n\nCovariation is how two variables vary together\nNumerical and Categorical\n\nNumerical variable has difference distribution for different values of the categorical variable\n\nTools: Boxplots, violin plots\n\nTwo Numerical Variables\n\nScatterplots, 2D density plots such as hex-plots\nPairs Plots"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#tukey-box-plot",
    "href": "meetups/Meetup4/Meetup4.html#tukey-box-plot",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Tukey Box Plot",
    "text": "Tukey Box Plot\n\nThe Tukey Box Plot is a robust visualization\nMedian, IQR, potential outliers"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#tukey-box-plot-1",
    "href": "meetups/Meetup4/Meetup4.html#tukey-box-plot-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Tukey Box Plot",
    "text": "Tukey Box Plot\n\nThe Tukey Box Plot is a robust visualization\nMedian, IQR, potential outliers"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#tukey-box-plot-2",
    "href": "meetups/Meetup4/Meetup4.html#tukey-box-plot-2",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Tukey Box Plot",
    "text": "Tukey Box Plot\n\nThe Tukey Box Plot is a robust visualization\nMedian, IQR, potential outliers\nCode:\n\n\nggplot(diamonds, aes(x = cut, y = price)) +\n  geom_boxplot() +\n  theme_bw(base_size=24)"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#eda-detective-work",
    "href": "meetups/Meetup4/Meetup4.html#eda-detective-work",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "EDA Detective Work",
    "text": "EDA Detective Work\n\nWhen you make a plot- ask questions\nShouldn’t high quality cost more?"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#look-at-other-variables",
    "href": "meetups/Meetup4/Meetup4.html#look-at-other-variables",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Look at other variables",
    "text": "Look at other variables\n\nClearer diamonds aren’t more expensive"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#look-at-other-variables-1",
    "href": "meetups/Meetup4/Meetup4.html#look-at-other-variables-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Look at other variables",
    "text": "Look at other variables\n\nBigger diamonds are very pricey"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#how-does-size-covary-with-cutclarity",
    "href": "meetups/Meetup4/Meetup4.html#how-does-size-covary-with-cutclarity",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "How does size covary with cut/clarity?",
    "text": "How does size covary with cut/clarity?\n\nBig diamonds have worse clarity"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#how-does-size-covary-with-cutclarity-1",
    "href": "meetups/Meetup4/Meetup4.html#how-does-size-covary-with-cutclarity-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "How does size covary with cut/clarity?",
    "text": "How does size covary with cut/clarity?\n\nBig diamonds have slightly worse clarity"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#simpsons-paradox",
    "href": "meetups/Meetup4/Meetup4.html#simpsons-paradox",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\n\nFor fixed size, quality leads to higher price\nSize is a common cause of price and quality"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#density-plots",
    "href": "meetups/Meetup4/Meetup4.html#density-plots",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Density Plots",
    "text": "Density Plots\n\nWhen there are lots of points, density plots are better than scatter plots"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#density-plots-1",
    "href": "meetups/Meetup4/Meetup4.html#density-plots-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Density Plots",
    "text": "Density Plots\n\nWhen there are lots of points, density plots are better than scatter plots\n\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_hex() +\n  theme_bw(base_size=24)"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#pair-plots-to-visualize-all-combos",
    "href": "meetups/Meetup4/Meetup4.html#pair-plots-to-visualize-all-combos",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Pair Plots to Visualize All Combos",
    "text": "Pair Plots to Visualize All Combos\n\nGGally package ggpairs function"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#pair-plots-to-visualize-all-combos-1",
    "href": "meetups/Meetup4/Meetup4.html#pair-plots-to-visualize-all-combos-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Pair Plots to Visualize All Combos",
    "text": "Pair Plots to Visualize All Combos\n\npenguins |&gt; ggpairs(columns=3:6)"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#fancy-pairs",
    "href": "meetups/Meetup4/Meetup4.html#fancy-pairs",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Fancy Pairs",
    "text": "Fancy Pairs\n\npenguins |&gt; ggpairs(columns=3:6,ggplot2::aes(colour = species))"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#multiway-visualizations-dot-plots",
    "href": "meetups/Meetup4/Meetup4.html#multiway-visualizations-dot-plots",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Multiway Visualizations: Dot Plots",
    "text": "Multiway Visualizations: Dot Plots\n\nMultiple categorical variables and a response\nCleveland and McGill 1984\nBest to worst rank of perceptual cues for learning:\n\n\n\nDot-Chart created to emphasize position comparisons"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#dot-plots",
    "href": "meetups/Meetup4/Meetup4.html#dot-plots",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Dot Plots",
    "text": "Dot Plots"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#dot-plots-1",
    "href": "meetups/Meetup4/Meetup4.html#dot-plots-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Dot Plots",
    "text": "Dot Plots\n\nbarley |&gt; \n  ggplot(aes(x=yield,y=variety,color=year)) +\n  geom_point(size=2) +\n  facet_wrap(~site,nrow = 3) +\n  theme_bw(base_size = 10)"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#what-didnt-we-talk-about",
    "href": "meetups/Meetup4/Meetup4.html#what-didnt-we-talk-about",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "What Didn’t We Talk About",
    "text": "What Didn’t We Talk About\n\nSimple Models are used a lot in EDA\n\nSee example in Chapter 10\nResiduals remove the effect of a major variable and see what remains\n\nTransformations\n\nWe will discuss transformations more next week\nReexpress data in new way\nMost important by far is log transform"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup4/Meetup4.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#week-summary",
    "href": "meetups/Meetup2/Meetup2.html#week-summary",
    "title": "Meetup 2: Visualizing Data",
    "section": "Week Summary",
    "text": "Week Summary\n\n1. Read Chapters 1-4 of R4DS (Work out the problems as you go!)\n\n2. Supplementary Readings- Data Viz (Wilke), Calling BS (West and Bergstrom)\n\n3. Lab 1: Airbnb in NYC- submit qmd file and pdf or html link on Brightspace by Sunday Midnight\n\n4. Data Science in Context- sign up if you haven’t yet\n\n5. If you don’t have a working software setup speak up in Slack!!!"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#why-make-graphs",
    "href": "meetups/Meetup2/Meetup2.html#why-make-graphs",
    "title": "Meetup 2: Visualizing Data",
    "section": "Why Make Graphs?",
    "text": "Why Make Graphs?\n\nGraphs are the main way we communicate data\nAlternative would be summary statistics, but visualizations are much richer.\nBig debate within statistics community because summary stats perceived as more “rigorous”\nGraphs won the debate- they are the main and most important thing people will remember about your data analyses"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#anscombes-quartet",
    "href": "meetups/Meetup2/Meetup2.html#anscombes-quartet",
    "title": "Meetup 2: Visualizing Data",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\n\n\nSet 1:\n\n\n\n\n\nx\ny\n\n\n\n\n10\n8.04\n\n\n8\n6.95\n\n\n13\n7.58\n\n\n9\n8.81\n\n\n11\n8.33\n\n\n14\n9.96\n\n\n6\n7.24\n\n\n4\n4.26\n\n\n12\n10.84\n\n\n7\n4.82\n\n\n5\n5.68\n\n\n\n\n\n\nSet 2:\n\n\n\n\n\nx\ny\n\n\n\n\n10\n9.14\n\n\n8\n8.14\n\n\n13\n8.74\n\n\n9\n8.77\n\n\n11\n9.26\n\n\n14\n8.10\n\n\n6\n6.13\n\n\n4\n3.10\n\n\n12\n9.13\n\n\n7\n7.26\n\n\n5\n4.74\n\n\n\n\n\n\nSet 3:\n\n\n\n\n\nx\ny\n\n\n\n\n10\n7.46\n\n\n8\n6.77\n\n\n13\n12.74\n\n\n9\n7.11\n\n\n11\n7.81\n\n\n14\n8.84\n\n\n6\n6.08\n\n\n4\n5.39\n\n\n12\n8.15\n\n\n7\n6.42\n\n\n5\n5.73\n\n\n\n\n\n\nSet 4:\n\n\n\n\n\nx\ny\n\n\n\n\n8\n6.58\n\n\n8\n5.76\n\n\n8\n7.71\n\n\n8\n8.84\n\n\n8\n8.47\n\n\n8\n7.04\n\n\n8\n5.25\n\n\n19\n12.50\n\n\n8\n5.56\n\n\n8\n7.91\n\n\n8\n6.89"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#anscombe-summary-stats",
    "href": "meetups/Meetup2/Meetup2.html#anscombe-summary-stats",
    "title": "Meetup 2: Visualizing Data",
    "section": "Anscombe Summary Stats:",
    "text": "Anscombe Summary Stats:\n\n\n\n\n\nSet\nmean(x)\nmean(y)\nsd(x)\nsd(y)\ncor(x,y)\n\n\n\n\n1\n9\n7.500909\n3.316625\n2.031568\n0.8164205\n\n\n2\n9\n7.500909\n3.316625\n2.031657\n0.8162365\n\n\n3\n9\n7.500000\n3.316625\n2.030424\n0.8162867\n\n\n4\n9\n7.500909\n3.316625\n2.030578\n0.8165214\n\n\n\n\n\nAre the datasets the basically the same?"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#anscombe-visualized",
    "href": "meetups/Meetup2/Meetup2.html#anscombe-visualized",
    "title": "Meetup 2: Visualizing Data",
    "section": "Anscombe Visualized:",
    "text": "Anscombe Visualized:"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#d-example",
    "href": "meetups/Meetup2/Meetup2.html#d-example",
    "title": "Meetup 2: Visualizing Data",
    "section": "1D Example",
    "text": "1D Example"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#datasaurus-dozen",
    "href": "meetups/Meetup2/Meetup2.html#datasaurus-dozen",
    "title": "Meetup 2: Visualizing Data",
    "section": "Datasaurus Dozen",
    "text": "Datasaurus Dozen"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#non-contrived-example",
    "href": "meetups/Meetup2/Meetup2.html#non-contrived-example",
    "title": "Meetup 2: Visualizing Data",
    "section": "Non-Contrived Example",
    "text": "Non-Contrived Example\n\n\nTable shows the results of a linear regression between income inequality and voter turnout, claiming to show a statistically significant negative correlation (Jackman 1980)"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#outlier-caused-relationship",
    "href": "meetups/Meetup2/Meetup2.html#outlier-caused-relationship",
    "title": "Meetup 2: Visualizing Data",
    "section": "Outlier caused relationship",
    "text": "Outlier caused relationship"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#graphs-as-comparisons",
    "href": "meetups/Meetup2/Meetup2.html#graphs-as-comparisons",
    "title": "Meetup 2: Visualizing Data",
    "section": "Graphs as Comparisons",
    "text": "Graphs as Comparisons\nTwo purposes for graphs:\n\nUnderstand and communicate the size and direction of comparisons that were already of interest\nDiscover new patterns\n\nA graph is often all someone will remember about your data analysis, can make the difference between convincing important decision makers or not."
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#improve-this-graph",
    "href": "meetups/Meetup2/Meetup2.html#improve-this-graph",
    "title": "Meetup 2: Visualizing Data",
    "section": "Improve this Graph",
    "text": "Improve this Graph"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#reformulating-the-data",
    "href": "meetups/Meetup2/Meetup2.html#reformulating-the-data",
    "title": "Meetup 2: Visualizing Data",
    "section": "Reformulating the Data",
    "text": "Reformulating the Data\n\nTwo Possible Stories\n\nChildren had higher accuracy than adults\nMultiple objects is harder than an individual object\n\nCaption suggests Adult vs. Child accuracy\n\nSolution:\n\nInstead of plotting responses, plot accuracy\nPlot Accuracy of each group on the same axis"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#case-study-solution-line-plot",
    "href": "meetups/Meetup2/Meetup2.html#case-study-solution-line-plot",
    "title": "Meetup 2: Visualizing Data",
    "section": "Case Study Solution: Line Plot",
    "text": "Case Study Solution: Line Plot"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#ggplot2-basics",
    "href": "meetups/Meetup2/Meetup2.html#ggplot2-basics",
    "title": "Meetup 2: Visualizing Data",
    "section": "ggplot2 Basics",
    "text": "ggplot2 Basics\n\n\n\nggplot2 is tidyverse plotting package\nGrammar of Graphics\nKey figure components\n\ndata maps to aesthetics\ninterpreted using geoms\nothers: theme, facets, coordinates, stats\n\n\n\n Source: DS-Box"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#vignette-transit-costs",
    "href": "meetups/Meetup2/Meetup2.html#vignette-transit-costs",
    "title": "Meetup 2: Visualizing Data",
    "section": "Vignette: Transit Costs",
    "text": "Vignette: Transit Costs\n\nSource tidy tuesday Download Vignette Download Data"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#data-science-in-context-presentation",
    "href": "meetups/Meetup2/Meetup2.html#data-science-in-context-presentation",
    "title": "Meetup 2: Visualizing Data",
    "section": "Data Science in Context Presentation",
    "text": "Data Science in Context Presentation"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#categories-of-graphs",
    "href": "meetups/Meetup2/Meetup2.html#categories-of-graphs",
    "title": "Meetup 2: Visualizing Data",
    "section": "Categories of Graphs",
    "text": "Categories of Graphs"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#avoiding-wrong",
    "href": "meetups/Meetup2/Meetup2.html#avoiding-wrong",
    "title": "Meetup 2: Visualizing Data",
    "section": "Avoiding Wrong",
    "text": "Avoiding Wrong\nWrong figures are missing information needed to decipher the meaning or contain mathematical errors\n\nLabel all axes and provide units if ambiguous\nAll axes need scales\nAestethic elements accurately represent the data\nLegends and labels present when appropriate to identify meaning of other visual elements"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#ethics-principal-of-proportional-ink",
    "href": "meetups/Meetup2/Meetup2.html#ethics-principal-of-proportional-ink",
    "title": "Meetup 2: Visualizing Data",
    "section": "Ethics: Principal of Proportional Ink",
    "text": "Ethics: Principal of Proportional Ink\n\n“The representation of numbers, as physically measured on the surface of the graphic itself, should be directly proportional to the numerical quantities represented.” (1983, p.56) Edward Tufte, The Visual Display of Quantitative Information\n\n\nThis principle avoids many misleading visualizations"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#example",
    "href": "meetups/Meetup2/Meetup2.html#example",
    "title": "Meetup 2: Visualizing Data",
    "section": "Example",
    "text": "Example\n\n\n\n\n\nScale doesn’t start at 0\nWidth expands with height\nDanger in bar plots, filled line plots, plots with elements that scale with data"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#bar-plots-should-start-at-0",
    "href": "meetups/Meetup2/Meetup2.html#bar-plots-should-start-at-0",
    "title": "Meetup 2: Visualizing Data",
    "section": "Bar Plots Should Start at 0",
    "text": "Bar Plots Should Start at 0\n\n\nWrong\n\n\n\n\n\n\n\n\n\nOr make them line charts:\n\nRight"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#filled-plots-should-start-at-0",
    "href": "meetups/Meetup2/Meetup2.html#filled-plots-should-start-at-0",
    "title": "Meetup 2: Visualizing Data",
    "section": "Filled Plots Should Start at 0",
    "text": "Filled Plots Should Start at 0\n\n\nWrong\n\n\n\n\n\n\n\n\n\nOr transform data to represent a net change:\n\nRight"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#scale-area-not-radius",
    "href": "meetups/Meetup2/Meetup2.html#scale-area-not-radius",
    "title": "Meetup 2: Visualizing Data",
    "section": "Scale Area not Radius",
    "text": "Scale Area not Radius"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup2/Meetup2.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 2: Visualizing Data",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#acknowledgements",
    "href": "meetups/Meetup2/Meetup2.html#acknowledgements",
    "title": "Meetup 2: Visualizing Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nActive Statistics (Gelman, Hill, Vehtari)\nPrinciples of Data Visualization (Wilke)\nCalling BS (Bergstrom and West)\nData Visualization (Healy)\nImpact of Outliers on Income Inequality (Jackman)\n\n\n\n\n\nDATA 607"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#relational-databases",
    "href": "meetups/Meetup7/SQLSlides.html#relational-databases",
    "title": "SQL Slides",
    "section": "Relational Databases",
    "text": "Relational Databases\n\nDatabase is a collection of related tables\nDatabase schema defines the structure/relationships"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#features",
    "href": "meetups/Meetup7/SQLSlides.html#features",
    "title": "SQL Slides",
    "section": "Features",
    "text": "Features\n\nTables linked by keys\n\nCommon to create a separate unique index/id-number to serves as primary key\n\nTables correspond to entities\n\nEach table about 1 “thing”\n\nRows called records"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#entity-relationship-diagram-erd",
    "href": "meetups/Meetup7/SQLSlides.html#entity-relationship-diagram-erd",
    "title": "SQL Slides",
    "section": "Entity Relationship Diagram (ERD)",
    "text": "Entity Relationship Diagram (ERD)\n\nTeate SQL4DS"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#entity-relationship-diagram-erd-1",
    "href": "meetups/Meetup7/SQLSlides.html#entity-relationship-diagram-erd-1",
    "title": "SQL Slides",
    "section": "Entity Relationship Diagram (ERD)",
    "text": "Entity Relationship Diagram (ERD)\n\nTeate SQL4DS\nStars mark primary and foreign keys\n1 and Infinity symbol indicate one-many relationship"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#more-complex-erd",
    "href": "meetups/Meetup7/SQLSlides.html#more-complex-erd",
    "title": "SQL Slides",
    "section": "More Complex ERD",
    "text": "More Complex ERD\n\nTeate SQL4DS\nHere authors and books have a many-many relationship\nAssociative Table used as a junction Authors and Books\nCombo of ISBN and Author ID are primary key for Books-Authors"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#normalization-schemes",
    "href": "meetups/Meetup7/SQLSlides.html#normalization-schemes",
    "title": "SQL Slides",
    "section": "Normalization Schemes",
    "text": "Normalization Schemes\n\nDatabases often organized according to strict rules called normalization\nThese improve things like space efficiency, data consistency, etc\nMassive research topic in the 1970s"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#normalization-schemes-1",
    "href": "meetups/Meetup7/SQLSlides.html#normalization-schemes-1",
    "title": "SQL Slides",
    "section": "Normalization Schemes",
    "text": "Normalization Schemes\n\nDatabases often organized according to strict rules called normalization\nThese improve things like space efficiency, data consistency, etc\nMassive research topic in the 1970s\nIdeas often involve separating data into tables corresponding to entities\nEach fact stored in one place\nCan make a career as a database designer/architect"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#trade-offs",
    "href": "meetups/Meetup7/SQLSlides.html#trade-offs",
    "title": "SQL Slides",
    "section": "Trade Offs",
    "text": "Trade Offs\nWhen should you use a relational database instead of regular file(s) for your project?\n\nIf many people are using the data\nIf the data takes up lots of space\nIf the data has complex organization\nIf you are planning to scale up\n\nBut it is likely that your organization will be using it and so knowing how to interact with databases is key, will make you more efficient and more independent!"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#dbms-classes",
    "href": "meetups/Meetup7/SQLSlides.html#dbms-classes",
    "title": "SQL Slides",
    "section": "DBMS Classes",
    "text": "DBMS Classes\nUse Database Management Systems to access data/interact with databases\n\nClient-Server: (Most Traditional). Database hosted on a central server to which you connect (IBM, Oracle, MySQL server)\nCloud: Database hosted on cloud. Newer, easier to scale resources (Google, Amazon, Snowflake)\nIn Process: For smaller datasets with few users. Best for data analysis and learning (sqlite, duckdb)"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#structured-query-language-sql",
    "href": "meetups/Meetup7/SQLSlides.html#structured-query-language-sql",
    "title": "SQL Slides",
    "section": "Structured Query Language (SQL)",
    "text": "Structured Query Language (SQL)\n\nStored on disk, queried to generate smaller dataset for analysis elsewhere\nQueries composed of clauses (must be in order):\n\nSELECT, FROM, WHERE, GROUP BY, ORDER BY\nSELECT is combo of mutate, select, rename, summarize, relocate, summarize\nAlso has JOINS\n\ndbplyr translates tidyverse manipulations to SQL\nSQL is a standard with many flavors"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#connecting-to-a-database-in-r",
    "href": "meetups/Meetup7/SQLSlides.html#connecting-to-a-database-in-r",
    "title": "SQL Slides",
    "section": "Connecting to a Database in R",
    "text": "Connecting to a Database in R\n\nDBI library has functions to manipulate databases\n\n\nlibrary(DBI)\nlibrary(duckdb)\ncon = dbConnect(duckdb::duckdb(), dbdir = \"duckdb\")\n\n\ndbConnect() connections and initializes an empty database\nduckdb() creates an instance of a duck database\nCan use many options with DBI"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#writing-tables",
    "href": "meetups/Meetup7/SQLSlides.html#writing-tables",
    "title": "SQL Slides",
    "section": "Writing Tables",
    "text": "Writing Tables\n\ndbWriteTable writes data from R to your database\n\n\nlibrary(nycflights13)\n\ndbWriteTable(con, \"flights\", flights,overwrite=TRUE)\ndbWriteTable(con, \"planes\", planes,overwrite=TRUE)\n\ndbListTables(con)\n\n[1] \"flights\" \"planes\""
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#reading-tables",
    "href": "meetups/Meetup7/SQLSlides.html#reading-tables",
    "title": "SQL Slides",
    "section": "Reading Tables",
    "text": "Reading Tables\n\ndbReadTable reads data from your database to R\n\n\ncon |&gt; dbReadTable(\"flights\") |&gt; \n  as_tibble() |&gt; select(year:arr_time)\n\n# A tibble: 336,776 × 7\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#queries",
    "href": "meetups/Meetup7/SQLSlides.html#queries",
    "title": "SQL Slides",
    "section": "Queries",
    "text": "Queries\n\ndbGetQuery runs SQL queries\n\n\nquery = \"\nSELECT dep_delay, month, sched_dep_time\nFROM flights\nWHERE carrier = 'UA'\n\"\n\ncon |&gt; dbGetQuery(query) |&gt; as_tibble()\n\n# A tibble: 58,665 × 3\n   dep_delay month sched_dep_time\n       &lt;dbl&gt; &lt;int&gt;          &lt;int&gt;\n 1         2     1            515\n 2         4     1            529\n 3        -4     1            558\n 4        -2     1            600\n 5        -2     1            600\n 6        -1     1            600\n 7         0     1            607\n 8        11     1            600\n 9        -4     1            627\n10        -2     1            630\n# ℹ 58,655 more rows"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#dbplyr",
    "href": "meetups/Meetup7/SQLSlides.html#dbplyr",
    "title": "SQL Slides",
    "section": "dbplyr",
    "text": "dbplyr\n\nlibrary that translates dplyr to SQL\nUse it both to run queries and learn SQL\n\n\nlibrary(dbplyr)\nflights_tab = con |&gt; tbl(\"flights\")\nplanes_tab = con |&gt; tbl(\"planes\") |&gt; print(n=3)\n\n# Source:   table&lt;planes&gt; [?? x 9]\n# Database: DuckDB v1.1.0 [georgehagstrom@Linux 6.6.6-76060606-generic:R 4.4.1//home/georgehagstrom/work/Teaching/DATA607/website/meetups/Meetup7/duckdb]\n  tailnum  year type               manufacturer model engines seats speed engine\n  &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; \n1 N10156   2004 Fixed wing multi … EMBRAER      EMB-…       2    55    NA Turbo…\n2 N102UW   1998 Fixed wing multi … AIRBUS INDU… A320…       2   182    NA Turbo…\n3 N103US   1999 Fixed wing multi … AIRBUS INDU… A320…       2   182    NA Turbo…\n# ℹ more rows"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#lazy-evaluation",
    "href": "meetups/Meetup7/SQLSlides.html#lazy-evaluation",
    "title": "SQL Slides",
    "section": "Lazy Evaluation",
    "text": "Lazy Evaluation\n\ndbplyr doesn’t immediately execute code, builds up big query instead\nshow_query() to see it\n\n\nflights_tab |&gt; inner_join(planes_tab) |&gt; \n  group_by(tailnum) |&gt; \n  summarise(num_flights = n()) |&gt; \n  arrange(desc(num_flights)) |&gt; print(n=4)\n\n# Source:     SQL [?? x 2]\n# Database:   DuckDB v1.1.0 [georgehagstrom@Linux 6.6.6-76060606-generic:R 4.4.1//home/georgehagstrom/work/Teaching/DATA607/website/meetups/Meetup7/duckdb]\n# Ordered by: desc(num_flights)\n  tailnum num_flights\n  &lt;chr&gt;         &lt;dbl&gt;\n1 N354JB          333\n2 N355JB          282\n3 N358JB          271\n4 N374JB          236\n# ℹ more rows"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#lazy-evaluation-1",
    "href": "meetups/Meetup7/SQLSlides.html#lazy-evaluation-1",
    "title": "SQL Slides",
    "section": "Lazy Evaluation",
    "text": "Lazy Evaluation\n\ndbplyr doesn’t immediately execute code, builds up big query instead\nshow_query() to see it\n\n\nflights_tab |&gt; inner_join(planes_tab) |&gt; \n  group_by(tailnum) |&gt; \n  summarise(num_flights = n()) |&gt; \n  arrange(desc(num_flights)) |&gt; show_query()\n\n&lt;SQL&gt;\nSELECT tailnum, COUNT(*) AS num_flights\nFROM (\n  SELECT flights.*, \"type\", manufacturer, model, engines, seats, speed, engine\n  FROM flights\n  INNER JOIN planes\n    ON (flights.\"year\" = planes.\"year\" AND flights.tailnum = planes.tailnum)\n) q01\nGROUP BY tailnum\nORDER BY num_flights DESC"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#lazy-evaluation-2",
    "href": "meetups/Meetup7/SQLSlides.html#lazy-evaluation-2",
    "title": "SQL Slides",
    "section": "Lazy Evaluation",
    "text": "Lazy Evaluation\n\ndbplyr doesn’t immediately execute code, builds up big query instead\ncollect to execute it\n\n\nflights_tab |&gt; inner_join(planes_tab) |&gt; \n  group_by(tailnum) |&gt; \n  summarise(num_flights = n()) |&gt; \n  arrange(desc(num_flights)) |&gt; collect()\n\n# A tibble: 92 × 2\n   tailnum num_flights\n   &lt;chr&gt;         &lt;dbl&gt;\n 1 N354JB          333\n 2 N355JB          282\n 3 N358JB          271\n 4 N374JB          236\n 5 N373JB          232\n 6 N368JB          230\n 7 N827JB          125\n 8 N37465          111\n 9 N37468          102\n10 N36469          102\n# ℹ 82 more rows"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#sql-basics",
    "href": "meetups/Meetup7/SQLSlides.html#sql-basics",
    "title": "SQL Slides",
    "section": "SQL Basics",
    "text": "SQL Basics\n\nSQL queries composed of statements in order:\n\nSELECT, FROM, WHERE, GROUP BY, ORDER BY\n\nEvaluation occurs in different order:\nFROM WHERE GROUP BY SELECT ORDER BY\nSELECT FROM:\n\n\n flights_tab |&gt; show_query()\n\n&lt;SQL&gt;\nSELECT *\nFROM flights\n\nflights_tab |&gt; select(carrier, flight, arr_time) |&gt; show_query()\n\n&lt;SQL&gt;\nSELECT carrier, flight, arr_time\nFROM flights"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#sql-basics-1",
    "href": "meetups/Meetup7/SQLSlides.html#sql-basics-1",
    "title": "SQL Slides",
    "section": "SQL Basics",
    "text": "SQL Basics\n\nWHERE is like filter()\nORDER BY is like arrange()\n\n\nflights_tab |&gt; filter(dep_delay &gt; 30, carrier == 'UA') |&gt; \n  arrange(arr_delay) |&gt; show_query()\n\n&lt;SQL&gt;\nSELECT flights.*\nFROM flights\nWHERE (dep_delay &gt; 30.0) AND (carrier = 'UA')\nORDER BY arr_delay"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#select-aggregates",
    "href": "meetups/Meetup7/SQLSlides.html#select-aggregates",
    "title": "SQL Slides",
    "section": "SELECT aggregates",
    "text": "SELECT aggregates\n\nflights_tab |&gt; \n  group_by(carrier) |&gt; \n  summarise(dep_delay = mean(dep_delay,na.rm=TRUE),\n            num_flights = n()) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT carrier, AVG(dep_delay) AS dep_delay, COUNT(*) AS num_flights\nFROM flights\nGROUP BY carrier\n\n\n\nsummarise variables put in select statement"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#select-rename-relocate",
    "href": "meetups/Meetup7/SQLSlides.html#select-rename-relocate",
    "title": "SQL Slides",
    "section": "select, rename, relocate",
    "text": "select, rename, relocate\n\nplanes_tab |&gt; \n  select(tailnum, type, manufacturer, model, year) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT tailnum, \"type\", manufacturer, model, \"year\"\nFROM planes\n\nplanes_tab |&gt; \n  select(tailnum, type, manufacturer, model, year) |&gt; \n  rename(year_built = year) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT tailnum, \"type\", manufacturer, model, \"year\" AS year_built\nFROM planes\n\nplanes_tab |&gt; \n  select(tailnum, type, manufacturer, model, year) |&gt; \n  relocate(manufacturer, model, .before = type) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT tailnum, manufacturer, model, \"type\", \"year\"\nFROM planes"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#select-and-mutate",
    "href": "meetups/Meetup7/SQLSlides.html#select-and-mutate",
    "title": "SQL Slides",
    "section": "SELECT and mutate",
    "text": "SELECT and mutate\n\nmutate formulas appear as in SELECT statements\n\n\nflights_tab |&gt; \n  mutate(speed = distance/(air_time/60.0)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT flights.*, distance / (air_time / 60.0) AS speed\nFROM flights\n\nflights_tab |&gt; \n  mutate(speed = distance/(air_time/60.0)) |&gt; \n  collect() |&gt; \n  select(origin,dest,time_hour,carrier,flight,speed) |&gt;\n  head(5)\n\n# A tibble: 5 × 6\n  origin dest  time_hour           carrier flight speed\n  &lt;chr&gt;  &lt;chr&gt; &lt;dttm&gt;              &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n1 EWR    IAH   2013-01-01 10:00:00 UA        1545  370.\n2 LGA    IAH   2013-01-01 10:00:00 UA        1714  374.\n3 JFK    MIA   2013-01-01 10:00:00 AA        1141  408.\n4 JFK    BQN   2013-01-01 10:00:00 B6         725  517.\n5 LGA    ATL   2013-01-01 11:00:00 DL         461  394."
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#having-clause",
    "href": "meetups/Meetup7/SQLSlides.html#having-clause",
    "title": "SQL Slides",
    "section": "HAVING clause",
    "text": "HAVING clause\n\nIf you filter after summarise, HAVING instead of WHERE\n\n\n\n&lt;SQL&gt;\nSELECT carrier, AVG(dep_delay) AS dep_delay, COUNT(*) AS num_flights\nFROM flights\nGROUP BY carrier\nHAVING (COUNT(*) &gt; 100.0)"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#sub-queries",
    "href": "meetups/Meetup7/SQLSlides.html#sub-queries",
    "title": "SQL Slides",
    "section": "Sub-queries",
    "text": "Sub-queries\n\nSQL uses sub-queries to create sources of data for further queries:\n\n\nflights_tab |&gt; \n  mutate(speed = distance/(air_time/60.0)) |&gt; \n  filter(speed &gt; 450) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT q01.*\nFROM (\n  SELECT flights.*, distance / (air_time / 60.0) AS speed\n  FROM flights\n) q01\nWHERE (speed &gt; 450.0)\n\n\n\nquery name q01 generated by dbplyr\nSaw this in join example earlier"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#pitfalls",
    "href": "meetups/Meetup7/SQLSlides.html#pitfalls",
    "title": "SQL Slides",
    "section": "Pitfalls",
    "text": "Pitfalls\n\ndbplyr won’t always write nicest code\nIt takes deep knowledge to write performant/fast queries for large databases\n\nIf you need to become a pro in this, read use the index, Luke\n\nMany many SQL standards"
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "We acknowledge the wonderful resources made available on the open-source course Data Science in a Box, from which we adapted some course materials and homework assignments. This material is shared under a CC BY-SA 4.0 Creative Commons Share Alike License\nThis website would not be possible without the quarto package.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "modules/module5.html",
    "href": "modules/module5.html",
    "title": "Module 5 - Data Transformations",
    "section": "",
    "text": "Learning Objectives\n\nWorking with boolean data, basic logical operations, conditionals, and logical subsetting\nTransformations and summaries of numeric data\nUsing dplyr window functions\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 12, 13, 17\n\n\n\nAdditional Resources:\n\nFeature Engineering and Selection, Chapter 1 Feature engineering is a natural extension of data transformation that is commonly used prior to the application of machine learning or statistical analyses.\nforcats package reference index . This is the reference for a very useful package for handling categorical data\nWrangling Categorical Data in R. A. McNamara and N. Horton. The American Statistician (2018) Vol 72.\n\nThis paper discusses both tidyverse and base R approaches to categorical data.",
    "crumbs": [
      "Topics",
      "5 - Data Transformations"
    ]
  },
  {
    "objectID": "modules/module12.html",
    "href": "modules/module12.html",
    "title": "Module 12 - Graphs and Graph Data",
    "section": "",
    "text": "Learning Objectives\n\nBasic Graph Theory\nCalculate and interpret basic graph statistics\nVisualize graphs\nGraph Databases\nSocial Network Analysis\n\n\n\nReadings\n\nMath is Fun on Graphs",
    "crumbs": [
      "Topics",
      "12 - Graphs and Graph Data"
    ]
  },
  {
    "objectID": "modules/module9.html",
    "href": "modules/module9.html",
    "title": "Module 9 - Web Scraping and APIs",
    "section": "",
    "text": "Learning Objectives\n\nUnderstand and manipulate hierarchical/web Data Structures: XML, JSON, HTML\nScraping websites with R\nLearn the httr package for interactive with WebAPIs using R\nEthics: Data Privacy Issues\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 23-24\nhttr Guide\nOK Cupid Case Study\n\n\n\nVideos\n\nExtracting Data from the Web Part I\nExtracting Data from the Web Part II\nCambridge Analytica Whistleblower",
    "crumbs": [
      "Topics",
      "9 - Web Scraping and APIs"
    ]
  },
  {
    "objectID": "modules/module6.html",
    "href": "modules/module6.html",
    "title": "Module 6 - Processing Strings and Text",
    "section": "",
    "text": "Learning Objectives\n\nLearn to use regular expressions (regex) to find text\nProcess and transform strings in R\nWorking with factors for Categorical data\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 14, 15, 16\n\n\n\nAdditional Resources:\n\nRegex Buddy\nRegex Crossword\n\nRegular expressions are useful in a large number of applications. The first website provides an advanced tutorial, the second one provides fun exercises. If you are unsatisfied and want to learn even more, regular expressions are equivalent to a computer science concept known as “finite state automata”. You can dive way deeper by reading these lecture notes:\n\nFinite State Automata\nstringr package: reference index. This is the tidyverse package for processing strings. If you are stuck on a string problem the solution is probably somewhere here\nforcats package reference index . This is the reference for a very useful package for handling categorical data\nWrangling Categorical Data in R. A. McNamara and N. Horton. The American Statistician (2018) Vol 72.\n\nThis paper discusses both tidyverse and base R approaches to categorical data.",
    "crumbs": [
      "Topics",
      "6 - Processing Strings and Text"
    ]
  },
  {
    "objectID": "modules/module3.html",
    "href": "modules/module3.html",
    "title": "Module 3 - Data Tidying",
    "section": "",
    "text": "Learning Objectives\n\nConverting between wide and long data formats with tidyr\nChanging the shape of your data with dplyr\nImporting data into R\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 5-7\n\n\n\nAdditional Resources:\n\ntidyr pivot vignette\n\nThis vignette has some even more in depth discussion than the textbook.\n\nTidy Data. Hadley Wickham. Journal of statistical software 59 (2014): 1-23.\ntidyr vignette\n\nThe original Tidy Data paper is one of the most influential papers on data processing. It relates Tidy Data to database normalization, which might be familiar to you if you are an expert in SQL. The accompanying vignette has code used to generate the results in the paper. If you do read the paper, note that the definition of tidy data is different than the one in your textbook. The original definition is close to a relational database style of organization, while the current definition is more of a flat file approach that resembles a data matrix and which is more readily analyzed.\n\nTutorial on Cleveland Dot Plots\n\nThis is a classic data visualization tool that we will use in this week’s homework\n\nwidyr package\n\nA useful package that gets a little bit more at the trade-offs between different data organization schemes.\n\n\nVideos\nThere are some excellent optional video lectures that discuss tidy data and data wrangling:\n\nData Science Box Intro to Tidy Data\nData Science Box Data Wranglig\nData Science Box Data Transformations\nData Science Box Exploration of Tidy Data",
    "crumbs": [
      "Topics",
      "3 - Data Tidying"
    ]
  },
  {
    "objectID": "modules/module7.html",
    "href": "modules/module7.html",
    "title": "Module 7 - Working with Databases and SQL",
    "section": "",
    "text": "Learning Objectives\n\nWorking with multiple dataframes, Joins and Keys\nRelational Database basics: design and tradeoffs\nWorking with R and SQL\nBasic database normalization\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 18-21\n\n\n\nAdditional Resources:\n\nSQL Cheat Sheet\nPractical SQL: A Beginner’s Guide to Storytelling with Data. Anthony DeBarros. No Start Press (2022)\nSQL for Data Scientists. Renee Teate. Wiley.\nduckdb page and SQL intro\n\n\n\nVideos\nMother Duck tutorial",
    "crumbs": [
      "Topics",
      "7 - Working with Databases and SQL"
    ]
  },
  {
    "objectID": "modules/module1.html",
    "href": "modules/module1.html",
    "title": "Module 1 - Introduction to R and the Data Science Workflow",
    "section": "",
    "text": "Overview and Deliverables\nThis week is about meeting the other students, making sure you get a good start with required course software, and taking a bird’s eye view at data science workflows and the material that we will cover in the rest of the course. As such, the only official required assignments that you must turn in are to make a post on course Brightspace page introducing yourself, and to complete the meetup reflection (this is the same thing as the 1 minute papers of Data 606 if you are also taking that course). Howeer, you should also read the introduction posts of your classmates, perform the readings, and make sure to follow the tutorials and install the required software. Start early if you can with the software so we can sort out any issues asap. I’ll be monitoring the course slack page, which is the place where we will have most of our asynchronous communication during the course.\n\n08/28: Attend the initial course meetup at 6:45PM Eastern\nDue 9/01: Introduction Post in Brightspace Discussions\nSign up for GitHub and the course Slack channel\nComplete the readings of RDS (Intro and 28), the quarto tutorial, and up to section 15 in the happygitwithr tutorial\nInstall R, RStudio, git, etc (see the software page for more details\nSign up using the doodle poll for your data science in context presentation.\n\n\nLearning Objectives\n\nData Science Workflow\nCourse Toolkit: R, RStudio, tidyverse, Quarto, git\n\n\n\nReadings\n\nRDS (R for Data Science): Introduction, Chapter 28\n\nThe Introduction chapter will describe the workflow of a typical data science project and pave the road for the rest of the material we are covering in this course. Chapter 28 describes the quarto markdown language, which a flexible system for authoring projects and presentations that produces well typeset documents in a variety of file formats and integrates with a large number of different programming languages, so that computations and their results can be directly embedded and visualized in your documents. I suggest that you use quarto to create your homework assignments.\n\nQuarto tutorial\n\nThis is an official quarto tutorial.\n\nHappy Git and GitHub for the R User: Tutorial up through Section 15\n\nGit is a tool that allows a group of people to work together on a software project, enabling version control that allows differences to be more easily resolved. GitHub is a repository for software projects that integrates well with Git. Git is a commonly used tool, and is almost a requirement for teams with large codebases and more than a few people working on them. GitHub has almost been like the “LinkedIn” for Data Scientists and Software Developers. One of the goals of this degree program is to help you build a portfolio of work on GitHub that you can use to demonstrate your coding proficiency to potential employers. These tools can be intimidating because they have a steep learning curve, so I think it is important to start introducing them slowly, at the very start. There are many ways to interact with Git and GitHub, but this excellent book by Jenny Bryant focuses on the specific case of a Data Scientist working in R and RStudio. If you already know Git and have a good system, you aren’t required to follow this tutorial, but for those of you who are new to these tools this I recommend taking some time this week to get Git up and running and integrated with your RStudio installation.\n\nData Visualization: A Practical Introduction Appendix A1: How to Read an R Help File\n\nThis is another excellent resource on learning R, but the reading I’ve suggested here is an appendix which discusses the structure of help files. Being able to use help files successfully is a skill that separates novice and intermediate programmers from experienced ones, as help files often have a very formal structure that doesn’t read like a normal text document or piece of writing. This section can you know what to expect and how to get what you need from them instead of feeling frustrated.\n\n\nVideos\nHadley Wickham Introduces the Tidyverse\nThis course uses something called the tidyverse. The tidyverse is a series of packages that replaces much of the functionality of base R while at the same time giving the resulting R code a very different flavor. This course, and many of the courses in this program, start by jumping right into the tidyverse. This video provides an introduction to the tidyverse from one of its main creators. However, while this decision has some initial benefits, it isn’t without tradeoffs, so it is important for you to keep in mind that there are other, different ways to do things in R. We will explore some of these later in the class in the Big Data module and the Advanced R programming module.\nIf you want to read a little bit more about the trade offs of the different R ecosystems this is a short link:\nR Comparisons\nSpeaking with colleagues, the choice to use base R versus the tidyverse is quite polarizing, especially as you work in more complex projects.",
    "crumbs": [
      "Topics",
      "1 - Data Science Workflow and Toolkit"
    ]
  },
  {
    "objectID": "posts/2024-09-18-How-to-seek-coding-help.html",
    "href": "posts/2024-09-18-How-to-seek-coding-help.html",
    "title": "How to find help and make a reproducible example (Debugging)",
    "section": "",
    "text": "I made a short coding demo (length about 20 minutes) where I slowly go through one example of making a reproducible example of out code with a bug, and debuggin the code in the process. I talk a little bit about different places to find help. If you know what a reproducible example is, have used stack overflow, etc, this video may not be useful for you. If those concepts are new to you, then its very important for you to learn about them, unfortunately they are part of the “hidden curriculum” of data science and programming.\nYou can watch the video on youtube youtube by clicking here.\nHelpful resources for this subject include chapter 8 of your textbook, the help page for the reprex library, and the stack overflow website"
  },
  {
    "objectID": "posts/2024-09-11-Pivot-Demo.html",
    "href": "posts/2024-09-11-Pivot-Demo.html",
    "title": "Pivot Coding Demo",
    "section": "",
    "text": "I made a short coding demo (length about 15 minutes) where I slowly go through one example of data tidying and cleaning. The coding demo can be watched on this youtube video.\nDuring the beginning of the video I said “row” when I meant “column” and vice versa.\nYou can download the R script with all the commands I entered here: WeatherStationVignette.R\nThe data used for this (in case you want to try it) came from weather.csv\nYou can read about this vignette and more on the tidyr page"
  },
  {
    "objectID": "posts/2024-10-06-Meetup-7-Joins-and-Databases.html",
    "href": "posts/2024-10-06-Meetup-7-Joins-and-Databases.html",
    "title": "Week 7 Info: Joins and Databases",
    "section": "",
    "text": "Hello Class,\nToday is the start of week 7, where we will learn how to work with data that is stored in multiple different different sources. We will study the join function and learn about primary and foreign keys, as well as simple and compound or composite keys. The discussion of keys and joins will lead in to an introduction to relational databases, which are one of the most common ways for large organizations to store data. We will talk about the trade-offs of using databases versus other data storage schemes and introduce SQL (the most common standard for languages that access and modify databases) through R packages DBI and dbplyr and duckdb, which is a relational database system designed for data analysis and portability and which is also easy to learn from. In the first weeks of this program we have discussed how R and python are two required languages for data scientists to have proficiency in. SQL and databases are equally important.\nYou will have a lab assignment titled R and SQL due next Sunday at midnight.\nFor your reading assignments and more details about this week, head over to Module 7. If you can, complete the reading assignments before the weekly meetup on Wednesday. If you need help with with anything, don’t hesitate to post in slack or contact me directly."
  },
  {
    "objectID": "posts/2024-10-10-DBI-Normalization-Billboard-Vignette.html",
    "href": "posts/2024-10-10-DBI-Normalization-Billboard-Vignette.html",
    "title": "How to Create a Database from Within R using DBI",
    "section": "",
    "text": "I made a short coding demo (length about 20 minutes) where I take the original billboard dataset used in the tidy data paper and turn it into a duck database.\nYou can watch the video on youtube youtube by clicking here.\nTo follow along, you can download the code I typed at my github and also download the original billboard data here."
  },
  {
    "objectID": "posts/2024-09-26-Correlation-Trick.html",
    "href": "posts/2024-09-26-Correlation-Trick.html",
    "title": "The Correlation Trick (End of Meetup 5)",
    "section": "",
    "text": "I made a video discussing a few topics that we missed during Meetup 5. The subject is something called the “Spearman Correlation”, the “Spearman-Rank Correlation”, or alternatively the “Correlation Trick”. It is a useful tool that can help you determine when a data transformation might be useful.\nThe Video can be watched here on youtube\nThis is a useful heuristic that will help you at some point in your career (though maybe not this week) and I thought you should know about it. You can read more about Spearman Correlation here on wikipedia. The link to data transformation isn’t discussed in any open sources that I am aware of."
  },
  {
    "objectID": "posts/2024-10-13-Week-4-Info.html",
    "href": "posts/2024-10-13-Week-4-Info.html",
    "title": "Week 8 Info: Advanced R Programming and Project Proposals",
    "section": "",
    "text": "Hello Class,\nToday is the start of week 8, where we will learn several more sophisticated programming concepts in R which will enable you to write cleaner, more concise, and more understandable code and to also accomplish more complex programming tasks than previously. The key topics will be functions (Chapter 25); which allow you to re-use and re-factor code by aggregating chunks of code that perform specific functions into function definitions, iteration (Chapter 26); which allows you to perform the same operation on many different objects simultaneously, and important base R functions (Chapter 27); which will round out your R knowledge. Chapter 26 will introduce you to a programming paradigm known as functional programming, which has distinctive strengths and weaknesses compared to object oriented and imperative programming and is likely to be less familiar to most of you.\nThere will be no lab assignment this week, but you will need to complete a short (~2 page) project proposal by next Sunday: Project Proposal.\nFor your reading assignments and more details about this week, head over to Module 8. If you can, complete the reading assignments before the weekly meetup on Wednesday. If you need help with with anything, don’t hesitate to post in slack or contact me directly. Though we have no lab assignment this week, we will work the concepts of this week into future assignments and they are important for your success as an R programmer."
  },
  {
    "objectID": "posts/2024-09-18-Meetup-4-Exploratory-Data-Analysis.html",
    "href": "posts/2024-09-18-Meetup-4-Exploratory-Data-Analysis.html",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "",
    "text": "Meetup 4 will cover basic principles of data visualization. Click here to join the meetup on Zoom.\nYou can find the meetup slides here\nClick the Module 4 link to see the course materials and instructions for this week."
  },
  {
    "objectID": "posts/2024-09-30-Week-6-Info.html",
    "href": "posts/2024-09-30-Week-6-Info.html",
    "title": "Week 6 Info: Working With Text and Strings",
    "section": "",
    "text": "Hello Class,\nToday is the start of week 6, where we will learn tools for working with text, strings, and categories. These data types lack the strict mathematical structures that come with logical, numerical, and temporal data, and as a result require their own distinct set of tools. This week will involve a lot of practice learning new tools and functions for finding and matching patterns in text, such as regular expressions, and these tools will form the basis for a module in a few wooks on analyzing datasets derived from text mining or web scraping.\nYou will have a lab assignment titled Working with Text and Strings due next Sunday at midnight.\nFor your reading assignments and more details about this week, head over to Module 6. If you can, complete the reading assignments before the weekly meetup on Wednesday. If you need help with with anything, don’t hesitate to post in slack or contact me directly."
  },
  {
    "objectID": "posts/2024-09-25-Meetup-5-Data-Transformation.html",
    "href": "posts/2024-09-25-Meetup-5-Data-Transformation.html",
    "title": "Meetup 5: Data Transformations",
    "section": "",
    "text": "Meetup 5 will cover data transformations for logical and numerical data types. Click here to join the meetup on Zoom.\nYou can find the meetup slides here\nClick the Module 5 link to see the course materials and instructions for this week."
  },
  {
    "objectID": "posts/2024-09-01-Week-2-Info.html",
    "href": "posts/2024-09-01-Week-2-Info.html",
    "title": "Week 2 Info: Data Visualizations and Transformations",
    "section": "",
    "text": "Hello Class, it is the start of Week 2. During the first week (really half a week with a long weekend) you should have completed the reading assignments, familiarized yourself with R, RStudio, quarto, and git, and made sure these are working on your computer. If you haven’t done these things its extremely important that you do them asap to avoid falling behind. It was really nice to read all of your intro posts on Brightspace, thanks for putting in the effort to describe your interests and background in data science.\nThe biggest difference from Week 2 to Week 1 is that you will have your first homework assignment, a lab assignment titled Airbnb in NYC. It will be due next Sunday at midnight.\nFor your reading assignments and more details about this week, head over to Module 2. If you can, complete the reading assignments before the weekly meetup on Wednesday. If you need help with with anything, don’t hesitate to post in slack or contact me directly."
  },
  {
    "objectID": "posts/2024-10-03-Coding-Vignette-Text-Strings-Babynames.html",
    "href": "posts/2024-10-03-Coding-Vignette-Text-Strings-Babynames.html",
    "title": "Exploring Changes in US Baby Names Using stringr, forcats, and regex",
    "section": "",
    "text": "This coding demo (length about 30 minutes) shows how to use the stringr library and regular expressions to find insights about how baby names have varied throughout the past 140 years of US history.\nYou can watch the video on youtube youtube by clicking here.\nThe code for this vignette can be found by clicking here\nHelpful resources for this subject include chapter 14-16 of your textbook, the stringr package, the ‘forcats’ package regex buddy and regex crossword"
  },
  {
    "objectID": "posts/2024-10-02-Meetup-6-Text-and-Strings.html",
    "href": "posts/2024-10-02-Meetup-6-Text-and-Strings.html",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "",
    "text": "Meetup 6 will cover the stringr package and regular expressions. Click here to join the meetup on Zoom.\nYou can find the meetup slides here\nClick the Module 6 link to see the course materials and instructions for this week."
  },
  {
    "objectID": "course/software.html",
    "href": "course/software.html",
    "title": "Software",
    "section": "",
    "text": "R and RStudio\n\nWe will make use of R, an open source statistics program and language. Be sure to install R and RStudio on your own computers within the first few days of the class.\n\nR - download for Windows, Mac, or Linux.\nRStudio - Download Windows, Mac, or Linux versions from here\n\nIf using Windows, you also need to download RTools.\n\n\nSource Control\nAll course materials will be made available on Github which provides an implementation of the git open source version control system. RStudio supports git directly, but I recommend downloading Sourcetree. This is a free desktop client that provides an easier interface for working with Github. You will also need to create an account on Github.\nFor more information, Jenny Bryan’s Happy Git and Github for the useR is a free online book covering the important features of source control for R users.\n\n\nR Packages\n\nOnce everything is installed, execute the following command in RStudio to install the packages we will use for this class (you can copy-and-paste):\n\ninstall.packages(c('openintro','devtools','tidyverse', 'ggplot2',\n                   'psych','reshape2','knitr','markdown','shiny','R.rsp',\n                   'fivethirtyeight'))",
    "crumbs": [
      "Course information",
      "Software"
    ]
  },
  {
    "objectID": "course/textbooks.html",
    "href": "course/textbooks.html",
    "title": "Textbooks",
    "section": "",
    "text": "Required\n\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2023). R for Data Science (2e). O’Reilly\n\nThis is an open source textbook that can be freely read online at r4ds.hadley.nz/ but can be purchased from Amazon. The textbook was written in quarto and the source code can be found on github. This is the primary textbook for this course and the one that we will follow most closely and comprehensively.\n\n\nJennifer Bryan. Happy Git and GitHub for the R User.\n\nThis is an open source textbook that can be freely read online at happygitwithr.com/. This short online textbook introduces Git and GitHub to data scientists and statisticians and illustrates how to integrate them with the R ecosystem/toolkit.\n\n\nJulia Silge and David Robinson. (2017). Text Mining with R. O’Reilly\n\nThis is an open source textbook that can be freely read online at www.tidytextmining.com/ but can be purchased from Amazon.\n\n\n\nRecommended\nWickham, H. Advanced R. Baca Raton, FL: Taylor & Francis Group.\n\nMost of this book is available freely online at adv-r.had.co.nz but can be purchased from Amazon.",
    "crumbs": [
      "Course information",
      "Textbooks"
    ]
  },
  {
    "objectID": "course/overview.html",
    "href": "course/overview.html",
    "title": "DATA607 - Data Acquisition and Management",
    "section": "",
    "text": "In this course students will learn about core concepts of contemporary data collection and its management. Topics will include an introduction to programming and collaboration in statistical software packages, data visualization techniques, data wrangling and transformation, exploratory data analysis and data quality checks, data acquisition from a variety of sources including databases and the web, tools for working with textual and graph data, and working with large datasets in a cloud computing environment.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  }
]